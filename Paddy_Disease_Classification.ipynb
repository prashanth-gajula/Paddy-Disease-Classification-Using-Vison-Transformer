{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4d-VcL7qFrRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awtUgmn_0-CD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import random\n",
        "from shutil import copy2\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/Paddy_Images_train\""
      ],
      "metadata": {
        "id": "O9CBJAmkJ2lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = os.listdir(dataset_path)\n",
        "classes = [c for c in classes if os.path.isdir(os.path.join(dataset_path, c))]\n",
        "classes.sort()\n",
        "\n",
        "print(\"Total Classes:\", len(classes))\n",
        "print(\"\\nImages per class:\\n\")\n",
        "\n",
        "for cls in classes:\n",
        "    folder_path = os.path.join(dataset_path, cls)\n",
        "    num_imgs = len(os.listdir(folder_path))\n",
        "    print(f\"{cls}: {num_imgs} images\")\n"
      ],
      "metadata": {
        "id": "dSasGhN6Ka50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_class = random.choice(classes)\n",
        "sample_image_path = os.path.join(dataset_path, sample_class, random.choice(os.listdir(os.path.join(dataset_path, sample_class))))\n",
        "\n",
        "img = Image.open(sample_image_path)\n",
        "print(\"Sample image path:\", sample_image_path)\n",
        "print(\"Image size:\", img.size)   # (width, height)\n",
        "print(\"Mode:\", img.mode)         # RGB, RGBA, L etc.\n"
      ],
      "metadata": {
        "id": "Amg5LDDZNCfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n"
      ],
      "metadata": {
        "id": "HQgBu7iTQ9LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "full_dataset = datasets.ImageFolder(root=dataset_path)\n",
        "print(\"Classes:\", full_dataset.classes)\n",
        "print(\"Total images:\", len(full_dataset))\n",
        "\n",
        "num_samples = len(full_dataset)\n",
        "train_size = int(0.8 * num_samples)\n",
        "val_size = num_samples - train_size\n",
        "\n",
        "\n",
        "indices = torch.randperm(num_samples).tolist()\n",
        "train_indices = indices[:train_size]\n",
        "val_indices   = indices[train_size:]\n",
        "\n",
        "print(\"Train size:\", len(train_indices))\n",
        "print(\"Val size:\", len(val_indices))\n",
        "\n",
        "\n",
        "train_base = datasets.ImageFolder(dataset_path, transform=train_transform)\n",
        "val_base   = datasets.ImageFolder(dataset_path, transform=val_transform)\n",
        "\n",
        "\n",
        "train_dataset = Subset(train_base, train_indices)\n",
        "val_dataset   = Subset(val_base,   val_indices)\n",
        "\n",
        "\n",
        "train_targets = [full_dataset.targets[i] for i in train_indices]\n",
        "train_targets = torch.tensor(train_targets)\n",
        "\n",
        "class_counts = torch.bincount(train_targets)\n",
        "class_weights = 1.0 / class_counts.float()\n",
        "\n",
        "sample_weights = class_weights[train_targets]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# 4) DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    sampler=sampler,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "dCq4k8FtRGpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = len(train_loader)\n",
        "print(\"Total batches:\", num_batches)\n"
      ],
      "metadata": {
        "id": "0B4yhUYFY5eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 224\n",
        "num_channels = 3\n",
        "patch_size = 16\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "token_dim = 256\n",
        "num_heads = 8\n",
        "transformer_blocks = 8\n",
        "num_classes = 10\n",
        "batch_size = 64\n",
        "mlp_hidden_dim = 512\n",
        "learning_rate = 3e-4\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "tGyd7sehSbbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating patches**"
      ],
      "metadata": {
        "id": "49JAMGJCSVR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.patchembed = nn.Conv2d(num_channels,token_dim,kernel_size=patch_size,stride=patch_size)\n",
        "  def forward(self,x):\n",
        "    x = self.patchembed(x)#converting (64,3,224,224) ---->(64,256,14,14)\n",
        "    x = x.flatten(2)#converting (64,256,14,14) ---->(64,256,196) converting 2d array into a 1 d array.\n",
        "    x = x.transpose(1,2)#converting (64,256,196) ---->(64,196,256)\n",
        "    return x"
      ],
      "metadata": {
        "id": "_MaZ3SmoSZN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer Block**"
      ],
      "metadata": {
        "id": "Scco6S-HXSWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layernorm1 = nn.LayerNorm(token_dim)\n",
        "    self.layernorm2 = nn.LayerNorm(token_dim)\n",
        "    self.multiheadattention = nn.MultiheadAttention(token_dim, num_heads,batch_first=True)\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(token_dim,mlp_hidden_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(mlp_hidden_dim,token_dim)\n",
        "    )\n",
        "  def forward( self,x):\n",
        "    residual1 = x\n",
        "\n",
        "    x = self.layernorm1(x)\n",
        "    x = self.multiheadattention(x,x,x)[0]#x,x,x represent query, key aand value vectors and context vector matrix is stored at location 0 so we are using 0 in the output.\n",
        "    x = x + residual1\n",
        "\n",
        "    residual2 = x\n",
        "    x = self.layernorm2(x)\n",
        "    x = self.mlp(x)\n",
        "    x = x + residual2\n",
        "    return x"
      ],
      "metadata": {
        "id": "nQm4YAhgU6fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP Layer**"
      ],
      "metadata": {
        "id": "qjBbbinJXnVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPHead(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layernorm = nn.LayerNorm(token_dim)\n",
        "    self.mlp = nn.Linear(token_dim,num_classes)\n",
        "  def forward(self,x):\n",
        "    x = self.layernorm(x)\n",
        "    x = self.mlp(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "sxbfmKNWXqi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combining Everything**"
      ],
      "metadata": {
        "id": "lfsqHsY1Xym-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = PatchEmbedding()\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, token_dim))\n",
        "        self.position_embedding = nn.Parameter(torch.randn(1, num_patches + 1, token_dim))\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[TransformerEncoder() for _ in range(transformer_blocks)]\n",
        "        )\n",
        "        self.mlp_head = MLPHead()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        num_of_images_in_current_batch = x.shape[0]\n",
        "        cls_token = self.cls_token.expand(num_of_images_in_current_batch, -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        x = x + self.position_embedding\n",
        "        x = self.transformer_blocks(x)\n",
        "        x = x[:, 0]#we are extracting the cls_token only because this token has info about all the other tokens in the input image\n",
        "        x = self.mlp_head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AzZWqgFjXyFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VisionTransformer().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(device)"
      ],
      "metadata": {
        "id": "CzW7BALxX-UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []#to store loss after each epoch\n",
        "accuracy_list = []#to store accuracy after each epoch"
      ],
      "metadata": {
        "id": "LwOcWQnr21Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**"
      ],
      "metadata": {
        "id": "D0BFHJO_YBs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_epoch = 0\n",
        "    total_epoch = 0\n",
        "    print(f\"\\nEpoch {epoch+1}\")\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        #print(len(images),len(labels))\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        #print(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct = (preds == labels).sum().item()\n",
        "        accuracy = 100.0 * correct / labels.size(0)\n",
        "\n",
        "        correct_epoch += correct\n",
        "        total_epoch += labels.size(0)\n",
        "\n",
        "        #if batch_idx % 100 == 0:\n",
        "        print(f\"  Batch {batch_idx+1:3d}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.2f}%\")\n",
        "\n",
        "    epoch_acc = 100.0 * correct_epoch / total_epoch\n",
        "    loss_list.append(total_loss)\n",
        "    accuracy_list.append(epoch_acc)\n",
        "    print(f\"==> Epoch {epoch+1} Summary: Total Loss = {total_loss:.4f}, Accuracy = {epoch_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "kRsa2LsDYE4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vision Transformer Class - Switch to evaluation mode\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "test_acc = 100.0 * correct / total\n",
        "print(f\"\\n==> Val Accuracy: {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "jKkt74Ochog5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(loss_list) + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, loss_list, label='Train Loss', color='red')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs Epochs')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, accuracy_list, label='Train Accuracy', color='blue')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CCMNcuLjWyo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()   # paste your HF token when prompted"
      ],
      "metadata": {
        "id": "RcJP5GmhcUFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, torch\n",
        "\n",
        "save_dir = \"vit_paddy_pytorch\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# 1) save weights\n",
        "torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
        "\n",
        "# 2) save config so you can rebuild the model later\n",
        "config = {\n",
        "    \"model_type\": \"custom_vit_paddy\",\n",
        "    \"image_size\": image_size,\n",
        "    \"num_channels\": num_channels,\n",
        "    \"patch_size\": patch_size,\n",
        "    \"num_patches\": num_patches,\n",
        "    \"token_dim\": token_dim,\n",
        "    \"num_heads\": num_heads,\n",
        "    \"transformer_blocks\": transformer_blocks,\n",
        "    \"mlp_hidden_dim\": mlp_hidden_dim,\n",
        "    \"num_classes\": num_classes,\n",
        "}\n",
        "\n",
        "with open(os.path.join(save_dir, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "# 3) (optional but recommended) simple model card\n",
        "readme_text = f\"\"\"\n",
        "# ViT Paddy Disease Classifier\n",
        "\n",
        "- Architecture: Custom Vision Transformer in PyTorch\n",
        "- Classes: {num_classes} paddy diseases + normal\n",
        "- Image size: {image_size}x{image_size}\n",
        "- Train accuracy (final): 97.35%\n",
        "- Val accuracy (final): 93.32%\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(save_dir, \"README.md\"), \"w\") as f:\n",
        "    f.write(readme_text)\n"
      ],
      "metadata": {
        "id": "m4YYEZHucVgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo, upload_folder\n",
        "\n",
        "repo_id = \"prashanth2000/vit-paddy-disease-classifier\"\n",
        "\n",
        "# create the repo (does nothing if it already exists)\n",
        "create_repo(repo_id, exist_ok=True)\n",
        "\n",
        "# upload everything in the folder\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=save_dir,\n",
        "    commit_message=\"Add ViT paddy disease model\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "P0I6Ly5-deQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"vit_paddy_disease.pth\")"
      ],
      "metadata": {
        "id": "BvqMuzVWgOSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}