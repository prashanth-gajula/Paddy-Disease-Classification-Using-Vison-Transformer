{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b7418618c1e341d38f7e211a3d24bf08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_d0e6f1c2abb04098a254fc322026b40c"
          }
        },
        "f013cd7ff46c4413a29a7209a836286e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81d6aa29ee8c4ca4a2144e2c2e0b3a11",
            "placeholder": "​",
            "style": "IPY_MODEL_dfdf68b6ae7e4a73b98e4a788de05cc7",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "4e45bca530484eb0946d144543469900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_49e1377db5534de9afa7194e127995d1",
            "placeholder": "​",
            "style": "IPY_MODEL_637dfc58073748f7b75ef8295b03d4bc",
            "value": ""
          }
        },
        "7564cd2ad96846afadf1598defe92ff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_13357fa6f95344cc83fcc422b8dbb4c4",
            "style": "IPY_MODEL_f04b6e1a9e2c485e912d555fffa82848",
            "value": true
          }
        },
        "7c584954e3bf4f8191f8673d7a2a4e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_05e9cc9c7ce24af0bbe964867d961a21",
            "style": "IPY_MODEL_e238b1120dce4dc080298f124946a432",
            "tooltip": ""
          }
        },
        "f07043325063430fbe7a6a8e2a9c74ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_135ab96170f94465923e4124d280f412",
            "placeholder": "​",
            "style": "IPY_MODEL_33a6bf35720a48b2a74ad3574a744f6b",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "d0e6f1c2abb04098a254fc322026b40c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "81d6aa29ee8c4ca4a2144e2c2e0b3a11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfdf68b6ae7e4a73b98e4a788de05cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49e1377db5534de9afa7194e127995d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "637dfc58073748f7b75ef8295b03d4bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13357fa6f95344cc83fcc422b8dbb4c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f04b6e1a9e2c485e912d555fffa82848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05e9cc9c7ce24af0bbe964867d961a21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e238b1120dce4dc080298f124946a432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "135ab96170f94465923e4124d280f412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33a6bf35720a48b2a74ad3574a744f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30f913290bb64e89844d6f2ae3b5498b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdf13cd7c7714062a54a563b4eb8246b",
            "placeholder": "​",
            "style": "IPY_MODEL_43ea79a399494178b026ebde78d87ba0",
            "value": "Connecting..."
          }
        },
        "fdf13cd7c7714062a54a563b4eb8246b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43ea79a399494178b026ebde78d87ba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65b0f5f3301c4e18a86e7301b2a39e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33274c5f1d0743d097fe28b1022cfd38",
              "IPY_MODEL_edc272001d374fbe83305abdde4c438c",
              "IPY_MODEL_2afbe64238c44c9ab3caa2d58fe6f694"
            ],
            "layout": "IPY_MODEL_28f31d0229ca467cac2d3f5a8df7f328"
          }
        },
        "33274c5f1d0743d097fe28b1022cfd38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3faa6f8efac94117854fb22d9db3c2ef",
            "placeholder": "​",
            "style": "IPY_MODEL_c837ece78ef24c66970998dcb6e453f3",
            "value": "Processing Files (1 / 1)      : 100%"
          }
        },
        "edc272001d374fbe83305abdde4c438c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4e246ad9c9d44959f799178ab753e5b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55dd966df1f444bd99971f56efec2c0a",
            "value": 1
          }
        },
        "2afbe64238c44c9ab3caa2d58fe6f694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fd406882e92467fad0004a259ef23d3",
            "placeholder": "​",
            "style": "IPY_MODEL_508c5670b790449fb45a1151ceb05df3",
            "value": " 17.9MB / 17.9MB, 3.89MB/s  "
          }
        },
        "28f31d0229ca467cac2d3f5a8df7f328": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3faa6f8efac94117854fb22d9db3c2ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c837ece78ef24c66970998dcb6e453f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4e246ad9c9d44959f799178ab753e5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "55dd966df1f444bd99971f56efec2c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fd406882e92467fad0004a259ef23d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "508c5670b790449fb45a1151ceb05df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "955b0870b3a94fa8a787918e65f55803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ca80d4fc98d445490bcd32e07f8e531",
              "IPY_MODEL_c40e07b3e9ca4f249fa8aed395d83e07",
              "IPY_MODEL_aea4e230198e48a58d1a100057daea48"
            ],
            "layout": "IPY_MODEL_0bd33fc529de4b41aa56e3349823d8d0"
          }
        },
        "5ca80d4fc98d445490bcd32e07f8e531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63d22d6808d2424a9e668b24fb698856",
            "placeholder": "​",
            "style": "IPY_MODEL_9ce39f1cd69140a8b87bfca6ac04b8c5",
            "value": "New Data Upload               : 100%"
          }
        },
        "c40e07b3e9ca4f249fa8aed395d83e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22a7d846d3d4453f810ff1bd117052a6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c02aca0a6f2940fd98b0a469acb745b4",
            "value": 1
          }
        },
        "aea4e230198e48a58d1a100057daea48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7931b11d3fc040cb8866fec997ad0603",
            "placeholder": "​",
            "style": "IPY_MODEL_265b99b33a4948be999f9315383f39f6",
            "value": " 17.9MB / 17.9MB, 3.89MB/s  "
          }
        },
        "0bd33fc529de4b41aa56e3349823d8d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63d22d6808d2424a9e668b24fb698856": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ce39f1cd69140a8b87bfca6ac04b8c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22a7d846d3d4453f810ff1bd117052a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c02aca0a6f2940fd98b0a469acb745b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7931b11d3fc040cb8866fec997ad0603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "265b99b33a4948be999f9315383f39f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc0741fe11c746de80eb232c52bcd6fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f08d7e9bb543447190eb9f85ab186f68",
              "IPY_MODEL_efbc20312c8146608b834e0749526191",
              "IPY_MODEL_c96ddf97267d49afa25f917484a78fb6"
            ],
            "layout": "IPY_MODEL_b632c17571d14157ab6148ef30446c4e"
          }
        },
        "f08d7e9bb543447190eb9f85ab186f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dd0059dc50748ac9b56c172f106829f",
            "placeholder": "​",
            "style": "IPY_MODEL_84838508289441d89c97b43f69cea560",
            "value": "  ...pytorch/pytorch_model.bin: 100%"
          }
        },
        "efbc20312c8146608b834e0749526191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f81e9f6b9e9f4b74b6b0fdc52e5038b9",
            "max": 17909379,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7024f2b77b134d728a730bb63f69de00",
            "value": 17909379
          }
        },
        "c96ddf97267d49afa25f917484a78fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d56a18dcce74fc181ba39b852428736",
            "placeholder": "​",
            "style": "IPY_MODEL_f23dbed3b03a403db397b631487b1586",
            "value": " 17.9MB / 17.9MB            "
          }
        },
        "b632c17571d14157ab6148ef30446c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd0059dc50748ac9b56c172f106829f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84838508289441d89c97b43f69cea560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f81e9f6b9e9f4b74b6b0fdc52e5038b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7024f2b77b134d728a730bb63f69de00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d56a18dcce74fc181ba39b852428736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f23dbed3b03a403db397b631487b1586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d-VcL7qFrRk",
        "outputId": "7fa378ff-5e7f-438c-fc87-1df10086082d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "awtUgmn_0-CD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import random\n",
        "from shutil import copy2\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/Paddy_Images_train\""
      ],
      "metadata": {
        "id": "O9CBJAmkJ2lp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = os.listdir(dataset_path)\n",
        "classes = [c for c in classes if os.path.isdir(os.path.join(dataset_path, c))]\n",
        "classes.sort()\n",
        "\n",
        "print(\"Total Classes:\", len(classes))\n",
        "print(\"\\nImages per class:\\n\")\n",
        "\n",
        "for cls in classes:\n",
        "    folder_path = os.path.join(dataset_path, cls)\n",
        "    num_imgs = len(os.listdir(folder_path))\n",
        "    print(f\"{cls}: {num_imgs} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSasGhN6Ka50",
        "outputId": "6cdc3bee-f71e-49da-c1e8-f89230f861e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Classes: 10\n",
            "\n",
            "Images per class:\n",
            "\n",
            "bacterial_leaf_blight: 479 images\n",
            "bacterial_leaf_streak: 380 images\n",
            "bacterial_panicle_blight: 337 images\n",
            "blast: 1738 images\n",
            "brown_spot: 965 images\n",
            "dead_heart: 1442 images\n",
            "downy_mildew: 620 images\n",
            "hispa: 1594 images\n",
            "normal: 1764 images\n",
            "tungro: 1088 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_class = random.choice(classes)\n",
        "sample_image_path = os.path.join(dataset_path, sample_class, random.choice(os.listdir(os.path.join(dataset_path, sample_class))))\n",
        "\n",
        "img = Image.open(sample_image_path)\n",
        "print(\"Sample image path:\", sample_image_path)\n",
        "print(\"Image size:\", img.size)   # (width, height)\n",
        "print(\"Mode:\", img.mode)         # RGB, RGBA, L etc.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Amg5LDDZNCfp",
        "outputId": "c83d99dc-d2bd-434f-b0e1-0ad6c56b4939"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample image path: /content/drive/MyDrive/Paddy_Images_train/bacterial_leaf_streak/101600.jpg\n",
            "Image size: (480, 640)\n",
            "Mode: RGB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n"
      ],
      "metadata": {
        "id": "HQgBu7iTQ9LL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "full_dataset = datasets.ImageFolder(root=dataset_path)\n",
        "print(\"Classes:\", full_dataset.classes)\n",
        "print(\"Total images:\", len(full_dataset))\n",
        "\n",
        "num_samples = len(full_dataset)\n",
        "train_size = int(0.8 * num_samples)\n",
        "val_size = num_samples - train_size\n",
        "\n",
        "\n",
        "indices = torch.randperm(num_samples).tolist()\n",
        "train_indices = indices[:train_size]\n",
        "val_indices   = indices[train_size:]\n",
        "\n",
        "print(\"Train size:\", len(train_indices))\n",
        "print(\"Val size:\", len(val_indices))\n",
        "\n",
        "\n",
        "train_base = datasets.ImageFolder(dataset_path, transform=train_transform)\n",
        "val_base   = datasets.ImageFolder(dataset_path, transform=val_transform)\n",
        "\n",
        "\n",
        "train_dataset = Subset(train_base, train_indices)\n",
        "val_dataset   = Subset(val_base,   val_indices)\n",
        "\n",
        "\n",
        "train_targets = [full_dataset.targets[i] for i in train_indices]\n",
        "train_targets = torch.tensor(train_targets)\n",
        "\n",
        "class_counts = torch.bincount(train_targets)\n",
        "class_weights = 1.0 / class_counts.float()\n",
        "\n",
        "sample_weights = class_weights[train_targets]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# 4) DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    sampler=sampler,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCq4k8FtRGpj",
        "outputId": "147db8cf-61dd-48ea-8f44-db08c529f729"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['bacterial_leaf_blight', 'bacterial_leaf_streak', 'bacterial_panicle_blight', 'blast', 'brown_spot', 'dead_heart', 'downy_mildew', 'hispa', 'normal', 'tungro']\n",
            "Total images: 10407\n",
            "Train size: 8325\n",
            "Val size: 2082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = len(train_loader)\n",
        "print(\"Total batches:\", num_batches)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B4yhUYFY5eu",
        "outputId": "5cf0605e-a03c-4bec-e122-4168f26ee460"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total batches: 131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 224\n",
        "num_channels = 3\n",
        "patch_size = 16\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "token_dim = 256\n",
        "num_heads = 8\n",
        "transformer_blocks = 8\n",
        "num_classes = 10\n",
        "batch_size = 64\n",
        "mlp_hidden_dim = 512\n",
        "learning_rate = 3e-4\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "tGyd7sehSbbG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating patches**"
      ],
      "metadata": {
        "id": "49JAMGJCSVR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.patchembed = nn.Conv2d(num_channels,token_dim,kernel_size=patch_size,stride=patch_size)\n",
        "  def forward(self,x):\n",
        "    x = self.patchembed(x)#converting (64,3,224,224) ---->(64,256,14,14)\n",
        "    x = x.flatten(2)#converting (64,256,14,14) ---->(64,256,196) converting 2d array into a 1 d array.\n",
        "    x = x.transpose(1,2)#converting (64,256,196) ---->(64,196,256)\n",
        "    return x"
      ],
      "metadata": {
        "id": "_MaZ3SmoSZN8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer Block**"
      ],
      "metadata": {
        "id": "Scco6S-HXSWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layernorm1 = nn.LayerNorm(token_dim)\n",
        "    self.layernorm2 = nn.LayerNorm(token_dim)\n",
        "    self.multiheadattention = nn.MultiheadAttention(token_dim, num_heads,batch_first=True)\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(token_dim,mlp_hidden_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(mlp_hidden_dim,token_dim)\n",
        "    )\n",
        "  def forward( self,x):\n",
        "    residual1 = x\n",
        "\n",
        "    x = self.layernorm1(x)\n",
        "    x = self.multiheadattention(x,x,x)[0]#x,x,x represent query, key aand value vectors and context vector matrix is stored at location 0 so we are using 0 in the output.\n",
        "    x = x + residual1\n",
        "\n",
        "    residual2 = x\n",
        "    x = self.layernorm2(x)\n",
        "    x = self.mlp(x)\n",
        "    x = x + residual2\n",
        "    return x"
      ],
      "metadata": {
        "id": "nQm4YAhgU6fe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP Layer**"
      ],
      "metadata": {
        "id": "qjBbbinJXnVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPHead(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layernorm = nn.LayerNorm(token_dim)\n",
        "    self.mlp = nn.Linear(token_dim,num_classes)\n",
        "  def forward(self,x):\n",
        "    x = self.layernorm(x)\n",
        "    x = self.mlp(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "sxbfmKNWXqi_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combining Everything**"
      ],
      "metadata": {
        "id": "lfsqHsY1Xym-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = PatchEmbedding()\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, token_dim))\n",
        "        self.position_embedding = nn.Parameter(torch.randn(1, num_patches + 1, token_dim))\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[TransformerEncoder() for _ in range(transformer_blocks)]\n",
        "        )\n",
        "        self.mlp_head = MLPHead()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        num_of_images_in_current_batch = x.shape[0]\n",
        "        cls_token = self.cls_token.expand(num_of_images_in_current_batch, -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        x = x + self.position_embedding\n",
        "        x = self.transformer_blocks(x)\n",
        "        x = x[:, 0]#we are extracting the cls_token only because this token has info about all the other tokens in the input image\n",
        "        x = self.mlp_head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AzZWqgFjXyFI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VisionTransformer().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzW7BALxX-UT",
        "outputId": "29e00f0a-e7d3-4367-af03-d71669b3b405"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []#to store loss after each epoch\n",
        "accuracy_list = []#to store accuracy after each epoch"
      ],
      "metadata": {
        "id": "LwOcWQnr21Mf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**"
      ],
      "metadata": {
        "id": "D0BFHJO_YBs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_epoch = 0\n",
        "    total_epoch = 0\n",
        "    print(f\"\\nEpoch {epoch+1}\")\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        #print(len(images),len(labels))\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        #print(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct = (preds == labels).sum().item()\n",
        "        accuracy = 100.0 * correct / labels.size(0)\n",
        "\n",
        "        correct_epoch += correct\n",
        "        total_epoch += labels.size(0)\n",
        "\n",
        "        #if batch_idx % 100 == 0:\n",
        "        print(f\"  Batch {batch_idx+1:3d}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.2f}%\")\n",
        "\n",
        "    epoch_acc = 100.0 * correct_epoch / total_epoch\n",
        "    loss_list.append(total_loss)\n",
        "    accuracy_list.append(epoch_acc)\n",
        "    print(f\"==> Epoch {epoch+1} Summary: Total Loss = {total_loss:.4f}, Accuracy = {epoch_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRsa2LsDYE4A",
        "outputId": "074a3a65-8027-4ada-c4fd-425ed3f6efa7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  Batch  91: Loss = 0.0687, Accuracy = 98.44%\n",
            "  Batch  92: Loss = 0.0783, Accuracy = 98.44%\n",
            "  Batch  93: Loss = 0.0714, Accuracy = 98.44%\n",
            "  Batch  94: Loss = 0.1703, Accuracy = 90.62%\n",
            "  Batch  95: Loss = 0.1256, Accuracy = 93.75%\n",
            "  Batch  96: Loss = 0.0693, Accuracy = 96.88%\n",
            "  Batch  97: Loss = 0.0388, Accuracy = 100.00%\n",
            "  Batch  98: Loss = 0.2580, Accuracy = 89.06%\n",
            "  Batch  99: Loss = 0.1292, Accuracy = 95.31%\n",
            "  Batch 100: Loss = 0.1594, Accuracy = 95.31%\n",
            "  Batch 101: Loss = 0.1244, Accuracy = 96.88%\n",
            "  Batch 102: Loss = 0.1210, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.0487, Accuracy = 98.44%\n",
            "  Batch 104: Loss = 0.0689, Accuracy = 98.44%\n",
            "  Batch 105: Loss = 0.1520, Accuracy = 95.31%\n",
            "  Batch 106: Loss = 0.0437, Accuracy = 98.44%\n",
            "  Batch 107: Loss = 0.1937, Accuracy = 89.06%\n",
            "  Batch 108: Loss = 0.0354, Accuracy = 98.44%\n",
            "  Batch 109: Loss = 0.0720, Accuracy = 96.88%\n",
            "  Batch 110: Loss = 0.1319, Accuracy = 95.31%\n",
            "  Batch 111: Loss = 0.0677, Accuracy = 96.88%\n",
            "  Batch 112: Loss = 0.1449, Accuracy = 95.31%\n",
            "  Batch 113: Loss = 0.0685, Accuracy = 95.31%\n",
            "  Batch 114: Loss = 0.0963, Accuracy = 96.88%\n",
            "  Batch 115: Loss = 0.0471, Accuracy = 98.44%\n",
            "  Batch 116: Loss = 0.0558, Accuracy = 96.88%\n",
            "  Batch 117: Loss = 0.1122, Accuracy = 96.88%\n",
            "  Batch 118: Loss = 0.1205, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.1051, Accuracy = 95.31%\n",
            "  Batch 120: Loss = 0.0095, Accuracy = 100.00%\n",
            "  Batch 121: Loss = 0.0746, Accuracy = 98.44%\n",
            "  Batch 122: Loss = 0.2592, Accuracy = 92.19%\n",
            "  Batch 123: Loss = 0.0694, Accuracy = 96.88%\n",
            "  Batch 124: Loss = 0.1932, Accuracy = 93.75%\n",
            "  Batch 125: Loss = 0.1622, Accuracy = 95.31%\n",
            "  Batch 126: Loss = 0.1313, Accuracy = 95.31%\n",
            "  Batch 127: Loss = 0.0521, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0442, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.0764, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.0953, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0087, Accuracy = 100.00%\n",
            "==> Epoch 63 Summary: Total Loss = 17.1530, Accuracy = 95.57%\n",
            "\n",
            "Epoch 64\n",
            "  Batch   1: Loss = 0.0566, Accuracy = 96.88%\n",
            "  Batch   2: Loss = 0.1023, Accuracy = 98.44%\n",
            "  Batch   3: Loss = 0.1629, Accuracy = 93.75%\n",
            "  Batch   4: Loss = 0.1790, Accuracy = 93.75%\n",
            "  Batch   5: Loss = 0.0841, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.1259, Accuracy = 95.31%\n",
            "  Batch   7: Loss = 0.0817, Accuracy = 95.31%\n",
            "  Batch   8: Loss = 0.0466, Accuracy = 98.44%\n",
            "  Batch   9: Loss = 0.1361, Accuracy = 95.31%\n",
            "  Batch  10: Loss = 0.0399, Accuracy = 100.00%\n",
            "  Batch  11: Loss = 0.1201, Accuracy = 96.88%\n",
            "  Batch  12: Loss = 0.0425, Accuracy = 96.88%\n",
            "  Batch  13: Loss = 0.0875, Accuracy = 96.88%\n",
            "  Batch  14: Loss = 0.1651, Accuracy = 93.75%\n",
            "  Batch  15: Loss = 0.1297, Accuracy = 95.31%\n",
            "  Batch  16: Loss = 0.0506, Accuracy = 98.44%\n",
            "  Batch  17: Loss = 0.0387, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.0320, Accuracy = 98.44%\n",
            "  Batch  19: Loss = 0.0483, Accuracy = 98.44%\n",
            "  Batch  20: Loss = 0.0432, Accuracy = 98.44%\n",
            "  Batch  21: Loss = 0.0639, Accuracy = 96.88%\n",
            "  Batch  22: Loss = 0.0833, Accuracy = 96.88%\n",
            "  Batch  23: Loss = 0.0462, Accuracy = 98.44%\n",
            "  Batch  24: Loss = 0.0779, Accuracy = 95.31%\n",
            "  Batch  25: Loss = 0.0561, Accuracy = 98.44%\n",
            "  Batch  26: Loss = 0.2049, Accuracy = 93.75%\n",
            "  Batch  27: Loss = 0.1086, Accuracy = 96.88%\n",
            "  Batch  28: Loss = 0.2213, Accuracy = 92.19%\n",
            "  Batch  29: Loss = 0.1593, Accuracy = 92.19%\n",
            "  Batch  30: Loss = 0.0689, Accuracy = 98.44%\n",
            "  Batch  31: Loss = 0.0853, Accuracy = 96.88%\n",
            "  Batch  32: Loss = 0.0325, Accuracy = 100.00%\n",
            "  Batch  33: Loss = 0.0616, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.1120, Accuracy = 93.75%\n",
            "  Batch  35: Loss = 0.0925, Accuracy = 96.88%\n",
            "  Batch  36: Loss = 0.0607, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.0370, Accuracy = 100.00%\n",
            "  Batch  38: Loss = 0.2358, Accuracy = 92.19%\n",
            "  Batch  39: Loss = 0.2225, Accuracy = 92.19%\n",
            "  Batch  40: Loss = 0.1297, Accuracy = 95.31%\n",
            "  Batch  41: Loss = 0.0625, Accuracy = 98.44%\n",
            "  Batch  42: Loss = 0.1659, Accuracy = 95.31%\n",
            "  Batch  43: Loss = 0.0613, Accuracy = 98.44%\n",
            "  Batch  44: Loss = 0.0768, Accuracy = 96.88%\n",
            "  Batch  45: Loss = 0.1266, Accuracy = 95.31%\n",
            "  Batch  46: Loss = 0.1451, Accuracy = 92.19%\n",
            "  Batch  47: Loss = 0.0917, Accuracy = 96.88%\n",
            "  Batch  48: Loss = 0.0864, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.0477, Accuracy = 98.44%\n",
            "  Batch  50: Loss = 0.0756, Accuracy = 96.88%\n",
            "  Batch  51: Loss = 0.0840, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.1786, Accuracy = 93.75%\n",
            "  Batch  53: Loss = 0.0916, Accuracy = 96.88%\n",
            "  Batch  54: Loss = 0.0714, Accuracy = 95.31%\n",
            "  Batch  55: Loss = 0.0423, Accuracy = 96.88%\n",
            "  Batch  56: Loss = 0.1026, Accuracy = 95.31%\n",
            "  Batch  57: Loss = 0.0757, Accuracy = 96.88%\n",
            "  Batch  58: Loss = 0.0759, Accuracy = 96.88%\n",
            "  Batch  59: Loss = 0.1078, Accuracy = 96.88%\n",
            "  Batch  60: Loss = 0.1521, Accuracy = 92.19%\n",
            "  Batch  61: Loss = 0.1059, Accuracy = 95.31%\n",
            "  Batch  62: Loss = 0.0358, Accuracy = 98.44%\n",
            "  Batch  63: Loss = 0.1047, Accuracy = 95.31%\n",
            "  Batch  64: Loss = 0.2075, Accuracy = 89.06%\n",
            "  Batch  65: Loss = 0.1812, Accuracy = 95.31%\n",
            "  Batch  66: Loss = 0.0158, Accuracy = 100.00%\n",
            "  Batch  67: Loss = 0.1646, Accuracy = 93.75%\n",
            "  Batch  68: Loss = 0.1945, Accuracy = 93.75%\n",
            "  Batch  69: Loss = 0.1930, Accuracy = 93.75%\n",
            "  Batch  70: Loss = 0.0377, Accuracy = 100.00%\n",
            "  Batch  71: Loss = 0.0859, Accuracy = 96.88%\n",
            "  Batch  72: Loss = 0.0797, Accuracy = 96.88%\n",
            "  Batch  73: Loss = 0.0547, Accuracy = 98.44%\n",
            "  Batch  74: Loss = 0.1227, Accuracy = 96.88%\n",
            "  Batch  75: Loss = 0.0707, Accuracy = 96.88%\n",
            "  Batch  76: Loss = 0.0674, Accuracy = 98.44%\n",
            "  Batch  77: Loss = 0.0532, Accuracy = 96.88%\n",
            "  Batch  78: Loss = 0.1404, Accuracy = 96.88%\n",
            "  Batch  79: Loss = 0.1721, Accuracy = 95.31%\n",
            "  Batch  80: Loss = 0.0759, Accuracy = 95.31%\n",
            "  Batch  81: Loss = 0.0755, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.1304, Accuracy = 93.75%\n",
            "  Batch  83: Loss = 0.1227, Accuracy = 95.31%\n",
            "  Batch  84: Loss = 0.1235, Accuracy = 93.75%\n",
            "  Batch  85: Loss = 0.1226, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.0859, Accuracy = 96.88%\n",
            "  Batch  87: Loss = 0.0352, Accuracy = 100.00%\n",
            "  Batch  88: Loss = 0.0677, Accuracy = 98.44%\n",
            "  Batch  89: Loss = 0.0877, Accuracy = 96.88%\n",
            "  Batch  90: Loss = 0.0648, Accuracy = 98.44%\n",
            "  Batch  91: Loss = 0.0383, Accuracy = 100.00%\n",
            "  Batch  92: Loss = 0.1853, Accuracy = 93.75%\n",
            "  Batch  93: Loss = 0.0298, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.0887, Accuracy = 96.88%\n",
            "  Batch  95: Loss = 0.1391, Accuracy = 98.44%\n",
            "  Batch  96: Loss = 0.0429, Accuracy = 100.00%\n",
            "  Batch  97: Loss = 0.0633, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.1905, Accuracy = 93.75%\n",
            "  Batch  99: Loss = 0.0258, Accuracy = 100.00%\n",
            "  Batch 100: Loss = 0.1163, Accuracy = 93.75%\n",
            "  Batch 101: Loss = 0.0422, Accuracy = 98.44%\n",
            "  Batch 102: Loss = 0.1772, Accuracy = 95.31%\n",
            "  Batch 103: Loss = 0.0404, Accuracy = 98.44%\n",
            "  Batch 104: Loss = 0.1257, Accuracy = 92.19%\n",
            "  Batch 105: Loss = 0.0117, Accuracy = 100.00%\n",
            "  Batch 106: Loss = 0.1417, Accuracy = 95.31%\n",
            "  Batch 107: Loss = 0.1028, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.1399, Accuracy = 96.88%\n",
            "  Batch 109: Loss = 0.0848, Accuracy = 95.31%\n",
            "  Batch 110: Loss = 0.1260, Accuracy = 96.88%\n",
            "  Batch 111: Loss = 0.0220, Accuracy = 100.00%\n",
            "  Batch 112: Loss = 0.0554, Accuracy = 98.44%\n",
            "  Batch 113: Loss = 0.0408, Accuracy = 98.44%\n",
            "  Batch 114: Loss = 0.1197, Accuracy = 95.31%\n",
            "  Batch 115: Loss = 0.1379, Accuracy = 95.31%\n",
            "  Batch 116: Loss = 0.0745, Accuracy = 96.88%\n",
            "  Batch 117: Loss = 0.1032, Accuracy = 95.31%\n",
            "  Batch 118: Loss = 0.0830, Accuracy = 98.44%\n",
            "  Batch 119: Loss = 0.0872, Accuracy = 98.44%\n",
            "  Batch 120: Loss = 0.2300, Accuracy = 90.62%\n",
            "  Batch 121: Loss = 0.1979, Accuracy = 93.75%\n",
            "  Batch 122: Loss = 0.0233, Accuracy = 100.00%\n",
            "  Batch 123: Loss = 0.1209, Accuracy = 92.19%\n",
            "  Batch 124: Loss = 0.0431, Accuracy = 100.00%\n",
            "  Batch 125: Loss = 0.1793, Accuracy = 93.75%\n",
            "  Batch 126: Loss = 0.1027, Accuracy = 95.31%\n",
            "  Batch 127: Loss = 0.1497, Accuracy = 96.88%\n",
            "  Batch 128: Loss = 0.1061, Accuracy = 95.31%\n",
            "  Batch 129: Loss = 0.0523, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.0662, Accuracy = 95.31%\n",
            "  Batch 131: Loss = 0.0931, Accuracy = 100.00%\n",
            "==> Epoch 64 Summary: Total Loss = 12.9335, Accuracy = 96.50%\n",
            "\n",
            "Epoch 65\n",
            "  Batch   1: Loss = 0.1349, Accuracy = 93.75%\n",
            "  Batch   2: Loss = 0.1406, Accuracy = 93.75%\n",
            "  Batch   3: Loss = 0.0491, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0477, Accuracy = 98.44%\n",
            "  Batch   5: Loss = 0.0326, Accuracy = 98.44%\n",
            "  Batch   6: Loss = 0.1552, Accuracy = 95.31%\n",
            "  Batch   7: Loss = 0.1210, Accuracy = 95.31%\n",
            "  Batch   8: Loss = 0.0722, Accuracy = 98.44%\n",
            "  Batch   9: Loss = 0.1474, Accuracy = 96.88%\n",
            "  Batch  10: Loss = 0.0710, Accuracy = 96.88%\n",
            "  Batch  11: Loss = 0.0442, Accuracy = 96.88%\n",
            "  Batch  12: Loss = 0.1823, Accuracy = 93.75%\n",
            "  Batch  13: Loss = 0.0279, Accuracy = 100.00%\n",
            "  Batch  14: Loss = 0.0818, Accuracy = 98.44%\n",
            "  Batch  15: Loss = 0.1009, Accuracy = 95.31%\n",
            "  Batch  16: Loss = 0.3199, Accuracy = 87.50%\n",
            "  Batch  17: Loss = 0.0667, Accuracy = 96.88%\n",
            "  Batch  18: Loss = 0.1629, Accuracy = 93.75%\n",
            "  Batch  19: Loss = 0.0925, Accuracy = 92.19%\n",
            "  Batch  20: Loss = 0.0711, Accuracy = 98.44%\n",
            "  Batch  21: Loss = 0.1126, Accuracy = 98.44%\n",
            "  Batch  22: Loss = 0.3025, Accuracy = 89.06%\n",
            "  Batch  23: Loss = 0.2762, Accuracy = 90.62%\n",
            "  Batch  24: Loss = 0.1319, Accuracy = 96.88%\n",
            "  Batch  25: Loss = 0.0641, Accuracy = 96.88%\n",
            "  Batch  26: Loss = 0.1432, Accuracy = 95.31%\n",
            "  Batch  27: Loss = 0.0446, Accuracy = 100.00%\n",
            "  Batch  28: Loss = 0.1063, Accuracy = 96.88%\n",
            "  Batch  29: Loss = 0.1413, Accuracy = 93.75%\n",
            "  Batch  30: Loss = 0.1371, Accuracy = 95.31%\n",
            "  Batch  31: Loss = 0.0511, Accuracy = 98.44%\n",
            "  Batch  32: Loss = 0.1160, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.1177, Accuracy = 96.88%\n",
            "  Batch  34: Loss = 0.0858, Accuracy = 98.44%\n",
            "  Batch  35: Loss = 0.0608, Accuracy = 98.44%\n",
            "  Batch  36: Loss = 0.1122, Accuracy = 95.31%\n",
            "  Batch  37: Loss = 0.2784, Accuracy = 92.19%\n",
            "  Batch  38: Loss = 0.1137, Accuracy = 95.31%\n",
            "  Batch  39: Loss = 0.0933, Accuracy = 96.88%\n",
            "  Batch  40: Loss = 0.0749, Accuracy = 98.44%\n",
            "  Batch  41: Loss = 0.1893, Accuracy = 92.19%\n",
            "  Batch  42: Loss = 0.0945, Accuracy = 96.88%\n",
            "  Batch  43: Loss = 0.1295, Accuracy = 95.31%\n",
            "  Batch  44: Loss = 0.0757, Accuracy = 98.44%\n",
            "  Batch  45: Loss = 0.0757, Accuracy = 98.44%\n",
            "  Batch  46: Loss = 0.1546, Accuracy = 95.31%\n",
            "  Batch  47: Loss = 0.1916, Accuracy = 87.50%\n",
            "  Batch  48: Loss = 0.0894, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.1410, Accuracy = 95.31%\n",
            "  Batch  50: Loss = 0.0837, Accuracy = 93.75%\n",
            "  Batch  51: Loss = 0.1055, Accuracy = 96.88%\n",
            "  Batch  52: Loss = 0.0889, Accuracy = 95.31%\n",
            "  Batch  53: Loss = 0.0187, Accuracy = 100.00%\n",
            "  Batch  54: Loss = 0.0745, Accuracy = 96.88%\n",
            "  Batch  55: Loss = 0.2017, Accuracy = 95.31%\n",
            "  Batch  56: Loss = 0.1228, Accuracy = 95.31%\n",
            "  Batch  57: Loss = 0.1050, Accuracy = 93.75%\n",
            "  Batch  58: Loss = 0.2206, Accuracy = 89.06%\n",
            "  Batch  59: Loss = 0.1196, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.0880, Accuracy = 98.44%\n",
            "  Batch  61: Loss = 0.0595, Accuracy = 95.31%\n",
            "  Batch  62: Loss = 0.1623, Accuracy = 93.75%\n",
            "  Batch  63: Loss = 0.0682, Accuracy = 96.88%\n",
            "  Batch  64: Loss = 0.0950, Accuracy = 98.44%\n",
            "  Batch  65: Loss = 0.0576, Accuracy = 96.88%\n",
            "  Batch  66: Loss = 0.1109, Accuracy = 96.88%\n",
            "  Batch  67: Loss = 0.2282, Accuracy = 93.75%\n",
            "  Batch  68: Loss = 0.0396, Accuracy = 96.88%\n",
            "  Batch  69: Loss = 0.1465, Accuracy = 96.88%\n",
            "  Batch  70: Loss = 0.1359, Accuracy = 96.88%\n",
            "  Batch  71: Loss = 0.0879, Accuracy = 93.75%\n",
            "  Batch  72: Loss = 0.0431, Accuracy = 98.44%\n",
            "  Batch  73: Loss = 0.3359, Accuracy = 89.06%\n",
            "  Batch  74: Loss = 0.1127, Accuracy = 93.75%\n",
            "  Batch  75: Loss = 0.1307, Accuracy = 96.88%\n",
            "  Batch  76: Loss = 0.0825, Accuracy = 96.88%\n",
            "  Batch  77: Loss = 0.1195, Accuracy = 95.31%\n",
            "  Batch  78: Loss = 0.1063, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.1466, Accuracy = 96.88%\n",
            "  Batch  80: Loss = 0.2141, Accuracy = 95.31%\n",
            "  Batch  81: Loss = 0.0648, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.1645, Accuracy = 93.75%\n",
            "  Batch  83: Loss = 0.1828, Accuracy = 95.31%\n",
            "  Batch  84: Loss = 0.1567, Accuracy = 92.19%\n",
            "  Batch  85: Loss = 0.1223, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.0427, Accuracy = 100.00%\n",
            "  Batch  87: Loss = 0.2622, Accuracy = 89.06%\n",
            "  Batch  88: Loss = 0.0718, Accuracy = 96.88%\n",
            "  Batch  89: Loss = 0.1271, Accuracy = 95.31%\n",
            "  Batch  90: Loss = 0.1172, Accuracy = 93.75%\n",
            "  Batch  91: Loss = 0.0591, Accuracy = 96.88%\n",
            "  Batch  92: Loss = 0.1465, Accuracy = 95.31%\n",
            "  Batch  93: Loss = 0.1577, Accuracy = 90.62%\n",
            "  Batch  94: Loss = 0.0866, Accuracy = 92.19%\n",
            "  Batch  95: Loss = 0.1601, Accuracy = 95.31%\n",
            "  Batch  96: Loss = 0.0867, Accuracy = 95.31%\n",
            "  Batch  97: Loss = 0.1020, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.0754, Accuracy = 96.88%\n",
            "  Batch  99: Loss = 0.0943, Accuracy = 98.44%\n",
            "  Batch 100: Loss = 0.0800, Accuracy = 95.31%\n",
            "  Batch 101: Loss = 0.2119, Accuracy = 93.75%\n",
            "  Batch 102: Loss = 0.0458, Accuracy = 98.44%\n",
            "  Batch 103: Loss = 0.1374, Accuracy = 93.75%\n",
            "  Batch 104: Loss = 0.1969, Accuracy = 95.31%\n",
            "  Batch 105: Loss = 0.1547, Accuracy = 96.88%\n",
            "  Batch 106: Loss = 0.0542, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.1591, Accuracy = 93.75%\n",
            "  Batch 108: Loss = 0.0795, Accuracy = 96.88%\n",
            "  Batch 109: Loss = 0.0380, Accuracy = 100.00%\n",
            "  Batch 110: Loss = 0.1802, Accuracy = 92.19%\n",
            "  Batch 111: Loss = 0.0501, Accuracy = 98.44%\n",
            "  Batch 112: Loss = 0.1428, Accuracy = 96.88%\n",
            "  Batch 113: Loss = 0.0209, Accuracy = 100.00%\n",
            "  Batch 114: Loss = 0.0883, Accuracy = 96.88%\n",
            "  Batch 115: Loss = 0.1674, Accuracy = 96.88%\n",
            "  Batch 116: Loss = 0.1458, Accuracy = 93.75%\n",
            "  Batch 117: Loss = 0.1313, Accuracy = 93.75%\n",
            "  Batch 118: Loss = 0.0504, Accuracy = 98.44%\n",
            "  Batch 119: Loss = 0.0975, Accuracy = 96.88%\n",
            "  Batch 120: Loss = 0.0445, Accuracy = 98.44%\n",
            "  Batch 121: Loss = 0.0460, Accuracy = 98.44%\n",
            "  Batch 122: Loss = 0.0511, Accuracy = 96.88%\n",
            "  Batch 123: Loss = 0.0691, Accuracy = 98.44%\n",
            "  Batch 124: Loss = 0.0296, Accuracy = 100.00%\n",
            "  Batch 125: Loss = 0.1808, Accuracy = 93.75%\n",
            "  Batch 126: Loss = 0.0532, Accuracy = 98.44%\n",
            "  Batch 127: Loss = 0.2139, Accuracy = 96.88%\n",
            "  Batch 128: Loss = 0.0589, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.0697, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.0392, Accuracy = 100.00%\n",
            "  Batch 131: Loss = 0.0024, Accuracy = 100.00%\n",
            "==> Epoch 65 Summary: Total Loss = 14.9101, Accuracy = 96.00%\n",
            "\n",
            "Epoch 66\n",
            "  Batch   1: Loss = 0.0490, Accuracy = 98.44%\n",
            "  Batch   2: Loss = 0.1277, Accuracy = 93.75%\n",
            "  Batch   3: Loss = 0.0478, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0803, Accuracy = 98.44%\n",
            "  Batch   5: Loss = 0.0629, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.0179, Accuracy = 100.00%\n",
            "  Batch   7: Loss = 0.0821, Accuracy = 98.44%\n",
            "  Batch   8: Loss = 0.0610, Accuracy = 98.44%\n",
            "  Batch   9: Loss = 0.0609, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.0260, Accuracy = 100.00%\n",
            "  Batch  11: Loss = 0.1133, Accuracy = 95.31%\n",
            "  Batch  12: Loss = 0.0412, Accuracy = 98.44%\n",
            "  Batch  13: Loss = 0.0829, Accuracy = 98.44%\n",
            "  Batch  14: Loss = 0.0773, Accuracy = 96.88%\n",
            "  Batch  15: Loss = 0.0378, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.0935, Accuracy = 96.88%\n",
            "  Batch  17: Loss = 0.2219, Accuracy = 93.75%\n",
            "  Batch  18: Loss = 0.1374, Accuracy = 93.75%\n",
            "  Batch  19: Loss = 0.1348, Accuracy = 93.75%\n",
            "  Batch  20: Loss = 0.0202, Accuracy = 98.44%\n",
            "  Batch  21: Loss = 0.0172, Accuracy = 100.00%\n",
            "  Batch  22: Loss = 0.0743, Accuracy = 96.88%\n",
            "  Batch  23: Loss = 0.0258, Accuracy = 100.00%\n",
            "  Batch  24: Loss = 0.0233, Accuracy = 100.00%\n",
            "  Batch  25: Loss = 0.1440, Accuracy = 93.75%\n",
            "  Batch  26: Loss = 0.1256, Accuracy = 96.88%\n",
            "  Batch  27: Loss = 0.0702, Accuracy = 100.00%\n",
            "  Batch  28: Loss = 0.1244, Accuracy = 95.31%\n",
            "  Batch  29: Loss = 0.1249, Accuracy = 93.75%\n",
            "  Batch  30: Loss = 0.1260, Accuracy = 93.75%\n",
            "  Batch  31: Loss = 0.0741, Accuracy = 96.88%\n",
            "  Batch  32: Loss = 0.0697, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.1026, Accuracy = 96.88%\n",
            "  Batch  34: Loss = 0.1474, Accuracy = 95.31%\n",
            "  Batch  35: Loss = 0.1034, Accuracy = 98.44%\n",
            "  Batch  36: Loss = 0.0509, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.0507, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.0634, Accuracy = 98.44%\n",
            "  Batch  39: Loss = 0.0465, Accuracy = 98.44%\n",
            "  Batch  40: Loss = 0.0344, Accuracy = 98.44%\n",
            "  Batch  41: Loss = 0.0332, Accuracy = 100.00%\n",
            "  Batch  42: Loss = 0.0176, Accuracy = 100.00%\n",
            "  Batch  43: Loss = 0.0385, Accuracy = 98.44%\n",
            "  Batch  44: Loss = 0.0729, Accuracy = 95.31%\n",
            "  Batch  45: Loss = 0.0459, Accuracy = 98.44%\n",
            "  Batch  46: Loss = 0.0699, Accuracy = 98.44%\n",
            "  Batch  47: Loss = 0.0481, Accuracy = 98.44%\n",
            "  Batch  48: Loss = 0.1033, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.0768, Accuracy = 98.44%\n",
            "  Batch  50: Loss = 0.1635, Accuracy = 93.75%\n",
            "  Batch  51: Loss = 0.0809, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.0728, Accuracy = 96.88%\n",
            "  Batch  53: Loss = 0.0417, Accuracy = 98.44%\n",
            "  Batch  54: Loss = 0.0878, Accuracy = 98.44%\n",
            "  Batch  55: Loss = 0.1803, Accuracy = 95.31%\n",
            "  Batch  56: Loss = 0.2052, Accuracy = 90.62%\n",
            "  Batch  57: Loss = 0.0924, Accuracy = 98.44%\n",
            "  Batch  58: Loss = 0.0520, Accuracy = 98.44%\n",
            "  Batch  59: Loss = 0.0469, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.1110, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.0571, Accuracy = 98.44%\n",
            "  Batch  62: Loss = 0.0195, Accuracy = 100.00%\n",
            "  Batch  63: Loss = 0.0884, Accuracy = 98.44%\n",
            "  Batch  64: Loss = 0.2097, Accuracy = 90.62%\n",
            "  Batch  65: Loss = 0.0703, Accuracy = 98.44%\n",
            "  Batch  66: Loss = 0.0713, Accuracy = 95.31%\n",
            "  Batch  67: Loss = 0.0419, Accuracy = 98.44%\n",
            "  Batch  68: Loss = 0.0968, Accuracy = 96.88%\n",
            "  Batch  69: Loss = 0.0507, Accuracy = 96.88%\n",
            "  Batch  70: Loss = 0.0479, Accuracy = 98.44%\n",
            "  Batch  71: Loss = 0.0212, Accuracy = 100.00%\n",
            "  Batch  72: Loss = 0.0103, Accuracy = 100.00%\n",
            "  Batch  73: Loss = 0.1973, Accuracy = 96.88%\n",
            "  Batch  74: Loss = 0.0480, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.2367, Accuracy = 93.75%\n",
            "  Batch  76: Loss = 0.0551, Accuracy = 98.44%\n",
            "  Batch  77: Loss = 0.0631, Accuracy = 98.44%\n",
            "  Batch  78: Loss = 0.1239, Accuracy = 93.75%\n",
            "  Batch  79: Loss = 0.0356, Accuracy = 98.44%\n",
            "  Batch  80: Loss = 0.1094, Accuracy = 96.88%\n",
            "  Batch  81: Loss = 0.2006, Accuracy = 93.75%\n",
            "  Batch  82: Loss = 0.0683, Accuracy = 98.44%\n",
            "  Batch  83: Loss = 0.0374, Accuracy = 98.44%\n",
            "  Batch  84: Loss = 0.0346, Accuracy = 98.44%\n",
            "  Batch  85: Loss = 0.0392, Accuracy = 100.00%\n",
            "  Batch  86: Loss = 0.1415, Accuracy = 93.75%\n",
            "  Batch  87: Loss = 0.0234, Accuracy = 100.00%\n",
            "  Batch  88: Loss = 0.0969, Accuracy = 95.31%\n",
            "  Batch  89: Loss = 0.0133, Accuracy = 100.00%\n",
            "  Batch  90: Loss = 0.1165, Accuracy = 93.75%\n",
            "  Batch  91: Loss = 0.2352, Accuracy = 95.31%\n",
            "  Batch  92: Loss = 0.1267, Accuracy = 98.44%\n",
            "  Batch  93: Loss = 0.1773, Accuracy = 92.19%\n",
            "  Batch  94: Loss = 0.0930, Accuracy = 96.88%\n",
            "  Batch  95: Loss = 0.0759, Accuracy = 93.75%\n",
            "  Batch  96: Loss = 0.1440, Accuracy = 95.31%\n",
            "  Batch  97: Loss = 0.0284, Accuracy = 98.44%\n",
            "  Batch  98: Loss = 0.1551, Accuracy = 95.31%\n",
            "  Batch  99: Loss = 0.1527, Accuracy = 93.75%\n",
            "  Batch 100: Loss = 0.0862, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.2179, Accuracy = 95.31%\n",
            "  Batch 102: Loss = 0.0988, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.0691, Accuracy = 98.44%\n",
            "  Batch 104: Loss = 0.1326, Accuracy = 92.19%\n",
            "  Batch 105: Loss = 0.0511, Accuracy = 98.44%\n",
            "  Batch 106: Loss = 0.0208, Accuracy = 100.00%\n",
            "  Batch 107: Loss = 0.0641, Accuracy = 96.88%\n",
            "  Batch 108: Loss = 0.0793, Accuracy = 96.88%\n",
            "  Batch 109: Loss = 0.1309, Accuracy = 92.19%\n",
            "  Batch 110: Loss = 0.1217, Accuracy = 95.31%\n",
            "  Batch 111: Loss = 0.2581, Accuracy = 90.62%\n",
            "  Batch 112: Loss = 0.0241, Accuracy = 100.00%\n",
            "  Batch 113: Loss = 0.0160, Accuracy = 100.00%\n",
            "  Batch 114: Loss = 0.0412, Accuracy = 98.44%\n",
            "  Batch 115: Loss = 0.0876, Accuracy = 96.88%\n",
            "  Batch 116: Loss = 0.1257, Accuracy = 96.88%\n",
            "  Batch 117: Loss = 0.0643, Accuracy = 96.88%\n",
            "  Batch 118: Loss = 0.0753, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.0498, Accuracy = 100.00%\n",
            "  Batch 120: Loss = 0.1898, Accuracy = 98.44%\n",
            "  Batch 121: Loss = 0.0850, Accuracy = 96.88%\n",
            "  Batch 122: Loss = 0.2059, Accuracy = 92.19%\n",
            "  Batch 123: Loss = 0.0835, Accuracy = 96.88%\n",
            "  Batch 124: Loss = 0.1324, Accuracy = 96.88%\n",
            "  Batch 125: Loss = 0.1536, Accuracy = 93.75%\n",
            "  Batch 126: Loss = 0.0134, Accuracy = 100.00%\n",
            "  Batch 127: Loss = 0.0688, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.1305, Accuracy = 93.75%\n",
            "  Batch 129: Loss = 0.1313, Accuracy = 96.88%\n",
            "  Batch 130: Loss = 0.1160, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0021, Accuracy = 100.00%\n",
            "==> Epoch 66 Summary: Total Loss = 11.5562, Accuracy = 97.06%\n",
            "\n",
            "Epoch 67\n",
            "  Batch   1: Loss = 0.0750, Accuracy = 95.31%\n",
            "  Batch   2: Loss = 0.0278, Accuracy = 100.00%\n",
            "  Batch   3: Loss = 0.0233, Accuracy = 100.00%\n",
            "  Batch   4: Loss = 0.0977, Accuracy = 93.75%\n",
            "  Batch   5: Loss = 0.0239, Accuracy = 100.00%\n",
            "  Batch   6: Loss = 0.0882, Accuracy = 96.88%\n",
            "  Batch   7: Loss = 0.1144, Accuracy = 96.88%\n",
            "  Batch   8: Loss = 0.0273, Accuracy = 100.00%\n",
            "  Batch   9: Loss = 0.0419, Accuracy = 100.00%\n",
            "  Batch  10: Loss = 0.0226, Accuracy = 100.00%\n",
            "  Batch  11: Loss = 0.1391, Accuracy = 96.88%\n",
            "  Batch  12: Loss = 0.0897, Accuracy = 93.75%\n",
            "  Batch  13: Loss = 0.0692, Accuracy = 95.31%\n",
            "  Batch  14: Loss = 0.0680, Accuracy = 96.88%\n",
            "  Batch  15: Loss = 0.0429, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.1228, Accuracy = 98.44%\n",
            "  Batch  17: Loss = 0.1513, Accuracy = 95.31%\n",
            "  Batch  18: Loss = 0.2172, Accuracy = 93.75%\n",
            "  Batch  19: Loss = 0.0896, Accuracy = 95.31%\n",
            "  Batch  20: Loss = 0.0856, Accuracy = 98.44%\n",
            "  Batch  21: Loss = 0.1097, Accuracy = 95.31%\n",
            "  Batch  22: Loss = 0.0828, Accuracy = 96.88%\n",
            "  Batch  23: Loss = 0.0074, Accuracy = 100.00%\n",
            "  Batch  24: Loss = 0.0334, Accuracy = 100.00%\n",
            "  Batch  25: Loss = 0.1005, Accuracy = 96.88%\n",
            "  Batch  26: Loss = 0.0271, Accuracy = 100.00%\n",
            "  Batch  27: Loss = 0.0719, Accuracy = 98.44%\n",
            "  Batch  28: Loss = 0.0392, Accuracy = 98.44%\n",
            "  Batch  29: Loss = 0.0311, Accuracy = 98.44%\n",
            "  Batch  30: Loss = 0.0718, Accuracy = 98.44%\n",
            "  Batch  31: Loss = 0.0427, Accuracy = 98.44%\n",
            "  Batch  32: Loss = 0.1173, Accuracy = 95.31%\n",
            "  Batch  33: Loss = 0.0376, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.0831, Accuracy = 96.88%\n",
            "  Batch  35: Loss = 0.0577, Accuracy = 96.88%\n",
            "  Batch  36: Loss = 0.0433, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.0132, Accuracy = 100.00%\n",
            "  Batch  38: Loss = 0.0705, Accuracy = 96.88%\n",
            "  Batch  39: Loss = 0.0276, Accuracy = 100.00%\n",
            "  Batch  40: Loss = 0.1588, Accuracy = 95.31%\n",
            "  Batch  41: Loss = 0.0397, Accuracy = 100.00%\n",
            "  Batch  42: Loss = 0.1175, Accuracy = 93.75%\n",
            "  Batch  43: Loss = 0.0973, Accuracy = 96.88%\n",
            "  Batch  44: Loss = 0.2361, Accuracy = 93.75%\n",
            "  Batch  45: Loss = 0.0474, Accuracy = 98.44%\n",
            "  Batch  46: Loss = 0.1516, Accuracy = 92.19%\n",
            "  Batch  47: Loss = 0.0176, Accuracy = 100.00%\n",
            "  Batch  48: Loss = 0.1978, Accuracy = 89.06%\n",
            "  Batch  49: Loss = 0.1419, Accuracy = 93.75%\n",
            "  Batch  50: Loss = 0.0454, Accuracy = 96.88%\n",
            "  Batch  51: Loss = 0.0293, Accuracy = 100.00%\n",
            "  Batch  52: Loss = 0.2499, Accuracy = 95.31%\n",
            "  Batch  53: Loss = 0.0739, Accuracy = 95.31%\n",
            "  Batch  54: Loss = 0.0212, Accuracy = 100.00%\n",
            "  Batch  55: Loss = 0.0481, Accuracy = 98.44%\n",
            "  Batch  56: Loss = 0.0245, Accuracy = 100.00%\n",
            "  Batch  57: Loss = 0.0819, Accuracy = 96.88%\n",
            "  Batch  58: Loss = 0.0874, Accuracy = 98.44%\n",
            "  Batch  59: Loss = 0.1683, Accuracy = 95.31%\n",
            "  Batch  60: Loss = 0.0870, Accuracy = 95.31%\n",
            "  Batch  61: Loss = 0.0757, Accuracy = 95.31%\n",
            "  Batch  62: Loss = 0.0282, Accuracy = 98.44%\n",
            "  Batch  63: Loss = 0.0308, Accuracy = 98.44%\n",
            "  Batch  64: Loss = 0.1589, Accuracy = 95.31%\n",
            "  Batch  65: Loss = 0.1187, Accuracy = 93.75%\n",
            "  Batch  66: Loss = 0.1370, Accuracy = 95.31%\n",
            "  Batch  67: Loss = 0.0463, Accuracy = 98.44%\n",
            "  Batch  68: Loss = 0.1110, Accuracy = 95.31%\n",
            "  Batch  69: Loss = 0.1049, Accuracy = 95.31%\n",
            "  Batch  70: Loss = 0.1093, Accuracy = 96.88%\n",
            "  Batch  71: Loss = 0.1329, Accuracy = 95.31%\n",
            "  Batch  72: Loss = 0.0893, Accuracy = 93.75%\n",
            "  Batch  73: Loss = 0.0759, Accuracy = 98.44%\n",
            "  Batch  74: Loss = 0.1791, Accuracy = 96.88%\n",
            "  Batch  75: Loss = 0.0590, Accuracy = 98.44%\n",
            "  Batch  76: Loss = 0.0452, Accuracy = 96.88%\n",
            "  Batch  77: Loss = 0.0574, Accuracy = 98.44%\n",
            "  Batch  78: Loss = 0.1414, Accuracy = 95.31%\n",
            "  Batch  79: Loss = 0.1033, Accuracy = 95.31%\n",
            "  Batch  80: Loss = 0.1123, Accuracy = 96.88%\n",
            "  Batch  81: Loss = 0.0916, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.0656, Accuracy = 96.88%\n",
            "  Batch  83: Loss = 0.1872, Accuracy = 95.31%\n",
            "  Batch  84: Loss = 0.0833, Accuracy = 98.44%\n",
            "  Batch  85: Loss = 0.0740, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.1117, Accuracy = 96.88%\n",
            "  Batch  87: Loss = 0.1140, Accuracy = 95.31%\n",
            "  Batch  88: Loss = 0.0428, Accuracy = 98.44%\n",
            "  Batch  89: Loss = 0.0364, Accuracy = 98.44%\n",
            "  Batch  90: Loss = 0.1403, Accuracy = 93.75%\n",
            "  Batch  91: Loss = 0.0453, Accuracy = 98.44%\n",
            "  Batch  92: Loss = 0.1746, Accuracy = 95.31%\n",
            "  Batch  93: Loss = 0.0502, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.1085, Accuracy = 93.75%\n",
            "  Batch  95: Loss = 0.0743, Accuracy = 96.88%\n",
            "  Batch  96: Loss = 0.0852, Accuracy = 96.88%\n",
            "  Batch  97: Loss = 0.0181, Accuracy = 100.00%\n",
            "  Batch  98: Loss = 0.0384, Accuracy = 100.00%\n",
            "  Batch  99: Loss = 0.1110, Accuracy = 98.44%\n",
            "  Batch 100: Loss = 0.0867, Accuracy = 98.44%\n",
            "  Batch 101: Loss = 0.1379, Accuracy = 96.88%\n",
            "  Batch 102: Loss = 0.1238, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.1136, Accuracy = 93.75%\n",
            "  Batch 104: Loss = 0.2453, Accuracy = 90.62%\n",
            "  Batch 105: Loss = 0.0757, Accuracy = 96.88%\n",
            "  Batch 106: Loss = 0.1118, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.1518, Accuracy = 96.88%\n",
            "  Batch 108: Loss = 0.1322, Accuracy = 98.44%\n",
            "  Batch 109: Loss = 0.0806, Accuracy = 98.44%\n",
            "  Batch 110: Loss = 0.0950, Accuracy = 95.31%\n",
            "  Batch 111: Loss = 0.1378, Accuracy = 95.31%\n",
            "  Batch 112: Loss = 0.0129, Accuracy = 100.00%\n",
            "  Batch 113: Loss = 0.0511, Accuracy = 98.44%\n",
            "  Batch 114: Loss = 0.1982, Accuracy = 92.19%\n",
            "  Batch 115: Loss = 0.1965, Accuracy = 95.31%\n",
            "  Batch 116: Loss = 0.2089, Accuracy = 93.75%\n",
            "  Batch 117: Loss = 0.1002, Accuracy = 95.31%\n",
            "  Batch 118: Loss = 0.1233, Accuracy = 93.75%\n",
            "  Batch 119: Loss = 0.1091, Accuracy = 96.88%\n",
            "  Batch 120: Loss = 0.0606, Accuracy = 96.88%\n",
            "  Batch 121: Loss = 0.1110, Accuracy = 96.88%\n",
            "  Batch 122: Loss = 0.0986, Accuracy = 93.75%\n",
            "  Batch 123: Loss = 0.0197, Accuracy = 100.00%\n",
            "  Batch 124: Loss = 0.1407, Accuracy = 92.19%\n",
            "  Batch 125: Loss = 0.1562, Accuracy = 96.88%\n",
            "  Batch 126: Loss = 0.0796, Accuracy = 96.88%\n",
            "  Batch 127: Loss = 0.1921, Accuracy = 93.75%\n",
            "  Batch 128: Loss = 0.1367, Accuracy = 93.75%\n",
            "  Batch 129: Loss = 0.0833, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.1370, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.1085, Accuracy = 100.00%\n",
            "==> Epoch 67 Summary: Total Loss = 12.1934, Accuracy = 96.83%\n",
            "\n",
            "Epoch 68\n",
            "  Batch   1: Loss = 0.0239, Accuracy = 100.00%\n",
            "  Batch   2: Loss = 0.0638, Accuracy = 96.88%\n",
            "  Batch   3: Loss = 0.1964, Accuracy = 93.75%\n",
            "  Batch   4: Loss = 0.0405, Accuracy = 100.00%\n",
            "  Batch   5: Loss = 0.1723, Accuracy = 92.19%\n",
            "  Batch   6: Loss = 0.0981, Accuracy = 95.31%\n",
            "  Batch   7: Loss = 0.1601, Accuracy = 95.31%\n",
            "  Batch   8: Loss = 0.1148, Accuracy = 93.75%\n",
            "  Batch   9: Loss = 0.1498, Accuracy = 95.31%\n",
            "  Batch  10: Loss = 0.0659, Accuracy = 98.44%\n",
            "  Batch  11: Loss = 0.0545, Accuracy = 98.44%\n",
            "  Batch  12: Loss = 0.0643, Accuracy = 96.88%\n",
            "  Batch  13: Loss = 0.0660, Accuracy = 98.44%\n",
            "  Batch  14: Loss = 0.1378, Accuracy = 95.31%\n",
            "  Batch  15: Loss = 0.0927, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.0633, Accuracy = 98.44%\n",
            "  Batch  17: Loss = 0.0599, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.1600, Accuracy = 93.75%\n",
            "  Batch  19: Loss = 0.0751, Accuracy = 98.44%\n",
            "  Batch  20: Loss = 0.2289, Accuracy = 96.88%\n",
            "  Batch  21: Loss = 0.0552, Accuracy = 96.88%\n",
            "  Batch  22: Loss = 0.3481, Accuracy = 92.19%\n",
            "  Batch  23: Loss = 0.0188, Accuracy = 100.00%\n",
            "  Batch  24: Loss = 0.0912, Accuracy = 95.31%\n",
            "  Batch  25: Loss = 0.1198, Accuracy = 95.31%\n",
            "  Batch  26: Loss = 0.1409, Accuracy = 96.88%\n",
            "  Batch  27: Loss = 0.0544, Accuracy = 98.44%\n",
            "  Batch  28: Loss = 0.1408, Accuracy = 95.31%\n",
            "  Batch  29: Loss = 0.0861, Accuracy = 98.44%\n",
            "  Batch  30: Loss = 0.0590, Accuracy = 96.88%\n",
            "  Batch  31: Loss = 0.1445, Accuracy = 95.31%\n",
            "  Batch  32: Loss = 0.1095, Accuracy = 96.88%\n",
            "  Batch  33: Loss = 0.1235, Accuracy = 96.88%\n",
            "  Batch  34: Loss = 0.1755, Accuracy = 93.75%\n",
            "  Batch  35: Loss = 0.2024, Accuracy = 93.75%\n",
            "  Batch  36: Loss = 0.0401, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.0778, Accuracy = 96.88%\n",
            "  Batch  38: Loss = 0.1549, Accuracy = 95.31%\n",
            "  Batch  39: Loss = 0.1164, Accuracy = 95.31%\n",
            "  Batch  40: Loss = 0.1506, Accuracy = 96.88%\n",
            "  Batch  41: Loss = 0.0437, Accuracy = 98.44%\n",
            "  Batch  42: Loss = 0.1316, Accuracy = 90.62%\n",
            "  Batch  43: Loss = 0.0847, Accuracy = 96.88%\n",
            "  Batch  44: Loss = 0.2379, Accuracy = 89.06%\n",
            "  Batch  45: Loss = 0.2057, Accuracy = 93.75%\n",
            "  Batch  46: Loss = 0.0974, Accuracy = 96.88%\n",
            "  Batch  47: Loss = 0.0615, Accuracy = 96.88%\n",
            "  Batch  48: Loss = 0.1047, Accuracy = 95.31%\n",
            "  Batch  49: Loss = 0.0588, Accuracy = 98.44%\n",
            "  Batch  50: Loss = 0.0506, Accuracy = 98.44%\n",
            "  Batch  51: Loss = 0.0945, Accuracy = 96.88%\n",
            "  Batch  52: Loss = 0.1022, Accuracy = 98.44%\n",
            "  Batch  53: Loss = 0.1376, Accuracy = 93.75%\n",
            "  Batch  54: Loss = 0.0980, Accuracy = 95.31%\n",
            "  Batch  55: Loss = 0.2321, Accuracy = 93.75%\n",
            "  Batch  56: Loss = 0.0450, Accuracy = 98.44%\n",
            "  Batch  57: Loss = 0.0382, Accuracy = 98.44%\n",
            "  Batch  58: Loss = 0.0804, Accuracy = 98.44%\n",
            "  Batch  59: Loss = 0.1190, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.1021, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.1903, Accuracy = 92.19%\n",
            "  Batch  62: Loss = 0.0427, Accuracy = 98.44%\n",
            "  Batch  63: Loss = 0.0826, Accuracy = 96.88%\n",
            "  Batch  64: Loss = 0.1654, Accuracy = 95.31%\n",
            "  Batch  65: Loss = 0.0839, Accuracy = 96.88%\n",
            "  Batch  66: Loss = 0.0963, Accuracy = 96.88%\n",
            "  Batch  67: Loss = 0.1085, Accuracy = 95.31%\n",
            "  Batch  68: Loss = 0.1725, Accuracy = 95.31%\n",
            "  Batch  69: Loss = 0.0479, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.0811, Accuracy = 96.88%\n",
            "  Batch  71: Loss = 0.0627, Accuracy = 98.44%\n",
            "  Batch  72: Loss = 0.0285, Accuracy = 100.00%\n",
            "  Batch  73: Loss = 0.1411, Accuracy = 95.31%\n",
            "  Batch  74: Loss = 0.1119, Accuracy = 96.88%\n",
            "  Batch  75: Loss = 0.1618, Accuracy = 95.31%\n",
            "  Batch  76: Loss = 0.1991, Accuracy = 93.75%\n",
            "  Batch  77: Loss = 0.1164, Accuracy = 95.31%\n",
            "  Batch  78: Loss = 0.0737, Accuracy = 96.88%\n",
            "  Batch  79: Loss = 0.0358, Accuracy = 100.00%\n",
            "  Batch  80: Loss = 0.1754, Accuracy = 93.75%\n",
            "  Batch  81: Loss = 0.0648, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.0597, Accuracy = 98.44%\n",
            "  Batch  83: Loss = 0.0744, Accuracy = 98.44%\n",
            "  Batch  84: Loss = 0.0391, Accuracy = 96.88%\n",
            "  Batch  85: Loss = 0.1888, Accuracy = 92.19%\n",
            "  Batch  86: Loss = 0.0787, Accuracy = 96.88%\n",
            "  Batch  87: Loss = 0.1456, Accuracy = 93.75%\n",
            "  Batch  88: Loss = 0.0343, Accuracy = 98.44%\n",
            "  Batch  89: Loss = 0.0392, Accuracy = 98.44%\n",
            "  Batch  90: Loss = 0.1341, Accuracy = 92.19%\n",
            "  Batch  91: Loss = 0.0636, Accuracy = 96.88%\n",
            "  Batch  92: Loss = 0.0156, Accuracy = 100.00%\n",
            "  Batch  93: Loss = 0.1049, Accuracy = 96.88%\n",
            "  Batch  94: Loss = 0.0666, Accuracy = 98.44%\n",
            "  Batch  95: Loss = 0.0529, Accuracy = 98.44%\n",
            "  Batch  96: Loss = 0.0493, Accuracy = 96.88%\n",
            "  Batch  97: Loss = 0.0755, Accuracy = 98.44%\n",
            "  Batch  98: Loss = 0.1998, Accuracy = 95.31%\n",
            "  Batch  99: Loss = 0.0542, Accuracy = 98.44%\n",
            "  Batch 100: Loss = 0.0503, Accuracy = 98.44%\n",
            "  Batch 101: Loss = 0.1108, Accuracy = 95.31%\n",
            "  Batch 102: Loss = 0.0500, Accuracy = 98.44%\n",
            "  Batch 103: Loss = 0.0562, Accuracy = 98.44%\n",
            "  Batch 104: Loss = 0.2026, Accuracy = 95.31%\n",
            "  Batch 105: Loss = 0.1018, Accuracy = 95.31%\n",
            "  Batch 106: Loss = 0.0928, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.0900, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.0859, Accuracy = 95.31%\n",
            "  Batch 109: Loss = 0.1601, Accuracy = 93.75%\n",
            "  Batch 110: Loss = 0.0383, Accuracy = 98.44%\n",
            "  Batch 111: Loss = 0.0174, Accuracy = 100.00%\n",
            "  Batch 112: Loss = 0.0465, Accuracy = 98.44%\n",
            "  Batch 113: Loss = 0.0777, Accuracy = 96.88%\n",
            "  Batch 114: Loss = 0.0735, Accuracy = 96.88%\n",
            "  Batch 115: Loss = 0.0100, Accuracy = 100.00%\n",
            "  Batch 116: Loss = 0.0243, Accuracy = 100.00%\n",
            "  Batch 117: Loss = 0.1227, Accuracy = 95.31%\n",
            "  Batch 118: Loss = 0.0531, Accuracy = 95.31%\n",
            "  Batch 119: Loss = 0.1581, Accuracy = 95.31%\n",
            "  Batch 120: Loss = 0.0368, Accuracy = 100.00%\n",
            "  Batch 121: Loss = 0.0497, Accuracy = 98.44%\n",
            "  Batch 122: Loss = 0.0785, Accuracy = 98.44%\n",
            "  Batch 123: Loss = 0.0397, Accuracy = 96.88%\n",
            "  Batch 124: Loss = 0.1210, Accuracy = 95.31%\n",
            "  Batch 125: Loss = 0.0785, Accuracy = 93.75%\n",
            "  Batch 126: Loss = 0.0297, Accuracy = 100.00%\n",
            "  Batch 127: Loss = 0.0467, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0348, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.0631, Accuracy = 96.88%\n",
            "  Batch 130: Loss = 0.0124, Accuracy = 100.00%\n",
            "  Batch 131: Loss = 0.0169, Accuracy = 100.00%\n",
            "==> Epoch 68 Summary: Total Loss = 12.5719, Accuracy = 96.72%\n",
            "\n",
            "Epoch 69\n",
            "  Batch   1: Loss = 0.0921, Accuracy = 96.88%\n",
            "  Batch   2: Loss = 0.0484, Accuracy = 98.44%\n",
            "  Batch   3: Loss = 0.0965, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0770, Accuracy = 98.44%\n",
            "  Batch   5: Loss = 0.0817, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.0754, Accuracy = 96.88%\n",
            "  Batch   7: Loss = 0.0507, Accuracy = 96.88%\n",
            "  Batch   8: Loss = 0.0805, Accuracy = 98.44%\n",
            "  Batch   9: Loss = 0.0898, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.1147, Accuracy = 95.31%\n",
            "  Batch  11: Loss = 0.0563, Accuracy = 98.44%\n",
            "  Batch  12: Loss = 0.1831, Accuracy = 95.31%\n",
            "  Batch  13: Loss = 0.0899, Accuracy = 96.88%\n",
            "  Batch  14: Loss = 0.2094, Accuracy = 92.19%\n",
            "  Batch  15: Loss = 0.1460, Accuracy = 95.31%\n",
            "  Batch  16: Loss = 0.1046, Accuracy = 95.31%\n",
            "  Batch  17: Loss = 0.0940, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.1113, Accuracy = 96.88%\n",
            "  Batch  19: Loss = 0.1297, Accuracy = 93.75%\n",
            "  Batch  20: Loss = 0.3081, Accuracy = 89.06%\n",
            "  Batch  21: Loss = 0.1007, Accuracy = 95.31%\n",
            "  Batch  22: Loss = 0.0287, Accuracy = 100.00%\n",
            "  Batch  23: Loss = 0.0560, Accuracy = 98.44%\n",
            "  Batch  24: Loss = 0.0285, Accuracy = 98.44%\n",
            "  Batch  25: Loss = 0.0618, Accuracy = 98.44%\n",
            "  Batch  26: Loss = 0.0757, Accuracy = 95.31%\n",
            "  Batch  27: Loss = 0.2094, Accuracy = 95.31%\n",
            "  Batch  28: Loss = 0.1872, Accuracy = 95.31%\n",
            "  Batch  29: Loss = 0.0514, Accuracy = 98.44%\n",
            "  Batch  30: Loss = 0.1273, Accuracy = 96.88%\n",
            "  Batch  31: Loss = 0.0528, Accuracy = 96.88%\n",
            "  Batch  32: Loss = 0.1591, Accuracy = 93.75%\n",
            "  Batch  33: Loss = 0.0557, Accuracy = 96.88%\n",
            "  Batch  34: Loss = 0.0313, Accuracy = 100.00%\n",
            "  Batch  35: Loss = 0.2265, Accuracy = 95.31%\n",
            "  Batch  36: Loss = 0.1213, Accuracy = 92.19%\n",
            "  Batch  37: Loss = 0.1786, Accuracy = 95.31%\n",
            "  Batch  38: Loss = 0.1101, Accuracy = 98.44%\n",
            "  Batch  39: Loss = 0.1385, Accuracy = 96.88%\n",
            "  Batch  40: Loss = 0.0265, Accuracy = 100.00%\n",
            "  Batch  41: Loss = 0.0354, Accuracy = 98.44%\n",
            "  Batch  42: Loss = 0.1022, Accuracy = 96.88%\n",
            "  Batch  43: Loss = 0.0182, Accuracy = 100.00%\n",
            "  Batch  44: Loss = 0.0371, Accuracy = 98.44%\n",
            "  Batch  45: Loss = 0.0865, Accuracy = 98.44%\n",
            "  Batch  46: Loss = 0.0669, Accuracy = 96.88%\n",
            "  Batch  47: Loss = 0.0293, Accuracy = 100.00%\n",
            "  Batch  48: Loss = 0.0366, Accuracy = 100.00%\n",
            "  Batch  49: Loss = 0.0864, Accuracy = 98.44%\n",
            "  Batch  50: Loss = 0.0378, Accuracy = 98.44%\n",
            "  Batch  51: Loss = 0.0440, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.0599, Accuracy = 98.44%\n",
            "  Batch  53: Loss = 0.1003, Accuracy = 96.88%\n",
            "  Batch  54: Loss = 0.1062, Accuracy = 96.88%\n",
            "  Batch  55: Loss = 0.0659, Accuracy = 98.44%\n",
            "  Batch  56: Loss = 0.0962, Accuracy = 95.31%\n",
            "  Batch  57: Loss = 0.0465, Accuracy = 98.44%\n",
            "  Batch  58: Loss = 0.0212, Accuracy = 100.00%\n",
            "  Batch  59: Loss = 0.0277, Accuracy = 100.00%\n",
            "  Batch  60: Loss = 0.0645, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.0727, Accuracy = 98.44%\n",
            "  Batch  62: Loss = 0.1136, Accuracy = 96.88%\n",
            "  Batch  63: Loss = 0.0495, Accuracy = 98.44%\n",
            "  Batch  64: Loss = 0.1424, Accuracy = 92.19%\n",
            "  Batch  65: Loss = 0.0065, Accuracy = 100.00%\n",
            "  Batch  66: Loss = 0.1253, Accuracy = 95.31%\n",
            "  Batch  67: Loss = 0.0142, Accuracy = 100.00%\n",
            "  Batch  68: Loss = 0.0665, Accuracy = 96.88%\n",
            "  Batch  69: Loss = 0.1184, Accuracy = 96.88%\n",
            "  Batch  70: Loss = 0.0276, Accuracy = 100.00%\n",
            "  Batch  71: Loss = 0.0271, Accuracy = 100.00%\n",
            "  Batch  72: Loss = 0.0488, Accuracy = 98.44%\n",
            "  Batch  73: Loss = 0.0094, Accuracy = 100.00%\n",
            "  Batch  74: Loss = 0.0428, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.0097, Accuracy = 100.00%\n",
            "  Batch  76: Loss = 0.2591, Accuracy = 93.75%\n",
            "  Batch  77: Loss = 0.0336, Accuracy = 100.00%\n",
            "  Batch  78: Loss = 0.0429, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.0215, Accuracy = 98.44%\n",
            "  Batch  80: Loss = 0.0656, Accuracy = 96.88%\n",
            "  Batch  81: Loss = 0.0326, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.0492, Accuracy = 98.44%\n",
            "  Batch  83: Loss = 0.0662, Accuracy = 95.31%\n",
            "  Batch  84: Loss = 0.0219, Accuracy = 98.44%\n",
            "  Batch  85: Loss = 0.0868, Accuracy = 95.31%\n",
            "  Batch  86: Loss = 0.0722, Accuracy = 96.88%\n",
            "  Batch  87: Loss = 0.0368, Accuracy = 100.00%\n",
            "  Batch  88: Loss = 0.0537, Accuracy = 98.44%\n",
            "  Batch  89: Loss = 0.0233, Accuracy = 100.00%\n",
            "  Batch  90: Loss = 0.1327, Accuracy = 93.75%\n",
            "  Batch  91: Loss = 0.0240, Accuracy = 98.44%\n",
            "  Batch  92: Loss = 0.0557, Accuracy = 96.88%\n",
            "  Batch  93: Loss = 0.1863, Accuracy = 93.75%\n",
            "  Batch  94: Loss = 0.1032, Accuracy = 95.31%\n",
            "  Batch  95: Loss = 0.0401, Accuracy = 98.44%\n",
            "  Batch  96: Loss = 0.1265, Accuracy = 95.31%\n",
            "  Batch  97: Loss = 0.0507, Accuracy = 98.44%\n",
            "  Batch  98: Loss = 0.0679, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.1771, Accuracy = 93.75%\n",
            "  Batch 100: Loss = 0.0168, Accuracy = 100.00%\n",
            "  Batch 101: Loss = 0.0827, Accuracy = 96.88%\n",
            "  Batch 102: Loss = 0.0427, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.0710, Accuracy = 96.88%\n",
            "  Batch 104: Loss = 0.0711, Accuracy = 95.31%\n",
            "  Batch 105: Loss = 0.1056, Accuracy = 93.75%\n",
            "  Batch 106: Loss = 0.0570, Accuracy = 98.44%\n",
            "  Batch 107: Loss = 0.0691, Accuracy = 96.88%\n",
            "  Batch 108: Loss = 0.0908, Accuracy = 95.31%\n",
            "  Batch 109: Loss = 0.0084, Accuracy = 100.00%\n",
            "  Batch 110: Loss = 0.0818, Accuracy = 96.88%\n",
            "  Batch 111: Loss = 0.0585, Accuracy = 96.88%\n",
            "  Batch 112: Loss = 0.0988, Accuracy = 96.88%\n",
            "  Batch 113: Loss = 0.0990, Accuracy = 96.88%\n",
            "  Batch 114: Loss = 0.0178, Accuracy = 100.00%\n",
            "  Batch 115: Loss = 0.0511, Accuracy = 98.44%\n",
            "  Batch 116: Loss = 0.0796, Accuracy = 96.88%\n",
            "  Batch 117: Loss = 0.1700, Accuracy = 96.88%\n",
            "  Batch 118: Loss = 0.0501, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.2369, Accuracy = 93.75%\n",
            "  Batch 120: Loss = 0.1170, Accuracy = 96.88%\n",
            "  Batch 121: Loss = 0.1472, Accuracy = 95.31%\n",
            "  Batch 122: Loss = 0.0985, Accuracy = 98.44%\n",
            "  Batch 123: Loss = 0.1409, Accuracy = 96.88%\n",
            "  Batch 124: Loss = 0.0296, Accuracy = 100.00%\n",
            "  Batch 125: Loss = 0.1747, Accuracy = 95.31%\n",
            "  Batch 126: Loss = 0.0252, Accuracy = 98.44%\n",
            "  Batch 127: Loss = 0.0330, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.1021, Accuracy = 96.88%\n",
            "  Batch 129: Loss = 0.2107, Accuracy = 93.75%\n",
            "  Batch 130: Loss = 0.0757, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0120, Accuracy = 100.00%\n",
            "==> Epoch 69 Summary: Total Loss = 10.8640, Accuracy = 97.24%\n",
            "\n",
            "Epoch 70\n",
            "  Batch   1: Loss = 0.1096, Accuracy = 95.31%\n",
            "  Batch   2: Loss = 0.0819, Accuracy = 98.44%\n",
            "  Batch   3: Loss = 0.0550, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.1348, Accuracy = 93.75%\n",
            "  Batch   5: Loss = 0.2397, Accuracy = 92.19%\n",
            "  Batch   6: Loss = 0.0566, Accuracy = 96.88%\n",
            "  Batch   7: Loss = 0.0108, Accuracy = 100.00%\n",
            "  Batch   8: Loss = 0.1096, Accuracy = 95.31%\n",
            "  Batch   9: Loss = 0.0621, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.1029, Accuracy = 95.31%\n",
            "  Batch  11: Loss = 0.0758, Accuracy = 96.88%\n",
            "  Batch  12: Loss = 0.1069, Accuracy = 93.75%\n",
            "  Batch  13: Loss = 0.0718, Accuracy = 95.31%\n",
            "  Batch  14: Loss = 0.0888, Accuracy = 96.88%\n",
            "  Batch  15: Loss = 0.2782, Accuracy = 93.75%\n",
            "  Batch  16: Loss = 0.0840, Accuracy = 98.44%\n",
            "  Batch  17: Loss = 0.0629, Accuracy = 96.88%\n",
            "  Batch  18: Loss = 0.0836, Accuracy = 98.44%\n",
            "  Batch  19: Loss = 0.0447, Accuracy = 96.88%\n",
            "  Batch  20: Loss = 0.1177, Accuracy = 93.75%\n",
            "  Batch  21: Loss = 0.1101, Accuracy = 93.75%\n",
            "  Batch  22: Loss = 0.0668, Accuracy = 100.00%\n",
            "  Batch  23: Loss = 0.0491, Accuracy = 96.88%\n",
            "  Batch  24: Loss = 0.0754, Accuracy = 98.44%\n",
            "  Batch  25: Loss = 0.0899, Accuracy = 95.31%\n",
            "  Batch  26: Loss = 0.0606, Accuracy = 98.44%\n",
            "  Batch  27: Loss = 0.1579, Accuracy = 95.31%\n",
            "  Batch  28: Loss = 0.0406, Accuracy = 98.44%\n",
            "  Batch  29: Loss = 0.0209, Accuracy = 100.00%\n",
            "  Batch  30: Loss = 0.0609, Accuracy = 98.44%\n",
            "  Batch  31: Loss = 0.0778, Accuracy = 96.88%\n",
            "  Batch  32: Loss = 0.1043, Accuracy = 96.88%\n",
            "  Batch  33: Loss = 0.0467, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.0771, Accuracy = 98.44%\n",
            "  Batch  35: Loss = 0.0927, Accuracy = 96.88%\n",
            "  Batch  36: Loss = 0.0551, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.0477, Accuracy = 100.00%\n",
            "  Batch  38: Loss = 0.0988, Accuracy = 98.44%\n",
            "  Batch  39: Loss = 0.0521, Accuracy = 96.88%\n",
            "  Batch  40: Loss = 0.0671, Accuracy = 95.31%\n",
            "  Batch  41: Loss = 0.0851, Accuracy = 98.44%\n",
            "  Batch  42: Loss = 0.0372, Accuracy = 98.44%\n",
            "  Batch  43: Loss = 0.0541, Accuracy = 96.88%\n",
            "  Batch  44: Loss = 0.0493, Accuracy = 96.88%\n",
            "  Batch  45: Loss = 0.2174, Accuracy = 93.75%\n",
            "  Batch  46: Loss = 0.1167, Accuracy = 98.44%\n",
            "  Batch  47: Loss = 0.0678, Accuracy = 95.31%\n",
            "  Batch  48: Loss = 0.0876, Accuracy = 98.44%\n",
            "  Batch  49: Loss = 0.0954, Accuracy = 95.31%\n",
            "  Batch  50: Loss = 0.0927, Accuracy = 95.31%\n",
            "  Batch  51: Loss = 0.0522, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.0813, Accuracy = 95.31%\n",
            "  Batch  53: Loss = 0.1237, Accuracy = 96.88%\n",
            "  Batch  54: Loss = 0.0972, Accuracy = 95.31%\n",
            "  Batch  55: Loss = 0.1309, Accuracy = 96.88%\n",
            "  Batch  56: Loss = 0.0353, Accuracy = 100.00%\n",
            "  Batch  57: Loss = 0.1321, Accuracy = 96.88%\n",
            "  Batch  58: Loss = 0.1599, Accuracy = 95.31%\n",
            "  Batch  59: Loss = 0.0509, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.0226, Accuracy = 100.00%\n",
            "  Batch  61: Loss = 0.2351, Accuracy = 90.62%\n",
            "  Batch  62: Loss = 0.0993, Accuracy = 96.88%\n",
            "  Batch  63: Loss = 0.1205, Accuracy = 96.88%\n",
            "  Batch  64: Loss = 0.1099, Accuracy = 93.75%\n",
            "  Batch  65: Loss = 0.1521, Accuracy = 95.31%\n",
            "  Batch  66: Loss = 0.0879, Accuracy = 95.31%\n",
            "  Batch  67: Loss = 0.1381, Accuracy = 93.75%\n",
            "  Batch  68: Loss = 0.0835, Accuracy = 98.44%\n",
            "  Batch  69: Loss = 0.1046, Accuracy = 96.88%\n",
            "  Batch  70: Loss = 0.0305, Accuracy = 100.00%\n",
            "  Batch  71: Loss = 0.1818, Accuracy = 95.31%\n",
            "  Batch  72: Loss = 0.0585, Accuracy = 100.00%\n",
            "  Batch  73: Loss = 0.0344, Accuracy = 100.00%\n",
            "  Batch  74: Loss = 0.1287, Accuracy = 95.31%\n",
            "  Batch  75: Loss = 0.0651, Accuracy = 96.88%\n",
            "  Batch  76: Loss = 0.0560, Accuracy = 98.44%\n",
            "  Batch  77: Loss = 0.0474, Accuracy = 98.44%\n",
            "  Batch  78: Loss = 0.0426, Accuracy = 96.88%\n",
            "  Batch  79: Loss = 0.0339, Accuracy = 100.00%\n",
            "  Batch  80: Loss = 0.1579, Accuracy = 93.75%\n",
            "  Batch  81: Loss = 0.1270, Accuracy = 96.88%\n",
            "  Batch  82: Loss = 0.1867, Accuracy = 92.19%\n",
            "  Batch  83: Loss = 0.0527, Accuracy = 96.88%\n",
            "  Batch  84: Loss = 0.0868, Accuracy = 98.44%\n",
            "  Batch  85: Loss = 0.1763, Accuracy = 95.31%\n",
            "  Batch  86: Loss = 0.0775, Accuracy = 95.31%\n",
            "  Batch  87: Loss = 0.0457, Accuracy = 98.44%\n",
            "  Batch  88: Loss = 0.0514, Accuracy = 98.44%\n",
            "  Batch  89: Loss = 0.1215, Accuracy = 95.31%\n",
            "  Batch  90: Loss = 0.0603, Accuracy = 98.44%\n",
            "  Batch  91: Loss = 0.2148, Accuracy = 92.19%\n",
            "  Batch  92: Loss = 0.0753, Accuracy = 96.88%\n",
            "  Batch  93: Loss = 0.0437, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.0147, Accuracy = 100.00%\n",
            "  Batch  95: Loss = 0.0853, Accuracy = 96.88%\n",
            "  Batch  96: Loss = 0.0586, Accuracy = 96.88%\n",
            "  Batch  97: Loss = 0.0911, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.0603, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.0773, Accuracy = 98.44%\n",
            "  Batch 100: Loss = 0.0706, Accuracy = 98.44%\n",
            "  Batch 101: Loss = 0.1306, Accuracy = 95.31%\n",
            "  Batch 102: Loss = 0.0558, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.0324, Accuracy = 100.00%\n",
            "  Batch 104: Loss = 0.0406, Accuracy = 100.00%\n",
            "  Batch 105: Loss = 0.0355, Accuracy = 98.44%\n",
            "  Batch 106: Loss = 0.0818, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.0642, Accuracy = 95.31%\n",
            "  Batch 108: Loss = 0.0927, Accuracy = 98.44%\n",
            "  Batch 109: Loss = 0.0186, Accuracy = 100.00%\n",
            "  Batch 110: Loss = 0.0608, Accuracy = 96.88%\n",
            "  Batch 111: Loss = 0.0986, Accuracy = 95.31%\n",
            "  Batch 112: Loss = 0.0740, Accuracy = 98.44%\n",
            "  Batch 113: Loss = 0.1566, Accuracy = 95.31%\n",
            "  Batch 114: Loss = 0.1624, Accuracy = 95.31%\n",
            "  Batch 115: Loss = 0.0981, Accuracy = 96.88%\n",
            "  Batch 116: Loss = 0.0670, Accuracy = 96.88%\n",
            "  Batch 117: Loss = 0.0855, Accuracy = 98.44%\n",
            "  Batch 118: Loss = 0.0436, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.0065, Accuracy = 100.00%\n",
            "  Batch 120: Loss = 0.0178, Accuracy = 100.00%\n",
            "  Batch 121: Loss = 0.1186, Accuracy = 96.88%\n",
            "  Batch 122: Loss = 0.0274, Accuracy = 100.00%\n",
            "  Batch 123: Loss = 0.0743, Accuracy = 98.44%\n",
            "  Batch 124: Loss = 0.1130, Accuracy = 95.31%\n",
            "  Batch 125: Loss = 0.1271, Accuracy = 95.31%\n",
            "  Batch 126: Loss = 0.0551, Accuracy = 96.88%\n",
            "  Batch 127: Loss = 0.1059, Accuracy = 95.31%\n",
            "  Batch 128: Loss = 0.1321, Accuracy = 96.88%\n",
            "  Batch 129: Loss = 0.1521, Accuracy = 95.31%\n",
            "  Batch 130: Loss = 0.0904, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0007, Accuracy = 100.00%\n",
            "==> Epoch 70 Summary: Total Loss = 11.3833, Accuracy = 97.00%\n",
            "\n",
            "Epoch 71\n",
            "  Batch   1: Loss = 0.0851, Accuracy = 96.88%\n",
            "  Batch   2: Loss = 0.0768, Accuracy = 98.44%\n",
            "  Batch   3: Loss = 0.1249, Accuracy = 95.31%\n",
            "  Batch   4: Loss = 0.1697, Accuracy = 93.75%\n",
            "  Batch   5: Loss = 0.0811, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.0516, Accuracy = 98.44%\n",
            "  Batch   7: Loss = 0.0143, Accuracy = 100.00%\n",
            "  Batch   8: Loss = 0.0642, Accuracy = 98.44%\n",
            "  Batch   9: Loss = 0.1658, Accuracy = 93.75%\n",
            "  Batch  10: Loss = 0.1035, Accuracy = 96.88%\n",
            "  Batch  11: Loss = 0.0566, Accuracy = 98.44%\n",
            "  Batch  12: Loss = 0.0416, Accuracy = 98.44%\n",
            "  Batch  13: Loss = 0.1249, Accuracy = 96.88%\n",
            "  Batch  14: Loss = 0.1050, Accuracy = 96.88%\n",
            "  Batch  15: Loss = 0.0410, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.0258, Accuracy = 100.00%\n",
            "  Batch  17: Loss = 0.0512, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.0614, Accuracy = 96.88%\n",
            "  Batch  19: Loss = 0.0390, Accuracy = 100.00%\n",
            "  Batch  20: Loss = 0.1344, Accuracy = 92.19%\n",
            "  Batch  21: Loss = 0.2524, Accuracy = 93.75%\n",
            "  Batch  22: Loss = 0.0349, Accuracy = 100.00%\n",
            "  Batch  23: Loss = 0.0749, Accuracy = 95.31%\n",
            "  Batch  24: Loss = 0.1164, Accuracy = 96.88%\n",
            "  Batch  25: Loss = 0.0612, Accuracy = 96.88%\n",
            "  Batch  26: Loss = 0.0649, Accuracy = 98.44%\n",
            "  Batch  27: Loss = 0.1702, Accuracy = 93.75%\n",
            "  Batch  28: Loss = 0.0755, Accuracy = 98.44%\n",
            "  Batch  29: Loss = 0.1031, Accuracy = 96.88%\n",
            "  Batch  30: Loss = 0.0699, Accuracy = 98.44%\n",
            "  Batch  31: Loss = 0.0209, Accuracy = 100.00%\n",
            "  Batch  32: Loss = 0.1077, Accuracy = 96.88%\n",
            "  Batch  33: Loss = 0.0898, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.3109, Accuracy = 89.06%\n",
            "  Batch  35: Loss = 0.1101, Accuracy = 95.31%\n",
            "  Batch  36: Loss = 0.1242, Accuracy = 93.75%\n",
            "  Batch  37: Loss = 0.3028, Accuracy = 93.75%\n",
            "  Batch  38: Loss = 0.0925, Accuracy = 96.88%\n",
            "  Batch  39: Loss = 0.2377, Accuracy = 92.19%\n",
            "  Batch  40: Loss = 0.1036, Accuracy = 96.88%\n",
            "  Batch  41: Loss = 0.1057, Accuracy = 96.88%\n",
            "  Batch  42: Loss = 0.1024, Accuracy = 96.88%\n",
            "  Batch  43: Loss = 0.1270, Accuracy = 93.75%\n",
            "  Batch  44: Loss = 0.0384, Accuracy = 98.44%\n",
            "  Batch  45: Loss = 0.0717, Accuracy = 96.88%\n",
            "  Batch  46: Loss = 0.0657, Accuracy = 98.44%\n",
            "  Batch  47: Loss = 0.0902, Accuracy = 98.44%\n",
            "  Batch  48: Loss = 0.0759, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.0710, Accuracy = 96.88%\n",
            "  Batch  50: Loss = 0.0297, Accuracy = 100.00%\n",
            "  Batch  51: Loss = 0.1052, Accuracy = 96.88%\n",
            "  Batch  52: Loss = 0.1658, Accuracy = 92.19%\n",
            "  Batch  53: Loss = 0.0639, Accuracy = 96.88%\n",
            "  Batch  54: Loss = 0.0756, Accuracy = 95.31%\n",
            "  Batch  55: Loss = 0.0496, Accuracy = 100.00%\n",
            "  Batch  56: Loss = 0.1365, Accuracy = 96.88%\n",
            "  Batch  57: Loss = 0.0857, Accuracy = 95.31%\n",
            "  Batch  58: Loss = 0.0211, Accuracy = 100.00%\n",
            "  Batch  59: Loss = 0.1119, Accuracy = 95.31%\n",
            "  Batch  60: Loss = 0.0473, Accuracy = 98.44%\n",
            "  Batch  61: Loss = 0.0603, Accuracy = 98.44%\n",
            "  Batch  62: Loss = 0.1126, Accuracy = 96.88%\n",
            "  Batch  63: Loss = 0.1145, Accuracy = 96.88%\n",
            "  Batch  64: Loss = 0.0369, Accuracy = 98.44%\n",
            "  Batch  65: Loss = 0.0921, Accuracy = 96.88%\n",
            "  Batch  66: Loss = 0.1661, Accuracy = 95.31%\n",
            "  Batch  67: Loss = 0.0426, Accuracy = 98.44%\n",
            "  Batch  68: Loss = 0.0383, Accuracy = 98.44%\n",
            "  Batch  69: Loss = 0.1718, Accuracy = 93.75%\n",
            "  Batch  70: Loss = 0.1008, Accuracy = 96.88%\n",
            "  Batch  71: Loss = 0.0586, Accuracy = 98.44%\n",
            "  Batch  72: Loss = 0.0539, Accuracy = 96.88%\n",
            "  Batch  73: Loss = 0.0831, Accuracy = 96.88%\n",
            "  Batch  74: Loss = 0.0216, Accuracy = 100.00%\n",
            "  Batch  75: Loss = 0.0897, Accuracy = 92.19%\n",
            "  Batch  76: Loss = 0.0749, Accuracy = 96.88%\n",
            "  Batch  77: Loss = 0.0167, Accuracy = 100.00%\n",
            "  Batch  78: Loss = 0.1125, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.1510, Accuracy = 92.19%\n",
            "  Batch  80: Loss = 0.0319, Accuracy = 100.00%\n",
            "  Batch  81: Loss = 0.0601, Accuracy = 96.88%\n",
            "  Batch  82: Loss = 0.0727, Accuracy = 95.31%\n",
            "  Batch  83: Loss = 0.0298, Accuracy = 100.00%\n",
            "  Batch  84: Loss = 0.0525, Accuracy = 98.44%\n",
            "  Batch  85: Loss = 0.0607, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.0880, Accuracy = 95.31%\n",
            "  Batch  87: Loss = 0.0464, Accuracy = 98.44%\n",
            "  Batch  88: Loss = 0.0343, Accuracy = 98.44%\n",
            "  Batch  89: Loss = 0.0266, Accuracy = 98.44%\n",
            "  Batch  90: Loss = 0.1221, Accuracy = 96.88%\n",
            "  Batch  91: Loss = 0.0377, Accuracy = 100.00%\n",
            "  Batch  92: Loss = 0.0633, Accuracy = 98.44%\n",
            "  Batch  93: Loss = 0.0276, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.0577, Accuracy = 98.44%\n",
            "  Batch  95: Loss = 0.0587, Accuracy = 98.44%\n",
            "  Batch  96: Loss = 0.1367, Accuracy = 95.31%\n",
            "  Batch  97: Loss = 0.0458, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.1765, Accuracy = 93.75%\n",
            "  Batch  99: Loss = 0.1576, Accuracy = 93.75%\n",
            "  Batch 100: Loss = 0.0350, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.0580, Accuracy = 98.44%\n",
            "  Batch 102: Loss = 0.0927, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.0451, Accuracy = 100.00%\n",
            "  Batch 104: Loss = 0.1046, Accuracy = 96.88%\n",
            "  Batch 105: Loss = 0.1222, Accuracy = 95.31%\n",
            "  Batch 106: Loss = 0.1277, Accuracy = 93.75%\n",
            "  Batch 107: Loss = 0.1401, Accuracy = 96.88%\n",
            "  Batch 108: Loss = 0.0274, Accuracy = 100.00%\n",
            "  Batch 109: Loss = 0.1092, Accuracy = 96.88%\n",
            "  Batch 110: Loss = 0.0735, Accuracy = 96.88%\n",
            "  Batch 111: Loss = 0.0899, Accuracy = 95.31%\n",
            "  Batch 112: Loss = 0.0594, Accuracy = 96.88%\n",
            "  Batch 113: Loss = 0.0609, Accuracy = 96.88%\n",
            "  Batch 114: Loss = 0.0906, Accuracy = 95.31%\n",
            "  Batch 115: Loss = 0.1200, Accuracy = 96.88%\n",
            "  Batch 116: Loss = 0.1938, Accuracy = 95.31%\n",
            "  Batch 117: Loss = 0.2193, Accuracy = 92.19%\n",
            "  Batch 118: Loss = 0.1013, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.3297, Accuracy = 92.19%\n",
            "  Batch 120: Loss = 0.1196, Accuracy = 95.31%\n",
            "  Batch 121: Loss = 0.1733, Accuracy = 95.31%\n",
            "  Batch 122: Loss = 0.0560, Accuracy = 98.44%\n",
            "  Batch 123: Loss = 0.1831, Accuracy = 93.75%\n",
            "  Batch 124: Loss = 0.0941, Accuracy = 95.31%\n",
            "  Batch 125: Loss = 0.0667, Accuracy = 98.44%\n",
            "  Batch 126: Loss = 0.2389, Accuracy = 89.06%\n",
            "  Batch 127: Loss = 0.0750, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.1244, Accuracy = 96.88%\n",
            "  Batch 129: Loss = 0.1101, Accuracy = 95.31%\n",
            "  Batch 130: Loss = 0.0495, Accuracy = 100.00%\n",
            "  Batch 131: Loss = 0.0089, Accuracy = 100.00%\n",
            "==> Epoch 71 Summary: Total Loss = 12.2666, Accuracy = 96.79%\n",
            "\n",
            "Epoch 72\n",
            "  Batch   1: Loss = 0.1587, Accuracy = 95.31%\n",
            "  Batch   2: Loss = 0.0934, Accuracy = 95.31%\n",
            "  Batch   3: Loss = 0.0797, Accuracy = 96.88%\n",
            "  Batch   4: Loss = 0.1597, Accuracy = 92.19%\n",
            "  Batch   5: Loss = 0.0931, Accuracy = 93.75%\n",
            "  Batch   6: Loss = 0.0931, Accuracy = 95.31%\n",
            "  Batch   7: Loss = 0.0648, Accuracy = 96.88%\n",
            "  Batch   8: Loss = 0.0728, Accuracy = 96.88%\n",
            "  Batch   9: Loss = 0.0952, Accuracy = 96.88%\n",
            "  Batch  10: Loss = 0.3136, Accuracy = 90.62%\n",
            "  Batch  11: Loss = 0.0636, Accuracy = 96.88%\n",
            "  Batch  12: Loss = 0.0637, Accuracy = 98.44%\n",
            "  Batch  13: Loss = 0.2341, Accuracy = 93.75%\n",
            "  Batch  14: Loss = 0.0510, Accuracy = 98.44%\n",
            "  Batch  15: Loss = 0.1373, Accuracy = 95.31%\n",
            "  Batch  16: Loss = 0.0999, Accuracy = 98.44%\n",
            "  Batch  17: Loss = 0.1554, Accuracy = 92.19%\n",
            "  Batch  18: Loss = 0.0333, Accuracy = 98.44%\n",
            "  Batch  19: Loss = 0.0211, Accuracy = 100.00%\n",
            "  Batch  20: Loss = 0.0858, Accuracy = 95.31%\n",
            "  Batch  21: Loss = 0.1042, Accuracy = 98.44%\n",
            "  Batch  22: Loss = 0.1006, Accuracy = 95.31%\n",
            "  Batch  23: Loss = 0.2043, Accuracy = 92.19%\n",
            "  Batch  24: Loss = 0.2480, Accuracy = 95.31%\n",
            "  Batch  25: Loss = 0.1150, Accuracy = 95.31%\n",
            "  Batch  26: Loss = 0.0533, Accuracy = 96.88%\n",
            "  Batch  27: Loss = 0.1005, Accuracy = 96.88%\n",
            "  Batch  28: Loss = 0.0900, Accuracy = 96.88%\n",
            "  Batch  29: Loss = 0.0434, Accuracy = 98.44%\n",
            "  Batch  30: Loss = 0.1376, Accuracy = 98.44%\n",
            "  Batch  31: Loss = 0.1215, Accuracy = 93.75%\n",
            "  Batch  32: Loss = 0.1973, Accuracy = 92.19%\n",
            "  Batch  33: Loss = 0.1021, Accuracy = 95.31%\n",
            "  Batch  34: Loss = 0.1763, Accuracy = 95.31%\n",
            "  Batch  35: Loss = 0.0415, Accuracy = 98.44%\n",
            "  Batch  36: Loss = 0.1272, Accuracy = 96.88%\n",
            "  Batch  37: Loss = 0.1301, Accuracy = 96.88%\n",
            "  Batch  38: Loss = 0.1009, Accuracy = 96.88%\n",
            "  Batch  39: Loss = 0.0940, Accuracy = 96.88%\n",
            "  Batch  40: Loss = 0.1395, Accuracy = 93.75%\n",
            "  Batch  41: Loss = 0.2027, Accuracy = 92.19%\n",
            "  Batch  42: Loss = 0.0764, Accuracy = 98.44%\n",
            "  Batch  43: Loss = 0.0623, Accuracy = 98.44%\n",
            "  Batch  44: Loss = 0.1211, Accuracy = 95.31%\n",
            "  Batch  45: Loss = 0.1356, Accuracy = 96.88%\n",
            "  Batch  46: Loss = 0.0411, Accuracy = 98.44%\n",
            "  Batch  47: Loss = 0.1312, Accuracy = 95.31%\n",
            "  Batch  48: Loss = 0.0550, Accuracy = 98.44%\n",
            "  Batch  49: Loss = 0.1247, Accuracy = 96.88%\n",
            "  Batch  50: Loss = 0.1581, Accuracy = 92.19%\n",
            "  Batch  51: Loss = 0.0581, Accuracy = 96.88%\n",
            "  Batch  52: Loss = 0.0467, Accuracy = 98.44%\n",
            "  Batch  53: Loss = 0.0901, Accuracy = 95.31%\n",
            "  Batch  54: Loss = 0.0544, Accuracy = 98.44%\n",
            "  Batch  55: Loss = 0.0693, Accuracy = 98.44%\n",
            "  Batch  56: Loss = 0.0973, Accuracy = 95.31%\n",
            "  Batch  57: Loss = 0.1089, Accuracy = 93.75%\n",
            "  Batch  58: Loss = 0.1966, Accuracy = 95.31%\n",
            "  Batch  59: Loss = 0.0503, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.0975, Accuracy = 95.31%\n",
            "  Batch  61: Loss = 0.1252, Accuracy = 93.75%\n",
            "  Batch  62: Loss = 0.0557, Accuracy = 98.44%\n",
            "  Batch  63: Loss = 0.0270, Accuracy = 100.00%\n",
            "  Batch  64: Loss = 0.0517, Accuracy = 98.44%\n",
            "  Batch  65: Loss = 0.0270, Accuracy = 98.44%\n",
            "  Batch  66: Loss = 0.0321, Accuracy = 100.00%\n",
            "  Batch  67: Loss = 0.0868, Accuracy = 95.31%\n",
            "  Batch  68: Loss = 0.1458, Accuracy = 93.75%\n",
            "  Batch  69: Loss = 0.1885, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.0628, Accuracy = 96.88%\n",
            "  Batch  71: Loss = 0.0528, Accuracy = 98.44%\n",
            "  Batch  72: Loss = 0.0216, Accuracy = 100.00%\n",
            "  Batch  73: Loss = 0.0236, Accuracy = 100.00%\n",
            "  Batch  74: Loss = 0.0233, Accuracy = 100.00%\n",
            "  Batch  75: Loss = 0.0735, Accuracy = 98.44%\n",
            "  Batch  76: Loss = 0.0586, Accuracy = 98.44%\n",
            "  Batch  77: Loss = 0.1001, Accuracy = 93.75%\n",
            "  Batch  78: Loss = 0.1015, Accuracy = 96.88%\n",
            "  Batch  79: Loss = 0.0831, Accuracy = 96.88%\n",
            "  Batch  80: Loss = 0.1266, Accuracy = 93.75%\n",
            "  Batch  81: Loss = 0.2325, Accuracy = 93.75%\n",
            "  Batch  82: Loss = 0.0949, Accuracy = 96.88%\n",
            "  Batch  83: Loss = 0.2980, Accuracy = 90.62%\n",
            "  Batch  84: Loss = 0.0230, Accuracy = 100.00%\n",
            "  Batch  85: Loss = 0.0702, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.0644, Accuracy = 96.88%\n",
            "  Batch  87: Loss = 0.0609, Accuracy = 98.44%\n",
            "  Batch  88: Loss = 0.1312, Accuracy = 95.31%\n",
            "  Batch  89: Loss = 0.1342, Accuracy = 98.44%\n",
            "  Batch  90: Loss = 0.1118, Accuracy = 96.88%\n",
            "  Batch  91: Loss = 0.1932, Accuracy = 93.75%\n",
            "  Batch  92: Loss = 0.1387, Accuracy = 95.31%\n",
            "  Batch  93: Loss = 0.0908, Accuracy = 95.31%\n",
            "  Batch  94: Loss = 0.0709, Accuracy = 96.88%\n",
            "  Batch  95: Loss = 0.0375, Accuracy = 98.44%\n",
            "  Batch  96: Loss = 0.0801, Accuracy = 96.88%\n",
            "  Batch  97: Loss = 0.0527, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.1141, Accuracy = 93.75%\n",
            "  Batch  99: Loss = 0.0117, Accuracy = 100.00%\n",
            "  Batch 100: Loss = 0.0819, Accuracy = 95.31%\n",
            "  Batch 101: Loss = 0.0842, Accuracy = 95.31%\n",
            "  Batch 102: Loss = 0.0929, Accuracy = 95.31%\n",
            "  Batch 103: Loss = 0.0155, Accuracy = 100.00%\n",
            "  Batch 104: Loss = 0.0592, Accuracy = 98.44%\n",
            "  Batch 105: Loss = 0.1139, Accuracy = 95.31%\n",
            "  Batch 106: Loss = 0.0951, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.0558, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.2235, Accuracy = 93.75%\n",
            "  Batch 109: Loss = 0.1119, Accuracy = 98.44%\n",
            "  Batch 110: Loss = 0.1767, Accuracy = 96.88%\n",
            "  Batch 111: Loss = 0.0650, Accuracy = 96.88%\n",
            "  Batch 112: Loss = 0.0212, Accuracy = 100.00%\n",
            "  Batch 113: Loss = 0.0654, Accuracy = 98.44%\n",
            "  Batch 114: Loss = 0.0878, Accuracy = 96.88%\n",
            "  Batch 115: Loss = 0.2164, Accuracy = 93.75%\n",
            "  Batch 116: Loss = 0.1764, Accuracy = 92.19%\n",
            "  Batch 117: Loss = 0.0800, Accuracy = 98.44%\n",
            "  Batch 118: Loss = 0.0988, Accuracy = 93.75%\n",
            "  Batch 119: Loss = 0.1309, Accuracy = 95.31%\n",
            "  Batch 120: Loss = 0.0771, Accuracy = 95.31%\n",
            "  Batch 121: Loss = 0.0911, Accuracy = 98.44%\n",
            "  Batch 122: Loss = 0.1144, Accuracy = 95.31%\n",
            "  Batch 123: Loss = 0.0862, Accuracy = 96.88%\n",
            "  Batch 124: Loss = 0.0398, Accuracy = 98.44%\n",
            "  Batch 125: Loss = 0.1399, Accuracy = 92.19%\n",
            "  Batch 126: Loss = 0.1479, Accuracy = 95.31%\n",
            "  Batch 127: Loss = 0.2938, Accuracy = 92.19%\n",
            "  Batch 128: Loss = 0.0451, Accuracy = 100.00%\n",
            "  Batch 129: Loss = 0.0306, Accuracy = 100.00%\n",
            "  Batch 130: Loss = 0.0331, Accuracy = 100.00%\n",
            "  Batch 131: Loss = 0.0054, Accuracy = 100.00%\n",
            "==> Epoch 72 Summary: Total Loss = 13.2089, Accuracy = 96.47%\n",
            "\n",
            "Epoch 73\n",
            "  Batch   1: Loss = 0.0190, Accuracy = 100.00%\n",
            "  Batch   2: Loss = 0.0732, Accuracy = 95.31%\n",
            "  Batch   3: Loss = 0.0626, Accuracy = 96.88%\n",
            "  Batch   4: Loss = 0.0549, Accuracy = 98.44%\n",
            "  Batch   5: Loss = 0.0393, Accuracy = 98.44%\n",
            "  Batch   6: Loss = 0.0798, Accuracy = 98.44%\n",
            "  Batch   7: Loss = 0.1125, Accuracy = 96.88%\n",
            "  Batch   8: Loss = 0.1647, Accuracy = 96.88%\n",
            "  Batch   9: Loss = 0.1067, Accuracy = 96.88%\n",
            "  Batch  10: Loss = 0.0709, Accuracy = 95.31%\n",
            "  Batch  11: Loss = 0.1206, Accuracy = 95.31%\n",
            "  Batch  12: Loss = 0.0227, Accuracy = 100.00%\n",
            "  Batch  13: Loss = 0.0895, Accuracy = 95.31%\n",
            "  Batch  14: Loss = 0.0805, Accuracy = 98.44%\n",
            "  Batch  15: Loss = 0.0857, Accuracy = 96.88%\n",
            "  Batch  16: Loss = 0.0234, Accuracy = 100.00%\n",
            "  Batch  17: Loss = 0.0369, Accuracy = 100.00%\n",
            "  Batch  18: Loss = 0.0503, Accuracy = 98.44%\n",
            "  Batch  19: Loss = 0.0682, Accuracy = 96.88%\n",
            "  Batch  20: Loss = 0.1465, Accuracy = 96.88%\n",
            "  Batch  21: Loss = 0.1075, Accuracy = 93.75%\n",
            "  Batch  22: Loss = 0.0393, Accuracy = 96.88%\n",
            "  Batch  23: Loss = 0.0253, Accuracy = 98.44%\n",
            "  Batch  24: Loss = 0.0177, Accuracy = 100.00%\n",
            "  Batch  25: Loss = 0.0391, Accuracy = 98.44%\n",
            "  Batch  26: Loss = 0.0722, Accuracy = 96.88%\n",
            "  Batch  27: Loss = 0.1279, Accuracy = 95.31%\n",
            "  Batch  28: Loss = 0.0832, Accuracy = 96.88%\n",
            "  Batch  29: Loss = 0.0684, Accuracy = 96.88%\n",
            "  Batch  30: Loss = 0.1909, Accuracy = 92.19%\n",
            "  Batch  31: Loss = 0.0447, Accuracy = 98.44%\n",
            "  Batch  32: Loss = 0.0575, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.1511, Accuracy = 95.31%\n",
            "  Batch  34: Loss = 0.1200, Accuracy = 92.19%\n",
            "  Batch  35: Loss = 0.2127, Accuracy = 92.19%\n",
            "  Batch  36: Loss = 0.0312, Accuracy = 100.00%\n",
            "  Batch  37: Loss = 0.0606, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.0882, Accuracy = 98.44%\n",
            "  Batch  39: Loss = 0.0456, Accuracy = 98.44%\n",
            "  Batch  40: Loss = 0.1242, Accuracy = 95.31%\n",
            "  Batch  41: Loss = 0.0490, Accuracy = 96.88%\n",
            "  Batch  42: Loss = 0.0422, Accuracy = 98.44%\n",
            "  Batch  43: Loss = 0.0842, Accuracy = 98.44%\n",
            "  Batch  44: Loss = 0.1861, Accuracy = 90.62%\n",
            "  Batch  45: Loss = 0.1042, Accuracy = 96.88%\n",
            "  Batch  46: Loss = 0.2394, Accuracy = 95.31%\n",
            "  Batch  47: Loss = 0.0684, Accuracy = 95.31%\n",
            "  Batch  48: Loss = 0.1359, Accuracy = 95.31%\n",
            "  Batch  49: Loss = 0.1277, Accuracy = 95.31%\n",
            "  Batch  50: Loss = 0.1275, Accuracy = 95.31%\n",
            "  Batch  51: Loss = 0.0816, Accuracy = 95.31%\n",
            "  Batch  52: Loss = 0.0376, Accuracy = 98.44%\n",
            "  Batch  53: Loss = 0.0699, Accuracy = 96.88%\n",
            "  Batch  54: Loss = 0.0752, Accuracy = 96.88%\n",
            "  Batch  55: Loss = 0.0919, Accuracy = 96.88%\n",
            "  Batch  56: Loss = 0.0599, Accuracy = 98.44%\n",
            "  Batch  57: Loss = 0.0843, Accuracy = 98.44%\n",
            "  Batch  58: Loss = 0.0519, Accuracy = 96.88%\n",
            "  Batch  59: Loss = 0.0481, Accuracy = 96.88%\n",
            "  Batch  60: Loss = 0.1118, Accuracy = 95.31%\n",
            "  Batch  61: Loss = 0.1010, Accuracy = 95.31%\n",
            "  Batch  62: Loss = 0.2659, Accuracy = 92.19%\n",
            "  Batch  63: Loss = 0.0602, Accuracy = 96.88%\n",
            "  Batch  64: Loss = 0.1149, Accuracy = 95.31%\n",
            "  Batch  65: Loss = 0.0878, Accuracy = 95.31%\n",
            "  Batch  66: Loss = 0.0951, Accuracy = 96.88%\n",
            "  Batch  67: Loss = 0.0515, Accuracy = 98.44%\n",
            "  Batch  68: Loss = 0.0804, Accuracy = 96.88%\n",
            "  Batch  69: Loss = 0.2242, Accuracy = 89.06%\n",
            "  Batch  70: Loss = 0.1227, Accuracy = 92.19%\n",
            "  Batch  71: Loss = 0.0928, Accuracy = 95.31%\n",
            "  Batch  72: Loss = 0.1037, Accuracy = 95.31%\n",
            "  Batch  73: Loss = 0.0488, Accuracy = 98.44%\n",
            "  Batch  74: Loss = 0.1949, Accuracy = 90.62%\n",
            "  Batch  75: Loss = 0.0908, Accuracy = 98.44%\n",
            "  Batch  76: Loss = 0.1064, Accuracy = 95.31%\n",
            "  Batch  77: Loss = 0.0820, Accuracy = 96.88%\n",
            "  Batch  78: Loss = 0.2472, Accuracy = 93.75%\n",
            "  Batch  79: Loss = 0.0338, Accuracy = 98.44%\n",
            "  Batch  80: Loss = 0.0360, Accuracy = 98.44%\n",
            "  Batch  81: Loss = 0.1857, Accuracy = 95.31%\n",
            "  Batch  82: Loss = 0.0480, Accuracy = 96.88%\n",
            "  Batch  83: Loss = 0.1706, Accuracy = 93.75%\n",
            "  Batch  84: Loss = 0.1593, Accuracy = 95.31%\n",
            "  Batch  85: Loss = 0.1188, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.1696, Accuracy = 95.31%\n",
            "  Batch  87: Loss = 0.1865, Accuracy = 93.75%\n",
            "  Batch  88: Loss = 0.2024, Accuracy = 92.19%\n",
            "  Batch  89: Loss = 0.1032, Accuracy = 96.88%\n",
            "  Batch  90: Loss = 0.0287, Accuracy = 100.00%\n",
            "  Batch  91: Loss = 0.1126, Accuracy = 96.88%\n",
            "  Batch  92: Loss = 0.1243, Accuracy = 95.31%\n",
            "  Batch  93: Loss = 0.0612, Accuracy = 98.44%\n",
            "  Batch  94: Loss = 0.1961, Accuracy = 95.31%\n",
            "  Batch  95: Loss = 0.0810, Accuracy = 96.88%\n",
            "  Batch  96: Loss = 0.0891, Accuracy = 96.88%\n",
            "  Batch  97: Loss = 0.1527, Accuracy = 95.31%\n",
            "  Batch  98: Loss = 0.1018, Accuracy = 96.88%\n",
            "  Batch  99: Loss = 0.2149, Accuracy = 90.62%\n",
            "  Batch 100: Loss = 0.0479, Accuracy = 98.44%\n",
            "  Batch 101: Loss = 0.0279, Accuracy = 98.44%\n",
            "  Batch 102: Loss = 0.0650, Accuracy = 98.44%\n",
            "  Batch 103: Loss = 0.0611, Accuracy = 98.44%\n",
            "  Batch 104: Loss = 0.0721, Accuracy = 96.88%\n",
            "  Batch 105: Loss = 0.0672, Accuracy = 96.88%\n",
            "  Batch 106: Loss = 0.0408, Accuracy = 100.00%\n",
            "  Batch 107: Loss = 0.0741, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.1922, Accuracy = 93.75%\n",
            "  Batch 109: Loss = 0.1304, Accuracy = 96.88%\n",
            "  Batch 110: Loss = 0.1265, Accuracy = 96.88%\n",
            "  Batch 111: Loss = 0.0706, Accuracy = 98.44%\n",
            "  Batch 112: Loss = 0.0803, Accuracy = 96.88%\n",
            "  Batch 113: Loss = 0.0066, Accuracy = 100.00%\n",
            "  Batch 114: Loss = 0.0098, Accuracy = 100.00%\n",
            "  Batch 115: Loss = 0.0249, Accuracy = 100.00%\n",
            "  Batch 116: Loss = 0.0311, Accuracy = 98.44%\n",
            "  Batch 117: Loss = 0.0778, Accuracy = 95.31%\n",
            "  Batch 118: Loss = 0.1234, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.1632, Accuracy = 95.31%\n",
            "  Batch 120: Loss = 0.0428, Accuracy = 96.88%\n",
            "  Batch 121: Loss = 0.0884, Accuracy = 98.44%\n",
            "  Batch 122: Loss = 0.1075, Accuracy = 96.88%\n",
            "  Batch 123: Loss = 0.0774, Accuracy = 96.88%\n",
            "  Batch 124: Loss = 0.0385, Accuracy = 100.00%\n",
            "  Batch 125: Loss = 0.0576, Accuracy = 98.44%\n",
            "  Batch 126: Loss = 0.1413, Accuracy = 96.88%\n",
            "  Batch 127: Loss = 0.0362, Accuracy = 100.00%\n",
            "  Batch 128: Loss = 0.1633, Accuracy = 90.62%\n",
            "  Batch 129: Loss = 0.2905, Accuracy = 87.50%\n",
            "  Batch 130: Loss = 0.0383, Accuracy = 98.44%\n",
            "  Batch 131: Loss = 0.0520, Accuracy = 100.00%\n",
            "==> Epoch 73 Summary: Total Loss = 12.4654, Accuracy = 96.59%\n",
            "\n",
            "Epoch 74\n",
            "  Batch   1: Loss = 0.0851, Accuracy = 95.31%\n",
            "  Batch   2: Loss = 0.0804, Accuracy = 98.44%\n",
            "  Batch   3: Loss = 0.0446, Accuracy = 100.00%\n",
            "  Batch   4: Loss = 0.0502, Accuracy = 98.44%\n",
            "  Batch   5: Loss = 0.0822, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.0985, Accuracy = 96.88%\n",
            "  Batch   7: Loss = 0.1602, Accuracy = 93.75%\n",
            "  Batch   8: Loss = 0.0242, Accuracy = 100.00%\n",
            "  Batch   9: Loss = 0.1044, Accuracy = 96.88%\n",
            "  Batch  10: Loss = 0.0941, Accuracy = 95.31%\n",
            "  Batch  11: Loss = 0.0622, Accuracy = 98.44%\n",
            "  Batch  12: Loss = 0.2044, Accuracy = 95.31%\n",
            "  Batch  13: Loss = 0.0813, Accuracy = 96.88%\n",
            "  Batch  14: Loss = 0.0471, Accuracy = 98.44%\n",
            "  Batch  15: Loss = 0.0402, Accuracy = 100.00%\n",
            "  Batch  16: Loss = 0.0415, Accuracy = 100.00%\n",
            "  Batch  17: Loss = 0.1257, Accuracy = 93.75%\n",
            "  Batch  18: Loss = 0.2445, Accuracy = 93.75%\n",
            "  Batch  19: Loss = 0.0656, Accuracy = 98.44%\n",
            "  Batch  20: Loss = 0.0329, Accuracy = 100.00%\n",
            "  Batch  21: Loss = 0.1282, Accuracy = 95.31%\n",
            "  Batch  22: Loss = 0.0414, Accuracy = 98.44%\n",
            "  Batch  23: Loss = 0.0369, Accuracy = 98.44%\n",
            "  Batch  24: Loss = 0.1647, Accuracy = 95.31%\n",
            "  Batch  25: Loss = 0.0520, Accuracy = 98.44%\n",
            "  Batch  26: Loss = 0.1300, Accuracy = 96.88%\n",
            "  Batch  27: Loss = 0.0269, Accuracy = 98.44%\n",
            "  Batch  28: Loss = 0.0195, Accuracy = 100.00%\n",
            "  Batch  29: Loss = 0.0806, Accuracy = 96.88%\n",
            "  Batch  30: Loss = 0.0534, Accuracy = 98.44%\n",
            "  Batch  31: Loss = 0.1223, Accuracy = 95.31%\n",
            "  Batch  32: Loss = 0.0669, Accuracy = 96.88%\n",
            "  Batch  33: Loss = 0.1112, Accuracy = 95.31%\n",
            "  Batch  34: Loss = 0.1987, Accuracy = 95.31%\n",
            "  Batch  35: Loss = 0.0518, Accuracy = 100.00%\n",
            "  Batch  36: Loss = 0.0686, Accuracy = 96.88%\n",
            "  Batch  37: Loss = 0.0400, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.0843, Accuracy = 96.88%\n",
            "  Batch  39: Loss = 0.0501, Accuracy = 98.44%\n",
            "  Batch  40: Loss = 0.0384, Accuracy = 98.44%\n",
            "  Batch  41: Loss = 0.1021, Accuracy = 93.75%\n",
            "  Batch  42: Loss = 0.0836, Accuracy = 95.31%\n",
            "  Batch  43: Loss = 0.1037, Accuracy = 95.31%\n",
            "  Batch  44: Loss = 0.0735, Accuracy = 95.31%\n",
            "  Batch  45: Loss = 0.0435, Accuracy = 98.44%\n",
            "  Batch  46: Loss = 0.0852, Accuracy = 96.88%\n",
            "  Batch  47: Loss = 0.0821, Accuracy = 95.31%\n",
            "  Batch  48: Loss = 0.0957, Accuracy = 95.31%\n",
            "  Batch  49: Loss = 0.1142, Accuracy = 96.88%\n",
            "  Batch  50: Loss = 0.1477, Accuracy = 90.62%\n",
            "  Batch  51: Loss = 0.0186, Accuracy = 100.00%\n",
            "  Batch  52: Loss = 0.0513, Accuracy = 96.88%\n",
            "  Batch  53: Loss = 0.2563, Accuracy = 92.19%\n",
            "  Batch  54: Loss = 0.0325, Accuracy = 98.44%\n",
            "  Batch  55: Loss = 0.2762, Accuracy = 90.62%\n",
            "  Batch  56: Loss = 0.0204, Accuracy = 98.44%\n",
            "  Batch  57: Loss = 0.1418, Accuracy = 95.31%\n",
            "  Batch  58: Loss = 0.2087, Accuracy = 93.75%\n",
            "  Batch  59: Loss = 0.0788, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.0248, Accuracy = 98.44%\n",
            "  Batch  61: Loss = 0.0536, Accuracy = 98.44%\n",
            "  Batch  62: Loss = 0.1742, Accuracy = 90.62%\n",
            "  Batch  63: Loss = 0.1223, Accuracy = 93.75%\n",
            "  Batch  64: Loss = 0.0509, Accuracy = 98.44%\n",
            "  Batch  65: Loss = 0.0680, Accuracy = 96.88%\n",
            "  Batch  66: Loss = 0.0472, Accuracy = 96.88%\n",
            "  Batch  67: Loss = 0.0141, Accuracy = 100.00%\n",
            "  Batch  68: Loss = 0.2225, Accuracy = 93.75%\n",
            "  Batch  69: Loss = 0.1248, Accuracy = 95.31%\n",
            "  Batch  70: Loss = 0.0822, Accuracy = 98.44%\n",
            "  Batch  71: Loss = 0.0813, Accuracy = 95.31%\n",
            "  Batch  72: Loss = 0.0748, Accuracy = 96.88%\n",
            "  Batch  73: Loss = 0.0336, Accuracy = 100.00%\n",
            "  Batch  74: Loss = 0.0967, Accuracy = 96.88%\n",
            "  Batch  75: Loss = 0.0228, Accuracy = 100.00%\n",
            "  Batch  76: Loss = 0.0962, Accuracy = 96.88%\n",
            "  Batch  77: Loss = 0.0250, Accuracy = 100.00%\n",
            "  Batch  78: Loss = 0.0588, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.1978, Accuracy = 93.75%\n",
            "  Batch  80: Loss = 0.0656, Accuracy = 98.44%\n",
            "  Batch  81: Loss = 0.0975, Accuracy = 95.31%\n",
            "  Batch  82: Loss = 0.0329, Accuracy = 98.44%\n",
            "  Batch  83: Loss = 0.1078, Accuracy = 98.44%\n",
            "  Batch  84: Loss = 0.1664, Accuracy = 92.19%\n",
            "  Batch  85: Loss = 0.0516, Accuracy = 100.00%\n",
            "  Batch  86: Loss = 0.0980, Accuracy = 95.31%\n",
            "  Batch  87: Loss = 0.1421, Accuracy = 95.31%\n",
            "  Batch  88: Loss = 0.2451, Accuracy = 92.19%\n",
            "  Batch  89: Loss = 0.1172, Accuracy = 95.31%\n",
            "  Batch  90: Loss = 0.0911, Accuracy = 96.88%\n",
            "  Batch  91: Loss = 0.0978, Accuracy = 96.88%\n",
            "  Batch  92: Loss = 0.0624, Accuracy = 96.88%\n",
            "  Batch  93: Loss = 0.0249, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.1303, Accuracy = 95.31%\n",
            "  Batch  95: Loss = 0.1386, Accuracy = 93.75%\n",
            "  Batch  96: Loss = 0.0510, Accuracy = 98.44%\n",
            "  Batch  97: Loss = 0.0803, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.0394, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.0478, Accuracy = 98.44%\n",
            "  Batch 100: Loss = 0.0469, Accuracy = 98.44%\n",
            "  Batch 101: Loss = 0.0090, Accuracy = 100.00%\n",
            "  Batch 102: Loss = 0.1256, Accuracy = 95.31%\n",
            "  Batch 103: Loss = 0.0797, Accuracy = 96.88%\n",
            "  Batch 104: Loss = 0.1731, Accuracy = 90.62%\n",
            "  Batch 105: Loss = 0.1570, Accuracy = 93.75%\n",
            "  Batch 106: Loss = 0.1079, Accuracy = 95.31%\n",
            "  Batch 107: Loss = 0.0728, Accuracy = 96.88%\n",
            "  Batch 108: Loss = 0.1177, Accuracy = 95.31%\n",
            "  Batch 109: Loss = 0.0746, Accuracy = 98.44%\n",
            "  Batch 110: Loss = 0.0587, Accuracy = 98.44%\n",
            "  Batch 111: Loss = 0.0980, Accuracy = 96.88%\n",
            "  Batch 112: Loss = 0.0460, Accuracy = 100.00%\n",
            "  Batch 113: Loss = 0.0948, Accuracy = 96.88%\n",
            "  Batch 114: Loss = 0.0769, Accuracy = 98.44%\n",
            "  Batch 115: Loss = 0.1108, Accuracy = 96.88%\n",
            "  Batch 116: Loss = 0.0973, Accuracy = 95.31%\n",
            "  Batch 117: Loss = 0.0480, Accuracy = 98.44%\n",
            "  Batch 118: Loss = 0.0400, Accuracy = 100.00%\n",
            "  Batch 119: Loss = 0.1001, Accuracy = 93.75%\n",
            "  Batch 120: Loss = 0.0109, Accuracy = 100.00%\n",
            "  Batch 121: Loss = 0.0693, Accuracy = 96.88%\n",
            "  Batch 122: Loss = 0.0194, Accuracy = 100.00%\n",
            "  Batch 123: Loss = 0.0817, Accuracy = 96.88%\n",
            "  Batch 124: Loss = 0.0805, Accuracy = 95.31%\n",
            "  Batch 125: Loss = 0.0837, Accuracy = 98.44%\n",
            "  Batch 126: Loss = 0.0561, Accuracy = 98.44%\n",
            "  Batch 127: Loss = 0.0539, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0446, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.0635, Accuracy = 96.88%\n",
            "  Batch 130: Loss = 0.0992, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0304, Accuracy = 100.00%\n",
            "==> Epoch 74 Summary: Total Loss = 11.3382, Accuracy = 96.90%\n",
            "\n",
            "Epoch 75\n",
            "  Batch   1: Loss = 0.0484, Accuracy = 98.44%\n",
            "  Batch   2: Loss = 0.0959, Accuracy = 96.88%\n",
            "  Batch   3: Loss = 0.0562, Accuracy = 96.88%\n",
            "  Batch   4: Loss = 0.1326, Accuracy = 96.88%\n",
            "  Batch   5: Loss = 0.0203, Accuracy = 100.00%\n",
            "  Batch   6: Loss = 0.2123, Accuracy = 90.62%\n",
            "  Batch   7: Loss = 0.0463, Accuracy = 98.44%\n",
            "  Batch   8: Loss = 0.0476, Accuracy = 98.44%\n",
            "  Batch   9: Loss = 0.0390, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.1018, Accuracy = 95.31%\n",
            "  Batch  11: Loss = 0.1111, Accuracy = 95.31%\n",
            "  Batch  12: Loss = 0.0881, Accuracy = 96.88%\n",
            "  Batch  13: Loss = 0.0059, Accuracy = 100.00%\n",
            "  Batch  14: Loss = 0.0173, Accuracy = 100.00%\n",
            "  Batch  15: Loss = 0.0311, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.1801, Accuracy = 96.88%\n",
            "  Batch  17: Loss = 0.1124, Accuracy = 96.88%\n",
            "  Batch  18: Loss = 0.1614, Accuracy = 93.75%\n",
            "  Batch  19: Loss = 0.1432, Accuracy = 95.31%\n",
            "  Batch  20: Loss = 0.1156, Accuracy = 96.88%\n",
            "  Batch  21: Loss = 0.1529, Accuracy = 93.75%\n",
            "  Batch  22: Loss = 0.0622, Accuracy = 98.44%\n",
            "  Batch  23: Loss = 0.2674, Accuracy = 93.75%\n",
            "  Batch  24: Loss = 0.1115, Accuracy = 95.31%\n",
            "  Batch  25: Loss = 0.0527, Accuracy = 96.88%\n",
            "  Batch  26: Loss = 0.1542, Accuracy = 95.31%\n",
            "  Batch  27: Loss = 0.1423, Accuracy = 96.88%\n",
            "  Batch  28: Loss = 0.2104, Accuracy = 93.75%\n",
            "  Batch  29: Loss = 0.0839, Accuracy = 98.44%\n",
            "  Batch  30: Loss = 0.0923, Accuracy = 96.88%\n",
            "  Batch  31: Loss = 0.1056, Accuracy = 95.31%\n",
            "  Batch  32: Loss = 0.3077, Accuracy = 90.62%\n",
            "  Batch  33: Loss = 0.1025, Accuracy = 96.88%\n",
            "  Batch  34: Loss = 0.1980, Accuracy = 92.19%\n",
            "  Batch  35: Loss = 0.0991, Accuracy = 96.88%\n",
            "  Batch  36: Loss = 0.0223, Accuracy = 100.00%\n",
            "  Batch  37: Loss = 0.0989, Accuracy = 93.75%\n",
            "  Batch  38: Loss = 0.0954, Accuracy = 95.31%\n",
            "  Batch  39: Loss = 0.1448, Accuracy = 93.75%\n",
            "  Batch  40: Loss = 0.1820, Accuracy = 95.31%\n",
            "  Batch  41: Loss = 0.2978, Accuracy = 92.19%\n",
            "  Batch  42: Loss = 0.2041, Accuracy = 93.75%\n",
            "  Batch  43: Loss = 0.0836, Accuracy = 98.44%\n",
            "  Batch  44: Loss = 0.0608, Accuracy = 98.44%\n",
            "  Batch  45: Loss = 0.1746, Accuracy = 93.75%\n",
            "  Batch  46: Loss = 0.1440, Accuracy = 93.75%\n",
            "  Batch  47: Loss = 0.0502, Accuracy = 96.88%\n",
            "  Batch  48: Loss = 0.1280, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.1193, Accuracy = 93.75%\n",
            "  Batch  50: Loss = 0.1149, Accuracy = 95.31%\n",
            "  Batch  51: Loss = 0.3419, Accuracy = 89.06%\n",
            "  Batch  52: Loss = 0.1675, Accuracy = 93.75%\n",
            "  Batch  53: Loss = 0.0221, Accuracy = 100.00%\n",
            "  Batch  54: Loss = 0.0862, Accuracy = 96.88%\n",
            "  Batch  55: Loss = 0.0391, Accuracy = 98.44%\n",
            "  Batch  56: Loss = 0.1558, Accuracy = 93.75%\n",
            "  Batch  57: Loss = 0.1760, Accuracy = 96.88%\n",
            "  Batch  58: Loss = 0.1132, Accuracy = 96.88%\n",
            "  Batch  59: Loss = 0.1497, Accuracy = 93.75%\n",
            "  Batch  60: Loss = 0.0873, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.1616, Accuracy = 95.31%\n",
            "  Batch  62: Loss = 0.0766, Accuracy = 95.31%\n",
            "  Batch  63: Loss = 0.0534, Accuracy = 98.44%\n",
            "  Batch  64: Loss = 0.0553, Accuracy = 98.44%\n",
            "  Batch  65: Loss = 0.0752, Accuracy = 98.44%\n",
            "  Batch  66: Loss = 0.1155, Accuracy = 95.31%\n",
            "  Batch  67: Loss = 0.1303, Accuracy = 98.44%\n",
            "  Batch  68: Loss = 0.2102, Accuracy = 93.75%\n",
            "  Batch  69: Loss = 0.0396, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.1827, Accuracy = 95.31%\n",
            "  Batch  71: Loss = 0.0513, Accuracy = 98.44%\n",
            "  Batch  72: Loss = 0.1196, Accuracy = 95.31%\n",
            "  Batch  73: Loss = 0.0747, Accuracy = 95.31%\n",
            "  Batch  74: Loss = 0.0856, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.0499, Accuracy = 98.44%\n",
            "  Batch  76: Loss = 0.1059, Accuracy = 95.31%\n",
            "  Batch  77: Loss = 0.0870, Accuracy = 96.88%\n",
            "  Batch  78: Loss = 0.0966, Accuracy = 95.31%\n",
            "  Batch  79: Loss = 0.0891, Accuracy = 96.88%\n",
            "  Batch  80: Loss = 0.2383, Accuracy = 92.19%\n",
            "  Batch  81: Loss = 0.0961, Accuracy = 96.88%\n",
            "  Batch  82: Loss = 0.0464, Accuracy = 100.00%\n",
            "  Batch  83: Loss = 0.0655, Accuracy = 95.31%\n",
            "  Batch  84: Loss = 0.1756, Accuracy = 95.31%\n",
            "  Batch  85: Loss = 0.1428, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.0583, Accuracy = 98.44%\n",
            "  Batch  87: Loss = 0.0332, Accuracy = 98.44%\n",
            "  Batch  88: Loss = 0.1243, Accuracy = 95.31%\n",
            "  Batch  89: Loss = 0.0772, Accuracy = 98.44%\n",
            "  Batch  90: Loss = 0.0355, Accuracy = 100.00%\n",
            "  Batch  91: Loss = 0.1104, Accuracy = 95.31%\n",
            "  Batch  92: Loss = 0.0537, Accuracy = 96.88%\n",
            "  Batch  93: Loss = 0.1238, Accuracy = 96.88%\n",
            "  Batch  94: Loss = 0.2577, Accuracy = 95.31%\n",
            "  Batch  95: Loss = 0.0920, Accuracy = 95.31%\n",
            "  Batch  96: Loss = 0.0239, Accuracy = 98.44%\n",
            "  Batch  97: Loss = 0.0923, Accuracy = 95.31%\n",
            "  Batch  98: Loss = 0.0916, Accuracy = 96.88%\n",
            "  Batch  99: Loss = 0.0618, Accuracy = 98.44%\n",
            "  Batch 100: Loss = 0.1505, Accuracy = 95.31%\n",
            "  Batch 101: Loss = 0.2074, Accuracy = 93.75%\n",
            "  Batch 102: Loss = 0.0526, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.1078, Accuracy = 98.44%\n",
            "  Batch 104: Loss = 0.1129, Accuracy = 95.31%\n",
            "  Batch 105: Loss = 0.0687, Accuracy = 98.44%\n",
            "  Batch 106: Loss = 0.0706, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.0326, Accuracy = 100.00%\n",
            "  Batch 108: Loss = 0.0803, Accuracy = 96.88%\n",
            "  Batch 109: Loss = 0.0731, Accuracy = 98.44%\n",
            "  Batch 110: Loss = 0.1068, Accuracy = 96.88%\n",
            "  Batch 111: Loss = 0.0894, Accuracy = 95.31%\n",
            "  Batch 112: Loss = 0.0991, Accuracy = 96.88%\n",
            "  Batch 113: Loss = 0.1515, Accuracy = 93.75%\n",
            "  Batch 114: Loss = 0.0893, Accuracy = 96.88%\n",
            "  Batch 115: Loss = 0.0333, Accuracy = 100.00%\n",
            "  Batch 116: Loss = 0.1429, Accuracy = 95.31%\n",
            "  Batch 117: Loss = 0.0289, Accuracy = 100.00%\n",
            "  Batch 118: Loss = 0.0657, Accuracy = 98.44%\n",
            "  Batch 119: Loss = 0.0797, Accuracy = 96.88%\n",
            "  Batch 120: Loss = 0.0572, Accuracy = 96.88%\n",
            "  Batch 121: Loss = 0.0145, Accuracy = 100.00%\n",
            "  Batch 122: Loss = 0.1232, Accuracy = 98.44%\n",
            "  Batch 123: Loss = 0.0749, Accuracy = 96.88%\n",
            "  Batch 124: Loss = 0.0209, Accuracy = 100.00%\n",
            "  Batch 125: Loss = 0.0639, Accuracy = 96.88%\n",
            "  Batch 126: Loss = 0.0268, Accuracy = 100.00%\n",
            "  Batch 127: Loss = 0.0451, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.1460, Accuracy = 92.19%\n",
            "  Batch 129: Loss = 0.1435, Accuracy = 95.31%\n",
            "  Batch 130: Loss = 0.1516, Accuracy = 93.75%\n",
            "  Batch 131: Loss = 0.0186, Accuracy = 100.00%\n",
            "==> Epoch 75 Summary: Total Loss = 13.7586, Accuracy = 96.48%\n",
            "\n",
            "Epoch 76\n",
            "  Batch   1: Loss = 0.0415, Accuracy = 98.44%\n",
            "  Batch   2: Loss = 0.0495, Accuracy = 96.88%\n",
            "  Batch   3: Loss = 0.0495, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0802, Accuracy = 95.31%\n",
            "  Batch   5: Loss = 0.1457, Accuracy = 95.31%\n",
            "  Batch   6: Loss = 0.1390, Accuracy = 92.19%\n",
            "  Batch   7: Loss = 0.1095, Accuracy = 95.31%\n",
            "  Batch   8: Loss = 0.1023, Accuracy = 92.19%\n",
            "  Batch   9: Loss = 0.0601, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.0167, Accuracy = 100.00%\n",
            "  Batch  11: Loss = 0.1185, Accuracy = 95.31%\n",
            "  Batch  12: Loss = 0.0294, Accuracy = 100.00%\n",
            "  Batch  13: Loss = 0.0257, Accuracy = 100.00%\n",
            "  Batch  14: Loss = 0.0417, Accuracy = 98.44%\n",
            "  Batch  15: Loss = 0.0590, Accuracy = 96.88%\n",
            "  Batch  16: Loss = 0.0926, Accuracy = 93.75%\n",
            "  Batch  17: Loss = 0.1144, Accuracy = 96.88%\n",
            "  Batch  18: Loss = 0.0282, Accuracy = 98.44%\n",
            "  Batch  19: Loss = 0.0367, Accuracy = 98.44%\n",
            "  Batch  20: Loss = 0.1713, Accuracy = 90.62%\n",
            "  Batch  21: Loss = 0.0800, Accuracy = 98.44%\n",
            "  Batch  22: Loss = 0.1035, Accuracy = 93.75%\n",
            "  Batch  23: Loss = 0.1094, Accuracy = 96.88%\n",
            "  Batch  24: Loss = 0.0260, Accuracy = 98.44%\n",
            "  Batch  25: Loss = 0.0185, Accuracy = 100.00%\n",
            "  Batch  26: Loss = 0.1518, Accuracy = 92.19%\n",
            "  Batch  27: Loss = 0.1311, Accuracy = 95.31%\n",
            "  Batch  28: Loss = 0.1525, Accuracy = 96.88%\n",
            "  Batch  29: Loss = 0.0586, Accuracy = 98.44%\n",
            "  Batch  30: Loss = 0.0494, Accuracy = 96.88%\n",
            "  Batch  31: Loss = 0.0834, Accuracy = 98.44%\n",
            "  Batch  32: Loss = 0.1015, Accuracy = 95.31%\n",
            "  Batch  33: Loss = 0.1205, Accuracy = 96.88%\n",
            "  Batch  34: Loss = 0.0325, Accuracy = 98.44%\n",
            "  Batch  35: Loss = 0.0479, Accuracy = 96.88%\n",
            "  Batch  36: Loss = 0.0117, Accuracy = 100.00%\n",
            "  Batch  37: Loss = 0.0726, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.0989, Accuracy = 95.31%\n",
            "  Batch  39: Loss = 0.1539, Accuracy = 95.31%\n",
            "  Batch  40: Loss = 0.0944, Accuracy = 95.31%\n",
            "  Batch  41: Loss = 0.0595, Accuracy = 96.88%\n",
            "  Batch  42: Loss = 0.1258, Accuracy = 96.88%\n",
            "  Batch  43: Loss = 0.0953, Accuracy = 95.31%\n",
            "  Batch  44: Loss = 0.1399, Accuracy = 98.44%\n",
            "  Batch  45: Loss = 0.2253, Accuracy = 95.31%\n",
            "  Batch  46: Loss = 0.0475, Accuracy = 98.44%\n",
            "  Batch  47: Loss = 0.0783, Accuracy = 96.88%\n",
            "  Batch  48: Loss = 0.1390, Accuracy = 95.31%\n",
            "  Batch  49: Loss = 0.0452, Accuracy = 96.88%\n",
            "  Batch  50: Loss = 0.1277, Accuracy = 95.31%\n",
            "  Batch  51: Loss = 0.0469, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.1003, Accuracy = 95.31%\n",
            "  Batch  53: Loss = 0.0825, Accuracy = 96.88%\n",
            "  Batch  54: Loss = 0.0310, Accuracy = 100.00%\n",
            "  Batch  55: Loss = 0.1334, Accuracy = 98.44%\n",
            "  Batch  56: Loss = 0.1054, Accuracy = 96.88%\n",
            "  Batch  57: Loss = 0.0549, Accuracy = 96.88%\n",
            "  Batch  58: Loss = 0.1235, Accuracy = 98.44%\n",
            "  Batch  59: Loss = 0.0966, Accuracy = 95.31%\n",
            "  Batch  60: Loss = 0.0690, Accuracy = 98.44%\n",
            "  Batch  61: Loss = 0.1538, Accuracy = 98.44%\n",
            "  Batch  62: Loss = 0.0739, Accuracy = 98.44%\n",
            "  Batch  63: Loss = 0.2256, Accuracy = 93.75%\n",
            "  Batch  64: Loss = 0.0578, Accuracy = 96.88%\n",
            "  Batch  65: Loss = 0.1337, Accuracy = 95.31%\n",
            "  Batch  66: Loss = 0.1515, Accuracy = 93.75%\n",
            "  Batch  67: Loss = 0.0308, Accuracy = 98.44%\n",
            "  Batch  68: Loss = 0.0723, Accuracy = 98.44%\n",
            "  Batch  69: Loss = 0.0763, Accuracy = 96.88%\n",
            "  Batch  70: Loss = 0.0584, Accuracy = 98.44%\n",
            "  Batch  71: Loss = 0.2452, Accuracy = 89.06%\n",
            "  Batch  72: Loss = 0.1238, Accuracy = 95.31%\n",
            "  Batch  73: Loss = 0.1448, Accuracy = 93.75%\n",
            "  Batch  74: Loss = 0.0434, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.1601, Accuracy = 96.88%\n",
            "  Batch  76: Loss = 0.1718, Accuracy = 95.31%\n",
            "  Batch  77: Loss = 0.2075, Accuracy = 96.88%\n",
            "  Batch  78: Loss = 0.0731, Accuracy = 96.88%\n",
            "  Batch  79: Loss = 0.0724, Accuracy = 98.44%\n",
            "  Batch  80: Loss = 0.1485, Accuracy = 95.31%\n",
            "  Batch  81: Loss = 0.0924, Accuracy = 96.88%\n",
            "  Batch  82: Loss = 0.0453, Accuracy = 96.88%\n",
            "  Batch  83: Loss = 0.1077, Accuracy = 95.31%\n",
            "  Batch  84: Loss = 0.0698, Accuracy = 96.88%\n",
            "  Batch  85: Loss = 0.1651, Accuracy = 92.19%\n",
            "  Batch  86: Loss = 0.0474, Accuracy = 98.44%\n",
            "  Batch  87: Loss = 0.0322, Accuracy = 98.44%\n",
            "  Batch  88: Loss = 0.0842, Accuracy = 98.44%\n",
            "  Batch  89: Loss = 0.0350, Accuracy = 100.00%\n",
            "  Batch  90: Loss = 0.0684, Accuracy = 98.44%\n",
            "  Batch  91: Loss = 0.0836, Accuracy = 96.88%\n",
            "  Batch  92: Loss = 0.1715, Accuracy = 92.19%\n",
            "  Batch  93: Loss = 0.0223, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.2248, Accuracy = 90.62%\n",
            "  Batch  95: Loss = 0.1289, Accuracy = 93.75%\n",
            "  Batch  96: Loss = 0.1413, Accuracy = 93.75%\n",
            "  Batch  97: Loss = 0.1098, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.0623, Accuracy = 96.88%\n",
            "  Batch  99: Loss = 0.0117, Accuracy = 100.00%\n",
            "  Batch 100: Loss = 0.0832, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.0367, Accuracy = 100.00%\n",
            "  Batch 102: Loss = 0.1892, Accuracy = 93.75%\n",
            "  Batch 103: Loss = 0.1016, Accuracy = 95.31%\n",
            "  Batch 104: Loss = 0.1342, Accuracy = 95.31%\n",
            "  Batch 105: Loss = 0.1013, Accuracy = 95.31%\n",
            "  Batch 106: Loss = 0.1450, Accuracy = 93.75%\n",
            "  Batch 107: Loss = 0.0672, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.0704, Accuracy = 98.44%\n",
            "  Batch 109: Loss = 0.1192, Accuracy = 95.31%\n",
            "  Batch 110: Loss = 0.0775, Accuracy = 98.44%\n",
            "  Batch 111: Loss = 0.1209, Accuracy = 96.88%\n",
            "  Batch 112: Loss = 0.1094, Accuracy = 95.31%\n",
            "  Batch 113: Loss = 0.1156, Accuracy = 96.88%\n",
            "  Batch 114: Loss = 0.0586, Accuracy = 98.44%\n",
            "  Batch 115: Loss = 0.1095, Accuracy = 95.31%\n",
            "  Batch 116: Loss = 0.0432, Accuracy = 100.00%\n",
            "  Batch 117: Loss = 0.1660, Accuracy = 93.75%\n",
            "  Batch 118: Loss = 0.0412, Accuracy = 98.44%\n",
            "  Batch 119: Loss = 0.0455, Accuracy = 98.44%\n",
            "  Batch 120: Loss = 0.0642, Accuracy = 96.88%\n",
            "  Batch 121: Loss = 0.0359, Accuracy = 100.00%\n",
            "  Batch 122: Loss = 0.0857, Accuracy = 96.88%\n",
            "  Batch 123: Loss = 0.1271, Accuracy = 92.19%\n",
            "  Batch 124: Loss = 0.0859, Accuracy = 96.88%\n",
            "  Batch 125: Loss = 0.2144, Accuracy = 93.75%\n",
            "  Batch 126: Loss = 0.1102, Accuracy = 95.31%\n",
            "  Batch 127: Loss = 0.0196, Accuracy = 100.00%\n",
            "  Batch 128: Loss = 0.0323, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.0524, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.0355, Accuracy = 98.44%\n",
            "  Batch 131: Loss = 0.0058, Accuracy = 100.00%\n",
            "==> Epoch 76 Summary: Total Loss = 12.0691, Accuracy = 96.68%\n",
            "\n",
            "Epoch 77\n",
            "  Batch   1: Loss = 0.0200, Accuracy = 100.00%\n",
            "  Batch   2: Loss = 0.0743, Accuracy = 95.31%\n",
            "  Batch   3: Loss = 0.0898, Accuracy = 95.31%\n",
            "  Batch   4: Loss = 0.0408, Accuracy = 98.44%\n",
            "  Batch   5: Loss = 0.1885, Accuracy = 95.31%\n",
            "  Batch   6: Loss = 0.1619, Accuracy = 95.31%\n",
            "  Batch   7: Loss = 0.1357, Accuracy = 95.31%\n",
            "  Batch   8: Loss = 0.1159, Accuracy = 95.31%\n",
            "  Batch   9: Loss = 0.0476, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.0402, Accuracy = 98.44%\n",
            "  Batch  11: Loss = 0.1019, Accuracy = 96.88%\n",
            "  Batch  12: Loss = 0.0739, Accuracy = 96.88%\n",
            "  Batch  13: Loss = 0.1618, Accuracy = 96.88%\n",
            "  Batch  14: Loss = 0.1025, Accuracy = 96.88%\n",
            "  Batch  15: Loss = 0.0440, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.0452, Accuracy = 98.44%\n",
            "  Batch  17: Loss = 0.0170, Accuracy = 100.00%\n",
            "  Batch  18: Loss = 0.0181, Accuracy = 100.00%\n",
            "  Batch  19: Loss = 0.0323, Accuracy = 98.44%\n",
            "  Batch  20: Loss = 0.2172, Accuracy = 92.19%\n",
            "  Batch  21: Loss = 0.0210, Accuracy = 100.00%\n",
            "  Batch  22: Loss = 0.0867, Accuracy = 96.88%\n",
            "  Batch  23: Loss = 0.0819, Accuracy = 96.88%\n",
            "  Batch  24: Loss = 0.0767, Accuracy = 98.44%\n",
            "  Batch  25: Loss = 0.0122, Accuracy = 100.00%\n",
            "  Batch  26: Loss = 0.0549, Accuracy = 98.44%\n",
            "  Batch  27: Loss = 0.0770, Accuracy = 96.88%\n",
            "  Batch  28: Loss = 0.0800, Accuracy = 98.44%\n",
            "  Batch  29: Loss = 0.1556, Accuracy = 96.88%\n",
            "  Batch  30: Loss = 0.0474, Accuracy = 98.44%\n",
            "  Batch  31: Loss = 0.0954, Accuracy = 95.31%\n",
            "  Batch  32: Loss = 0.0596, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.0589, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.0952, Accuracy = 95.31%\n",
            "  Batch  35: Loss = 0.0852, Accuracy = 95.31%\n",
            "  Batch  36: Loss = 0.0285, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.0318, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.0249, Accuracy = 100.00%\n",
            "  Batch  39: Loss = 0.0628, Accuracy = 98.44%\n",
            "  Batch  40: Loss = 0.0778, Accuracy = 96.88%\n",
            "  Batch  41: Loss = 0.0990, Accuracy = 96.88%\n",
            "  Batch  42: Loss = 0.0151, Accuracy = 100.00%\n",
            "  Batch  43: Loss = 0.0590, Accuracy = 95.31%\n",
            "  Batch  44: Loss = 0.0493, Accuracy = 98.44%\n",
            "  Batch  45: Loss = 0.0778, Accuracy = 95.31%\n",
            "  Batch  46: Loss = 0.1894, Accuracy = 92.19%\n",
            "  Batch  47: Loss = 0.0997, Accuracy = 95.31%\n",
            "  Batch  48: Loss = 0.0298, Accuracy = 98.44%\n",
            "  Batch  49: Loss = 0.0383, Accuracy = 98.44%\n",
            "  Batch  50: Loss = 0.0368, Accuracy = 98.44%\n",
            "  Batch  51: Loss = 0.0499, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.0487, Accuracy = 98.44%\n",
            "  Batch  53: Loss = 0.0687, Accuracy = 98.44%\n",
            "  Batch  54: Loss = 0.0412, Accuracy = 98.44%\n",
            "  Batch  55: Loss = 0.0307, Accuracy = 98.44%\n",
            "  Batch  56: Loss = 0.0386, Accuracy = 98.44%\n",
            "  Batch  57: Loss = 0.0386, Accuracy = 98.44%\n",
            "  Batch  58: Loss = 0.0274, Accuracy = 98.44%\n",
            "  Batch  59: Loss = 0.0272, Accuracy = 100.00%\n",
            "  Batch  60: Loss = 0.1192, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.1043, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.0225, Accuracy = 100.00%\n",
            "  Batch  63: Loss = 0.1082, Accuracy = 96.88%\n",
            "  Batch  64: Loss = 0.0670, Accuracy = 95.31%\n",
            "  Batch  65: Loss = 0.0994, Accuracy = 95.31%\n",
            "  Batch  66: Loss = 0.0678, Accuracy = 98.44%\n",
            "  Batch  67: Loss = 0.0646, Accuracy = 98.44%\n",
            "  Batch  68: Loss = 0.1783, Accuracy = 95.31%\n",
            "  Batch  69: Loss = 0.0265, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.0148, Accuracy = 100.00%\n",
            "  Batch  71: Loss = 0.0739, Accuracy = 96.88%\n",
            "  Batch  72: Loss = 0.0325, Accuracy = 98.44%\n",
            "  Batch  73: Loss = 0.0516, Accuracy = 98.44%\n",
            "  Batch  74: Loss = 0.0193, Accuracy = 100.00%\n",
            "  Batch  75: Loss = 0.0218, Accuracy = 100.00%\n",
            "  Batch  76: Loss = 0.0309, Accuracy = 100.00%\n",
            "  Batch  77: Loss = 0.1609, Accuracy = 93.75%\n",
            "  Batch  78: Loss = 0.0299, Accuracy = 100.00%\n",
            "  Batch  79: Loss = 0.0448, Accuracy = 98.44%\n",
            "  Batch  80: Loss = 0.1571, Accuracy = 93.75%\n",
            "  Batch  81: Loss = 0.0898, Accuracy = 96.88%\n",
            "  Batch  82: Loss = 0.0390, Accuracy = 96.88%\n",
            "  Batch  83: Loss = 0.0832, Accuracy = 96.88%\n",
            "  Batch  84: Loss = 0.0948, Accuracy = 95.31%\n",
            "  Batch  85: Loss = 0.0309, Accuracy = 100.00%\n",
            "  Batch  86: Loss = 0.0359, Accuracy = 100.00%\n",
            "  Batch  87: Loss = 0.0741, Accuracy = 95.31%\n",
            "  Batch  88: Loss = 0.0467, Accuracy = 98.44%\n",
            "  Batch  89: Loss = 0.1028, Accuracy = 93.75%\n",
            "  Batch  90: Loss = 0.0589, Accuracy = 98.44%\n",
            "  Batch  91: Loss = 0.1174, Accuracy = 96.88%\n",
            "  Batch  92: Loss = 0.1393, Accuracy = 95.31%\n",
            "  Batch  93: Loss = 0.0473, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.0249, Accuracy = 100.00%\n",
            "  Batch  95: Loss = 0.0501, Accuracy = 98.44%\n",
            "  Batch  96: Loss = 0.1140, Accuracy = 95.31%\n",
            "  Batch  97: Loss = 0.0122, Accuracy = 100.00%\n",
            "  Batch  98: Loss = 0.1406, Accuracy = 95.31%\n",
            "  Batch  99: Loss = 0.0337, Accuracy = 98.44%\n",
            "  Batch 100: Loss = 0.0512, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.0243, Accuracy = 100.00%\n",
            "  Batch 102: Loss = 0.0317, Accuracy = 98.44%\n",
            "  Batch 103: Loss = 0.0477, Accuracy = 98.44%\n",
            "  Batch 104: Loss = 0.0620, Accuracy = 95.31%\n",
            "  Batch 105: Loss = 0.0098, Accuracy = 100.00%\n",
            "  Batch 106: Loss = 0.0343, Accuracy = 98.44%\n",
            "  Batch 107: Loss = 0.0243, Accuracy = 100.00%\n",
            "  Batch 108: Loss = 0.1773, Accuracy = 95.31%\n",
            "  Batch 109: Loss = 0.0380, Accuracy = 98.44%\n",
            "  Batch 110: Loss = 0.0190, Accuracy = 100.00%\n",
            "  Batch 111: Loss = 0.0352, Accuracy = 100.00%\n",
            "  Batch 112: Loss = 0.0943, Accuracy = 95.31%\n",
            "  Batch 113: Loss = 0.0821, Accuracy = 95.31%\n",
            "  Batch 114: Loss = 0.1115, Accuracy = 95.31%\n",
            "  Batch 115: Loss = 0.0221, Accuracy = 100.00%\n",
            "  Batch 116: Loss = 0.1303, Accuracy = 92.19%\n",
            "  Batch 117: Loss = 0.1718, Accuracy = 96.88%\n",
            "  Batch 118: Loss = 0.0188, Accuracy = 100.00%\n",
            "  Batch 119: Loss = 0.1276, Accuracy = 95.31%\n",
            "  Batch 120: Loss = 0.0778, Accuracy = 98.44%\n",
            "  Batch 121: Loss = 0.0489, Accuracy = 98.44%\n",
            "  Batch 122: Loss = 0.0561, Accuracy = 98.44%\n",
            "  Batch 123: Loss = 0.1111, Accuracy = 98.44%\n",
            "  Batch 124: Loss = 0.1323, Accuracy = 95.31%\n",
            "  Batch 125: Loss = 0.0479, Accuracy = 96.88%\n",
            "  Batch 126: Loss = 0.0966, Accuracy = 95.31%\n",
            "  Batch 127: Loss = 0.1509, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0322, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.0584, Accuracy = 96.88%\n",
            "  Batch 130: Loss = 0.0732, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0078, Accuracy = 100.00%\n",
            "==> Epoch 77 Summary: Total Loss = 9.1446, Accuracy = 97.55%\n",
            "\n",
            "Epoch 78\n",
            "  Batch   1: Loss = 0.4944, Accuracy = 89.06%\n",
            "  Batch   2: Loss = 0.0990, Accuracy = 96.88%\n",
            "  Batch   3: Loss = 0.0312, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0627, Accuracy = 96.88%\n",
            "  Batch   5: Loss = 0.0090, Accuracy = 100.00%\n",
            "  Batch   6: Loss = 0.1132, Accuracy = 98.44%\n",
            "  Batch   7: Loss = 0.1377, Accuracy = 93.75%\n",
            "  Batch   8: Loss = 0.1576, Accuracy = 93.75%\n",
            "  Batch   9: Loss = 0.1777, Accuracy = 96.88%\n",
            "  Batch  10: Loss = 0.0190, Accuracy = 100.00%\n",
            "  Batch  11: Loss = 0.1881, Accuracy = 95.31%\n",
            "  Batch  12: Loss = 0.0576, Accuracy = 96.88%\n",
            "  Batch  13: Loss = 0.0589, Accuracy = 96.88%\n",
            "  Batch  14: Loss = 0.0564, Accuracy = 98.44%\n",
            "  Batch  15: Loss = 0.0355, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.0149, Accuracy = 100.00%\n",
            "  Batch  17: Loss = 0.1670, Accuracy = 93.75%\n",
            "  Batch  18: Loss = 0.1151, Accuracy = 95.31%\n",
            "  Batch  19: Loss = 0.0933, Accuracy = 95.31%\n",
            "  Batch  20: Loss = 0.0208, Accuracy = 98.44%\n",
            "  Batch  21: Loss = 0.1406, Accuracy = 93.75%\n",
            "  Batch  22: Loss = 0.1318, Accuracy = 96.88%\n",
            "  Batch  23: Loss = 0.0793, Accuracy = 96.88%\n",
            "  Batch  24: Loss = 0.0671, Accuracy = 96.88%\n",
            "  Batch  25: Loss = 0.1014, Accuracy = 93.75%\n",
            "  Batch  26: Loss = 0.0430, Accuracy = 98.44%\n",
            "  Batch  27: Loss = 0.0257, Accuracy = 98.44%\n",
            "  Batch  28: Loss = 0.2278, Accuracy = 92.19%\n",
            "  Batch  29: Loss = 0.1756, Accuracy = 95.31%\n",
            "  Batch  30: Loss = 0.0754, Accuracy = 96.88%\n",
            "  Batch  31: Loss = 0.0697, Accuracy = 98.44%\n",
            "  Batch  32: Loss = 0.0703, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.0717, Accuracy = 96.88%\n",
            "  Batch  34: Loss = 0.0438, Accuracy = 98.44%\n",
            "  Batch  35: Loss = 0.1736, Accuracy = 95.31%\n",
            "  Batch  36: Loss = 0.0462, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.0194, Accuracy = 100.00%\n",
            "  Batch  38: Loss = 0.0579, Accuracy = 96.88%\n",
            "  Batch  39: Loss = 0.1344, Accuracy = 95.31%\n",
            "  Batch  40: Loss = 0.0301, Accuracy = 100.00%\n",
            "  Batch  41: Loss = 0.0663, Accuracy = 96.88%\n",
            "  Batch  42: Loss = 0.1037, Accuracy = 96.88%\n",
            "  Batch  43: Loss = 0.1341, Accuracy = 95.31%\n",
            "  Batch  44: Loss = 0.0245, Accuracy = 100.00%\n",
            "  Batch  45: Loss = 0.0778, Accuracy = 96.88%\n",
            "  Batch  46: Loss = 0.1088, Accuracy = 95.31%\n",
            "  Batch  47: Loss = 0.0566, Accuracy = 98.44%\n",
            "  Batch  48: Loss = 0.1299, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.1219, Accuracy = 95.31%\n",
            "  Batch  50: Loss = 0.1062, Accuracy = 96.88%\n",
            "  Batch  51: Loss = 0.0554, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.0624, Accuracy = 98.44%\n",
            "  Batch  53: Loss = 0.1176, Accuracy = 95.31%\n",
            "  Batch  54: Loss = 0.0161, Accuracy = 100.00%\n",
            "  Batch  55: Loss = 0.0232, Accuracy = 100.00%\n",
            "  Batch  56: Loss = 0.0569, Accuracy = 98.44%\n",
            "  Batch  57: Loss = 0.0416, Accuracy = 98.44%\n",
            "  Batch  58: Loss = 0.0149, Accuracy = 100.00%\n",
            "  Batch  59: Loss = 0.1077, Accuracy = 96.88%\n",
            "  Batch  60: Loss = 0.1292, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.0455, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.0482, Accuracy = 100.00%\n",
            "  Batch  63: Loss = 0.0884, Accuracy = 96.88%\n",
            "  Batch  64: Loss = 0.0496, Accuracy = 98.44%\n",
            "  Batch  65: Loss = 0.1263, Accuracy = 96.88%\n",
            "  Batch  66: Loss = 0.1066, Accuracy = 98.44%\n",
            "  Batch  67: Loss = 0.0594, Accuracy = 98.44%\n",
            "  Batch  68: Loss = 0.1287, Accuracy = 96.88%\n",
            "  Batch  69: Loss = 0.1423, Accuracy = 96.88%\n",
            "  Batch  70: Loss = 0.0282, Accuracy = 100.00%\n",
            "  Batch  71: Loss = 0.0316, Accuracy = 98.44%\n",
            "  Batch  72: Loss = 0.1945, Accuracy = 93.75%\n",
            "  Batch  73: Loss = 0.1409, Accuracy = 95.31%\n",
            "  Batch  74: Loss = 0.0648, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.0402, Accuracy = 98.44%\n",
            "  Batch  76: Loss = 0.0889, Accuracy = 98.44%\n",
            "  Batch  77: Loss = 0.1609, Accuracy = 93.75%\n",
            "  Batch  78: Loss = 0.1014, Accuracy = 93.75%\n",
            "  Batch  79: Loss = 0.0797, Accuracy = 96.88%\n",
            "  Batch  80: Loss = 0.1444, Accuracy = 95.31%\n",
            "  Batch  81: Loss = 0.0503, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.0406, Accuracy = 100.00%\n",
            "  Batch  83: Loss = 0.0404, Accuracy = 98.44%\n",
            "  Batch  84: Loss = 0.0689, Accuracy = 98.44%\n",
            "  Batch  85: Loss = 0.0622, Accuracy = 98.44%\n",
            "  Batch  86: Loss = 0.1076, Accuracy = 98.44%\n",
            "  Batch  87: Loss = 0.0878, Accuracy = 96.88%\n",
            "  Batch  88: Loss = 0.0452, Accuracy = 98.44%\n",
            "  Batch  89: Loss = 0.1416, Accuracy = 96.88%\n",
            "  Batch  90: Loss = 0.0529, Accuracy = 98.44%\n",
            "  Batch  91: Loss = 0.1331, Accuracy = 95.31%\n",
            "  Batch  92: Loss = 0.0891, Accuracy = 96.88%\n",
            "  Batch  93: Loss = 0.0955, Accuracy = 96.88%\n",
            "  Batch  94: Loss = 0.0425, Accuracy = 98.44%\n",
            "  Batch  95: Loss = 0.0108, Accuracy = 100.00%\n",
            "  Batch  96: Loss = 0.1448, Accuracy = 96.88%\n",
            "  Batch  97: Loss = 0.1797, Accuracy = 93.75%\n",
            "  Batch  98: Loss = 0.0490, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.1504, Accuracy = 93.75%\n",
            "  Batch 100: Loss = 0.1310, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.0595, Accuracy = 98.44%\n",
            "  Batch 102: Loss = 0.0353, Accuracy = 98.44%\n",
            "  Batch 103: Loss = 0.1664, Accuracy = 96.88%\n",
            "  Batch 104: Loss = 0.0603, Accuracy = 98.44%\n",
            "  Batch 105: Loss = 0.0999, Accuracy = 96.88%\n",
            "  Batch 106: Loss = 0.0807, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.0347, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.1214, Accuracy = 95.31%\n",
            "  Batch 109: Loss = 0.1542, Accuracy = 95.31%\n",
            "  Batch 110: Loss = 0.0523, Accuracy = 98.44%\n",
            "  Batch 111: Loss = 0.0544, Accuracy = 96.88%\n",
            "  Batch 112: Loss = 0.0384, Accuracy = 98.44%\n",
            "  Batch 113: Loss = 0.2485, Accuracy = 92.19%\n",
            "  Batch 114: Loss = 0.1444, Accuracy = 93.75%\n",
            "  Batch 115: Loss = 0.1025, Accuracy = 96.88%\n",
            "  Batch 116: Loss = 0.0850, Accuracy = 95.31%\n",
            "  Batch 117: Loss = 0.0931, Accuracy = 96.88%\n",
            "  Batch 118: Loss = 0.0251, Accuracy = 100.00%\n",
            "  Batch 119: Loss = 0.0168, Accuracy = 100.00%\n",
            "  Batch 120: Loss = 0.0228, Accuracy = 98.44%\n",
            "  Batch 121: Loss = 0.0221, Accuracy = 100.00%\n",
            "  Batch 122: Loss = 0.0793, Accuracy = 98.44%\n",
            "  Batch 123: Loss = 0.0548, Accuracy = 98.44%\n",
            "  Batch 124: Loss = 0.1103, Accuracy = 93.75%\n",
            "  Batch 125: Loss = 0.2572, Accuracy = 93.75%\n",
            "  Batch 126: Loss = 0.0448, Accuracy = 98.44%\n",
            "  Batch 127: Loss = 0.2111, Accuracy = 95.31%\n",
            "  Batch 128: Loss = 0.0971, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.0465, Accuracy = 100.00%\n",
            "  Batch 130: Loss = 0.0515, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0015, Accuracy = 100.00%\n",
            "==> Epoch 78 Summary: Total Loss = 11.7146, Accuracy = 97.15%\n",
            "\n",
            "Epoch 79\n",
            "  Batch   1: Loss = 0.1417, Accuracy = 93.75%\n",
            "  Batch   2: Loss = 0.0590, Accuracy = 98.44%\n",
            "  Batch   3: Loss = 0.1568, Accuracy = 93.75%\n",
            "  Batch   4: Loss = 0.2601, Accuracy = 92.19%\n",
            "  Batch   5: Loss = 0.0449, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.1748, Accuracy = 96.88%\n",
            "  Batch   7: Loss = 0.1000, Accuracy = 96.88%\n",
            "  Batch   8: Loss = 0.2169, Accuracy = 92.19%\n",
            "  Batch   9: Loss = 0.0645, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.0574, Accuracy = 96.88%\n",
            "  Batch  11: Loss = 0.0411, Accuracy = 98.44%\n",
            "  Batch  12: Loss = 0.2826, Accuracy = 92.19%\n",
            "  Batch  13: Loss = 0.0299, Accuracy = 100.00%\n",
            "  Batch  14: Loss = 0.0848, Accuracy = 96.88%\n",
            "  Batch  15: Loss = 0.0446, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.0735, Accuracy = 98.44%\n",
            "  Batch  17: Loss = 0.0373, Accuracy = 100.00%\n",
            "  Batch  18: Loss = 0.0500, Accuracy = 98.44%\n",
            "  Batch  19: Loss = 0.2293, Accuracy = 89.06%\n",
            "  Batch  20: Loss = 0.0718, Accuracy = 93.75%\n",
            "  Batch  21: Loss = 0.0579, Accuracy = 95.31%\n",
            "  Batch  22: Loss = 0.1009, Accuracy = 93.75%\n",
            "  Batch  23: Loss = 0.1086, Accuracy = 92.19%\n",
            "  Batch  24: Loss = 0.1300, Accuracy = 93.75%\n",
            "  Batch  25: Loss = 0.0917, Accuracy = 98.44%\n",
            "  Batch  26: Loss = 0.0756, Accuracy = 95.31%\n",
            "  Batch  27: Loss = 0.1507, Accuracy = 95.31%\n",
            "  Batch  28: Loss = 0.0923, Accuracy = 93.75%\n",
            "  Batch  29: Loss = 0.1060, Accuracy = 96.88%\n",
            "  Batch  30: Loss = 0.1511, Accuracy = 96.88%\n",
            "  Batch  31: Loss = 0.2636, Accuracy = 92.19%\n",
            "  Batch  32: Loss = 0.0616, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.0261, Accuracy = 100.00%\n",
            "  Batch  34: Loss = 0.0463, Accuracy = 100.00%\n",
            "  Batch  35: Loss = 0.0833, Accuracy = 96.88%\n",
            "  Batch  36: Loss = 0.0853, Accuracy = 96.88%\n",
            "  Batch  37: Loss = 0.1460, Accuracy = 95.31%\n",
            "  Batch  38: Loss = 0.1421, Accuracy = 96.88%\n",
            "  Batch  39: Loss = 0.1236, Accuracy = 95.31%\n",
            "  Batch  40: Loss = 0.1073, Accuracy = 95.31%\n",
            "  Batch  41: Loss = 0.0653, Accuracy = 95.31%\n",
            "  Batch  42: Loss = 0.0659, Accuracy = 96.88%\n",
            "  Batch  43: Loss = 0.0604, Accuracy = 95.31%\n",
            "  Batch  44: Loss = 0.0910, Accuracy = 96.88%\n",
            "  Batch  45: Loss = 0.0514, Accuracy = 98.44%\n",
            "  Batch  46: Loss = 0.0672, Accuracy = 96.88%\n",
            "  Batch  47: Loss = 0.0214, Accuracy = 98.44%\n",
            "  Batch  48: Loss = 0.0919, Accuracy = 95.31%\n",
            "  Batch  49: Loss = 0.0917, Accuracy = 96.88%\n",
            "  Batch  50: Loss = 0.0682, Accuracy = 98.44%\n",
            "  Batch  51: Loss = 0.1518, Accuracy = 93.75%\n",
            "  Batch  52: Loss = 0.1844, Accuracy = 89.06%\n",
            "  Batch  53: Loss = 0.0523, Accuracy = 98.44%\n",
            "  Batch  54: Loss = 0.0464, Accuracy = 96.88%\n",
            "  Batch  55: Loss = 0.0718, Accuracy = 96.88%\n",
            "  Batch  56: Loss = 0.1396, Accuracy = 93.75%\n",
            "  Batch  57: Loss = 0.1102, Accuracy = 95.31%\n",
            "  Batch  58: Loss = 0.0937, Accuracy = 95.31%\n",
            "  Batch  59: Loss = 0.0810, Accuracy = 96.88%\n",
            "  Batch  60: Loss = 0.1047, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.1319, Accuracy = 95.31%\n",
            "  Batch  62: Loss = 0.1174, Accuracy = 96.88%\n",
            "  Batch  63: Loss = 0.1041, Accuracy = 96.88%\n",
            "  Batch  64: Loss = 0.1494, Accuracy = 93.75%\n",
            "  Batch  65: Loss = 0.0515, Accuracy = 98.44%\n",
            "  Batch  66: Loss = 0.0850, Accuracy = 95.31%\n",
            "  Batch  67: Loss = 0.0441, Accuracy = 98.44%\n",
            "  Batch  68: Loss = 0.0142, Accuracy = 100.00%\n",
            "  Batch  69: Loss = 0.1055, Accuracy = 96.88%\n",
            "  Batch  70: Loss = 0.0580, Accuracy = 98.44%\n",
            "  Batch  71: Loss = 0.1239, Accuracy = 96.88%\n",
            "  Batch  72: Loss = 0.0661, Accuracy = 98.44%\n",
            "  Batch  73: Loss = 0.0894, Accuracy = 93.75%\n",
            "  Batch  74: Loss = 0.0272, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.1774, Accuracy = 95.31%\n",
            "  Batch  76: Loss = 0.1487, Accuracy = 95.31%\n",
            "  Batch  77: Loss = 0.0266, Accuracy = 100.00%\n",
            "  Batch  78: Loss = 0.0426, Accuracy = 100.00%\n",
            "  Batch  79: Loss = 0.0274, Accuracy = 98.44%\n",
            "  Batch  80: Loss = 0.0085, Accuracy = 100.00%\n",
            "  Batch  81: Loss = 0.1078, Accuracy = 96.88%\n",
            "  Batch  82: Loss = 0.0364, Accuracy = 98.44%\n",
            "  Batch  83: Loss = 0.0482, Accuracy = 98.44%\n",
            "  Batch  84: Loss = 0.1374, Accuracy = 93.75%\n",
            "  Batch  85: Loss = 0.0938, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.0849, Accuracy = 96.88%\n",
            "  Batch  87: Loss = 0.0916, Accuracy = 96.88%\n",
            "  Batch  88: Loss = 0.0307, Accuracy = 100.00%\n",
            "  Batch  89: Loss = 0.0553, Accuracy = 96.88%\n",
            "  Batch  90: Loss = 0.1441, Accuracy = 93.75%\n",
            "  Batch  91: Loss = 0.0288, Accuracy = 98.44%\n",
            "  Batch  92: Loss = 0.0201, Accuracy = 100.00%\n",
            "  Batch  93: Loss = 0.0172, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.0205, Accuracy = 100.00%\n",
            "  Batch  95: Loss = 0.0949, Accuracy = 93.75%\n",
            "  Batch  96: Loss = 0.0490, Accuracy = 100.00%\n",
            "  Batch  97: Loss = 0.0647, Accuracy = 95.31%\n",
            "  Batch  98: Loss = 0.1276, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.0211, Accuracy = 100.00%\n",
            "  Batch 100: Loss = 0.0148, Accuracy = 100.00%\n",
            "  Batch 101: Loss = 0.1047, Accuracy = 95.31%\n",
            "  Batch 102: Loss = 0.1137, Accuracy = 95.31%\n",
            "  Batch 103: Loss = 0.0165, Accuracy = 100.00%\n",
            "  Batch 104: Loss = 0.0475, Accuracy = 98.44%\n",
            "  Batch 105: Loss = 0.0446, Accuracy = 98.44%\n",
            "  Batch 106: Loss = 0.1358, Accuracy = 95.31%\n",
            "  Batch 107: Loss = 0.0784, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.1163, Accuracy = 95.31%\n",
            "  Batch 109: Loss = 0.0575, Accuracy = 98.44%\n",
            "  Batch 110: Loss = 0.0770, Accuracy = 98.44%\n",
            "  Batch 111: Loss = 0.0649, Accuracy = 95.31%\n",
            "  Batch 112: Loss = 0.0266, Accuracy = 98.44%\n",
            "  Batch 113: Loss = 0.0458, Accuracy = 96.88%\n",
            "  Batch 114: Loss = 0.1117, Accuracy = 95.31%\n",
            "  Batch 115: Loss = 0.1262, Accuracy = 95.31%\n",
            "  Batch 116: Loss = 0.0822, Accuracy = 96.88%\n",
            "  Batch 117: Loss = 0.1144, Accuracy = 95.31%\n",
            "  Batch 118: Loss = 0.0678, Accuracy = 98.44%\n",
            "  Batch 119: Loss = 0.1016, Accuracy = 96.88%\n",
            "  Batch 120: Loss = 0.0728, Accuracy = 96.88%\n",
            "  Batch 121: Loss = 0.0643, Accuracy = 96.88%\n",
            "  Batch 122: Loss = 0.0568, Accuracy = 96.88%\n",
            "  Batch 123: Loss = 0.1101, Accuracy = 95.31%\n",
            "  Batch 124: Loss = 0.0717, Accuracy = 98.44%\n",
            "  Batch 125: Loss = 0.0924, Accuracy = 93.75%\n",
            "  Batch 126: Loss = 0.1164, Accuracy = 95.31%\n",
            "  Batch 127: Loss = 0.0769, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0782, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.1685, Accuracy = 93.75%\n",
            "  Batch 130: Loss = 0.2282, Accuracy = 93.75%\n",
            "  Batch 131: Loss = 0.6847, Accuracy = 80.00%\n",
            "==> Epoch 79 Summary: Total Loss = 12.3234, Accuracy = 96.64%\n",
            "\n",
            "Epoch 80\n",
            "  Batch   1: Loss = 0.1313, Accuracy = 95.31%\n",
            "  Batch   2: Loss = 0.4872, Accuracy = 87.50%\n",
            "  Batch   3: Loss = 0.7789, Accuracy = 79.69%\n",
            "  Batch   4: Loss = 0.9645, Accuracy = 76.56%\n",
            "  Batch   5: Loss = 0.5636, Accuracy = 81.25%\n",
            "  Batch   6: Loss = 0.7123, Accuracy = 82.81%\n",
            "  Batch   7: Loss = 0.2236, Accuracy = 90.62%\n",
            "  Batch   8: Loss = 0.4532, Accuracy = 85.94%\n",
            "  Batch   9: Loss = 0.7075, Accuracy = 76.56%\n",
            "  Batch  10: Loss = 0.3304, Accuracy = 89.06%\n",
            "  Batch  11: Loss = 0.7257, Accuracy = 82.81%\n",
            "  Batch  12: Loss = 0.6779, Accuracy = 84.38%\n",
            "  Batch  13: Loss = 0.4516, Accuracy = 89.06%\n",
            "  Batch  14: Loss = 0.5292, Accuracy = 92.19%\n",
            "  Batch  15: Loss = 0.5664, Accuracy = 87.50%\n",
            "  Batch  16: Loss = 0.4920, Accuracy = 89.06%\n",
            "  Batch  17: Loss = 0.3168, Accuracy = 92.19%\n",
            "  Batch  18: Loss = 0.3316, Accuracy = 90.62%\n",
            "  Batch  19: Loss = 0.1642, Accuracy = 95.31%\n",
            "  Batch  20: Loss = 0.2775, Accuracy = 90.62%\n",
            "  Batch  21: Loss = 0.1978, Accuracy = 95.31%\n",
            "  Batch  22: Loss = 0.2909, Accuracy = 87.50%\n",
            "  Batch  23: Loss = 0.1917, Accuracy = 92.19%\n",
            "  Batch  24: Loss = 0.1171, Accuracy = 95.31%\n",
            "  Batch  25: Loss = 0.3854, Accuracy = 93.75%\n",
            "  Batch  26: Loss = 0.3349, Accuracy = 89.06%\n",
            "  Batch  27: Loss = 0.1871, Accuracy = 95.31%\n",
            "  Batch  28: Loss = 0.1765, Accuracy = 92.19%\n",
            "  Batch  29: Loss = 0.2142, Accuracy = 90.62%\n",
            "  Batch  30: Loss = 0.1883, Accuracy = 95.31%\n",
            "  Batch  31: Loss = 0.3034, Accuracy = 87.50%\n",
            "  Batch  32: Loss = 0.2357, Accuracy = 92.19%\n",
            "  Batch  33: Loss = 0.1672, Accuracy = 95.31%\n",
            "  Batch  34: Loss = 0.0481, Accuracy = 100.00%\n",
            "  Batch  35: Loss = 0.1455, Accuracy = 93.75%\n",
            "  Batch  36: Loss = 0.2469, Accuracy = 95.31%\n",
            "  Batch  37: Loss = 0.1569, Accuracy = 95.31%\n",
            "  Batch  38: Loss = 0.1099, Accuracy = 96.88%\n",
            "  Batch  39: Loss = 0.1709, Accuracy = 93.75%\n",
            "  Batch  40: Loss = 0.1530, Accuracy = 92.19%\n",
            "  Batch  41: Loss = 0.1697, Accuracy = 93.75%\n",
            "  Batch  42: Loss = 0.1722, Accuracy = 93.75%\n",
            "  Batch  43: Loss = 0.1354, Accuracy = 98.44%\n",
            "  Batch  44: Loss = 0.2192, Accuracy = 93.75%\n",
            "  Batch  45: Loss = 0.2656, Accuracy = 89.06%\n",
            "  Batch  46: Loss = 0.2782, Accuracy = 89.06%\n",
            "  Batch  47: Loss = 0.1294, Accuracy = 96.88%\n",
            "  Batch  48: Loss = 0.2684, Accuracy = 90.62%\n",
            "  Batch  49: Loss = 0.1725, Accuracy = 96.88%\n",
            "  Batch  50: Loss = 0.2769, Accuracy = 92.19%\n",
            "  Batch  51: Loss = 0.1878, Accuracy = 92.19%\n",
            "  Batch  52: Loss = 0.1267, Accuracy = 96.88%\n",
            "  Batch  53: Loss = 0.0493, Accuracy = 98.44%\n",
            "  Batch  54: Loss = 0.1801, Accuracy = 95.31%\n",
            "  Batch  55: Loss = 0.1395, Accuracy = 92.19%\n",
            "  Batch  56: Loss = 0.1581, Accuracy = 93.75%\n",
            "  Batch  57: Loss = 0.1970, Accuracy = 92.19%\n",
            "  Batch  58: Loss = 0.2843, Accuracy = 93.75%\n",
            "  Batch  59: Loss = 0.1981, Accuracy = 93.75%\n",
            "  Batch  60: Loss = 0.1018, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.1359, Accuracy = 95.31%\n",
            "  Batch  62: Loss = 0.1054, Accuracy = 95.31%\n",
            "  Batch  63: Loss = 0.2256, Accuracy = 92.19%\n",
            "  Batch  64: Loss = 0.0834, Accuracy = 98.44%\n",
            "  Batch  65: Loss = 0.0567, Accuracy = 98.44%\n",
            "  Batch  66: Loss = 0.0744, Accuracy = 96.88%\n",
            "  Batch  67: Loss = 0.0662, Accuracy = 96.88%\n",
            "  Batch  68: Loss = 0.1225, Accuracy = 93.75%\n",
            "  Batch  69: Loss = 0.0569, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.2282, Accuracy = 90.62%\n",
            "  Batch  71: Loss = 0.0705, Accuracy = 96.88%\n",
            "  Batch  72: Loss = 0.2649, Accuracy = 85.94%\n",
            "  Batch  73: Loss = 0.1488, Accuracy = 90.62%\n",
            "  Batch  74: Loss = 0.0312, Accuracy = 100.00%\n",
            "  Batch  75: Loss = 0.0444, Accuracy = 96.88%\n",
            "  Batch  76: Loss = 0.1112, Accuracy = 96.88%\n",
            "  Batch  77: Loss = 0.0824, Accuracy = 95.31%\n",
            "  Batch  78: Loss = 0.1835, Accuracy = 93.75%\n",
            "  Batch  79: Loss = 0.0504, Accuracy = 98.44%\n",
            "  Batch  80: Loss = 0.0833, Accuracy = 96.88%\n",
            "  Batch  81: Loss = 0.0752, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.0462, Accuracy = 98.44%\n",
            "  Batch  83: Loss = 0.1441, Accuracy = 95.31%\n",
            "  Batch  84: Loss = 0.0597, Accuracy = 98.44%\n",
            "  Batch  85: Loss = 0.0354, Accuracy = 100.00%\n",
            "  Batch  86: Loss = 0.0487, Accuracy = 98.44%\n",
            "  Batch  87: Loss = 0.0536, Accuracy = 100.00%\n",
            "  Batch  88: Loss = 0.1014, Accuracy = 96.88%\n",
            "  Batch  89: Loss = 0.0839, Accuracy = 96.88%\n",
            "  Batch  90: Loss = 0.0960, Accuracy = 96.88%\n",
            "  Batch  91: Loss = 0.1442, Accuracy = 93.75%\n",
            "  Batch  92: Loss = 0.0794, Accuracy = 95.31%\n",
            "  Batch  93: Loss = 0.0592, Accuracy = 96.88%\n",
            "  Batch  94: Loss = 0.1704, Accuracy = 96.88%\n",
            "  Batch  95: Loss = 0.0780, Accuracy = 98.44%\n",
            "  Batch  96: Loss = 0.0679, Accuracy = 96.88%\n",
            "  Batch  97: Loss = 0.1312, Accuracy = 93.75%\n",
            "  Batch  98: Loss = 0.0431, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.1965, Accuracy = 92.19%\n",
            "  Batch 100: Loss = 0.1271, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.0352, Accuracy = 98.44%\n",
            "  Batch 102: Loss = 0.0691, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.0904, Accuracy = 95.31%\n",
            "  Batch 104: Loss = 0.0574, Accuracy = 98.44%\n",
            "  Batch 105: Loss = 0.1122, Accuracy = 95.31%\n",
            "  Batch 106: Loss = 0.0136, Accuracy = 100.00%\n",
            "  Batch 107: Loss = 0.1007, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.1025, Accuracy = 95.31%\n",
            "  Batch 109: Loss = 0.0855, Accuracy = 96.88%\n",
            "  Batch 110: Loss = 0.0542, Accuracy = 100.00%\n",
            "  Batch 111: Loss = 0.0508, Accuracy = 98.44%\n",
            "  Batch 112: Loss = 0.1073, Accuracy = 96.88%\n",
            "  Batch 113: Loss = 0.1418, Accuracy = 95.31%\n",
            "  Batch 114: Loss = 0.1072, Accuracy = 95.31%\n",
            "  Batch 115: Loss = 0.1178, Accuracy = 96.88%\n",
            "  Batch 116: Loss = 0.0965, Accuracy = 96.88%\n",
            "  Batch 117: Loss = 0.1542, Accuracy = 96.88%\n",
            "  Batch 118: Loss = 0.1112, Accuracy = 93.75%\n",
            "  Batch 119: Loss = 0.1123, Accuracy = 95.31%\n",
            "  Batch 120: Loss = 0.0792, Accuracy = 96.88%\n",
            "  Batch 121: Loss = 0.1201, Accuracy = 92.19%\n",
            "  Batch 122: Loss = 0.0977, Accuracy = 96.88%\n",
            "  Batch 123: Loss = 0.0512, Accuracy = 100.00%\n",
            "  Batch 124: Loss = 0.0406, Accuracy = 98.44%\n",
            "  Batch 125: Loss = 0.3190, Accuracy = 89.06%\n",
            "  Batch 126: Loss = 0.0359, Accuracy = 100.00%\n",
            "  Batch 127: Loss = 0.0503, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0679, Accuracy = 96.88%\n",
            "  Batch 129: Loss = 0.1734, Accuracy = 95.31%\n",
            "  Batch 130: Loss = 0.0497, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0115, Accuracy = 100.00%\n",
            "==> Epoch 80 Summary: Total Loss = 24.5000, Accuracy = 94.13%\n",
            "\n",
            "Epoch 81\n",
            "  Batch   1: Loss = 0.1420, Accuracy = 95.31%\n",
            "  Batch   2: Loss = 0.1761, Accuracy = 95.31%\n",
            "  Batch   3: Loss = 0.1217, Accuracy = 95.31%\n",
            "  Batch   4: Loss = 0.0444, Accuracy = 100.00%\n",
            "  Batch   5: Loss = 0.0159, Accuracy = 100.00%\n",
            "  Batch   6: Loss = 0.2260, Accuracy = 89.06%\n",
            "  Batch   7: Loss = 0.0593, Accuracy = 98.44%\n",
            "  Batch   8: Loss = 0.0485, Accuracy = 98.44%\n",
            "  Batch   9: Loss = 0.0232, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.0519, Accuracy = 98.44%\n",
            "  Batch  11: Loss = 0.1609, Accuracy = 96.88%\n",
            "  Batch  12: Loss = 0.0305, Accuracy = 100.00%\n",
            "  Batch  13: Loss = 0.0286, Accuracy = 100.00%\n",
            "  Batch  14: Loss = 0.0549, Accuracy = 98.44%\n",
            "  Batch  15: Loss = 0.0841, Accuracy = 93.75%\n",
            "  Batch  16: Loss = 0.1229, Accuracy = 96.88%\n",
            "  Batch  17: Loss = 0.0708, Accuracy = 96.88%\n",
            "  Batch  18: Loss = 0.2629, Accuracy = 95.31%\n",
            "  Batch  19: Loss = 0.0332, Accuracy = 100.00%\n",
            "  Batch  20: Loss = 0.2507, Accuracy = 92.19%\n",
            "  Batch  21: Loss = 0.0725, Accuracy = 96.88%\n",
            "  Batch  22: Loss = 0.2682, Accuracy = 92.19%\n",
            "  Batch  23: Loss = 0.1084, Accuracy = 96.88%\n",
            "  Batch  24: Loss = 0.1633, Accuracy = 93.75%\n",
            "  Batch  25: Loss = 0.0465, Accuracy = 98.44%\n",
            "  Batch  26: Loss = 0.0154, Accuracy = 100.00%\n",
            "  Batch  27: Loss = 0.2280, Accuracy = 95.31%\n",
            "  Batch  28: Loss = 0.0471, Accuracy = 98.44%\n",
            "  Batch  29: Loss = 0.2326, Accuracy = 93.75%\n",
            "  Batch  30: Loss = 0.0655, Accuracy = 96.88%\n",
            "  Batch  31: Loss = 0.0891, Accuracy = 95.31%\n",
            "  Batch  32: Loss = 0.0288, Accuracy = 100.00%\n",
            "  Batch  33: Loss = 0.1338, Accuracy = 96.88%\n",
            "  Batch  34: Loss = 0.1100, Accuracy = 98.44%\n",
            "  Batch  35: Loss = 0.0352, Accuracy = 100.00%\n",
            "  Batch  36: Loss = 0.0891, Accuracy = 96.88%\n",
            "  Batch  37: Loss = 0.1625, Accuracy = 95.31%\n",
            "  Batch  38: Loss = 0.0949, Accuracy = 96.88%\n",
            "  Batch  39: Loss = 0.2861, Accuracy = 92.19%\n",
            "  Batch  40: Loss = 0.1290, Accuracy = 95.31%\n",
            "  Batch  41: Loss = 0.2211, Accuracy = 90.62%\n",
            "  Batch  42: Loss = 0.0778, Accuracy = 96.88%\n",
            "  Batch  43: Loss = 0.1183, Accuracy = 93.75%\n",
            "  Batch  44: Loss = 0.0817, Accuracy = 96.88%\n",
            "  Batch  45: Loss = 0.1802, Accuracy = 95.31%\n",
            "  Batch  46: Loss = 0.0591, Accuracy = 96.88%\n",
            "  Batch  47: Loss = 0.0608, Accuracy = 98.44%\n",
            "  Batch  48: Loss = 0.1145, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.0393, Accuracy = 98.44%\n",
            "  Batch  50: Loss = 0.1696, Accuracy = 95.31%\n",
            "  Batch  51: Loss = 0.0228, Accuracy = 100.00%\n",
            "  Batch  52: Loss = 0.2501, Accuracy = 90.62%\n",
            "  Batch  53: Loss = 0.0287, Accuracy = 100.00%\n",
            "  Batch  54: Loss = 0.1549, Accuracy = 95.31%\n",
            "  Batch  55: Loss = 0.0824, Accuracy = 98.44%\n",
            "  Batch  56: Loss = 0.1036, Accuracy = 95.31%\n",
            "  Batch  57: Loss = 0.0564, Accuracy = 98.44%\n",
            "  Batch  58: Loss = 0.1149, Accuracy = 96.88%\n",
            "  Batch  59: Loss = 0.0477, Accuracy = 100.00%\n",
            "  Batch  60: Loss = 0.1050, Accuracy = 93.75%\n",
            "  Batch  61: Loss = 0.1031, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.0589, Accuracy = 98.44%\n",
            "  Batch  63: Loss = 0.0240, Accuracy = 100.00%\n",
            "  Batch  64: Loss = 0.0113, Accuracy = 100.00%\n",
            "  Batch  65: Loss = 0.1230, Accuracy = 96.88%\n",
            "  Batch  66: Loss = 0.0330, Accuracy = 98.44%\n",
            "  Batch  67: Loss = 0.2102, Accuracy = 92.19%\n",
            "  Batch  68: Loss = 0.1607, Accuracy = 95.31%\n",
            "  Batch  69: Loss = 0.0444, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.0851, Accuracy = 95.31%\n",
            "  Batch  71: Loss = 0.1088, Accuracy = 96.88%\n",
            "  Batch  72: Loss = 0.1434, Accuracy = 93.75%\n",
            "  Batch  73: Loss = 0.1177, Accuracy = 98.44%\n",
            "  Batch  74: Loss = 0.1772, Accuracy = 93.75%\n",
            "  Batch  75: Loss = 0.0156, Accuracy = 100.00%\n",
            "  Batch  76: Loss = 0.0928, Accuracy = 96.88%\n",
            "  Batch  77: Loss = 0.0156, Accuracy = 100.00%\n",
            "  Batch  78: Loss = 0.0399, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.0747, Accuracy = 96.88%\n",
            "  Batch  80: Loss = 0.0738, Accuracy = 96.88%\n",
            "  Batch  81: Loss = 0.0764, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.0561, Accuracy = 98.44%\n",
            "  Batch  83: Loss = 0.0188, Accuracy = 100.00%\n",
            "  Batch  84: Loss = 0.0940, Accuracy = 93.75%\n",
            "  Batch  85: Loss = 0.1595, Accuracy = 93.75%\n",
            "  Batch  86: Loss = 0.0999, Accuracy = 96.88%\n",
            "  Batch  87: Loss = 0.0579, Accuracy = 98.44%\n",
            "  Batch  88: Loss = 0.0783, Accuracy = 93.75%\n",
            "  Batch  89: Loss = 0.0942, Accuracy = 96.88%\n",
            "  Batch  90: Loss = 0.0884, Accuracy = 96.88%\n",
            "  Batch  91: Loss = 0.0128, Accuracy = 100.00%\n",
            "  Batch  92: Loss = 0.0235, Accuracy = 100.00%\n",
            "  Batch  93: Loss = 0.1005, Accuracy = 98.44%\n",
            "  Batch  94: Loss = 0.0366, Accuracy = 98.44%\n",
            "  Batch  95: Loss = 0.0175, Accuracy = 100.00%\n",
            "  Batch  96: Loss = 0.0922, Accuracy = 96.88%\n",
            "  Batch  97: Loss = 0.0490, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.1214, Accuracy = 96.88%\n",
            "  Batch  99: Loss = 0.0623, Accuracy = 98.44%\n",
            "  Batch 100: Loss = 0.1102, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.0192, Accuracy = 100.00%\n",
            "  Batch 102: Loss = 0.0564, Accuracy = 98.44%\n",
            "  Batch 103: Loss = 0.1005, Accuracy = 96.88%\n",
            "  Batch 104: Loss = 0.1099, Accuracy = 95.31%\n",
            "  Batch 105: Loss = 0.0704, Accuracy = 96.88%\n",
            "  Batch 106: Loss = 0.0439, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.1351, Accuracy = 95.31%\n",
            "  Batch 108: Loss = 0.0826, Accuracy = 98.44%\n",
            "  Batch 109: Loss = 0.1477, Accuracy = 93.75%\n",
            "  Batch 110: Loss = 0.0233, Accuracy = 100.00%\n",
            "  Batch 111: Loss = 0.0816, Accuracy = 96.88%\n",
            "  Batch 112: Loss = 0.1001, Accuracy = 98.44%\n",
            "  Batch 113: Loss = 0.1312, Accuracy = 93.75%\n",
            "  Batch 114: Loss = 0.0402, Accuracy = 98.44%\n",
            "  Batch 115: Loss = 0.1012, Accuracy = 93.75%\n",
            "  Batch 116: Loss = 0.0110, Accuracy = 100.00%\n",
            "  Batch 117: Loss = 0.0506, Accuracy = 98.44%\n",
            "  Batch 118: Loss = 0.0452, Accuracy = 100.00%\n",
            "  Batch 119: Loss = 0.0210, Accuracy = 100.00%\n",
            "  Batch 120: Loss = 0.0533, Accuracy = 98.44%\n",
            "  Batch 121: Loss = 0.1520, Accuracy = 96.88%\n",
            "  Batch 122: Loss = 0.0513, Accuracy = 100.00%\n",
            "  Batch 123: Loss = 0.0213, Accuracy = 98.44%\n",
            "  Batch 124: Loss = 0.0559, Accuracy = 96.88%\n",
            "  Batch 125: Loss = 0.0393, Accuracy = 100.00%\n",
            "  Batch 126: Loss = 0.0552, Accuracy = 98.44%\n",
            "  Batch 127: Loss = 0.1002, Accuracy = 96.88%\n",
            "  Batch 128: Loss = 0.0533, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.0944, Accuracy = 95.31%\n",
            "  Batch 130: Loss = 0.0119, Accuracy = 100.00%\n",
            "  Batch 131: Loss = 0.0013, Accuracy = 100.00%\n",
            "==> Epoch 81 Summary: Total Loss = 11.7266, Accuracy = 97.09%\n",
            "\n",
            "Epoch 82\n",
            "  Batch   1: Loss = 0.0553, Accuracy = 98.44%\n",
            "  Batch   2: Loss = 0.0407, Accuracy = 98.44%\n",
            "  Batch   3: Loss = 0.0996, Accuracy = 96.88%\n",
            "  Batch   4: Loss = 0.0095, Accuracy = 100.00%\n",
            "  Batch   5: Loss = 0.0198, Accuracy = 100.00%\n",
            "  Batch   6: Loss = 0.0592, Accuracy = 96.88%\n",
            "  Batch   7: Loss = 0.0301, Accuracy = 98.44%\n",
            "  Batch   8: Loss = 0.0693, Accuracy = 96.88%\n",
            "  Batch   9: Loss = 0.1242, Accuracy = 95.31%\n",
            "  Batch  10: Loss = 0.0474, Accuracy = 98.44%\n",
            "  Batch  11: Loss = 0.0342, Accuracy = 98.44%\n",
            "  Batch  12: Loss = 0.0218, Accuracy = 100.00%\n",
            "  Batch  13: Loss = 0.0598, Accuracy = 95.31%\n",
            "  Batch  14: Loss = 0.0177, Accuracy = 100.00%\n",
            "  Batch  15: Loss = 0.0775, Accuracy = 96.88%\n",
            "  Batch  16: Loss = 0.1506, Accuracy = 93.75%\n",
            "  Batch  17: Loss = 0.0547, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.0843, Accuracy = 95.31%\n",
            "  Batch  19: Loss = 0.0745, Accuracy = 96.88%\n",
            "  Batch  20: Loss = 0.0358, Accuracy = 98.44%\n",
            "  Batch  21: Loss = 0.0938, Accuracy = 96.88%\n",
            "  Batch  22: Loss = 0.0887, Accuracy = 96.88%\n",
            "  Batch  23: Loss = 0.0717, Accuracy = 98.44%\n",
            "  Batch  24: Loss = 0.1515, Accuracy = 95.31%\n",
            "  Batch  25: Loss = 0.0765, Accuracy = 98.44%\n",
            "  Batch  26: Loss = 0.0377, Accuracy = 98.44%\n",
            "  Batch  27: Loss = 0.2475, Accuracy = 92.19%\n",
            "  Batch  28: Loss = 0.1596, Accuracy = 95.31%\n",
            "  Batch  29: Loss = 0.0204, Accuracy = 100.00%\n",
            "  Batch  30: Loss = 0.1689, Accuracy = 92.19%\n",
            "  Batch  31: Loss = 0.1237, Accuracy = 96.88%\n",
            "  Batch  32: Loss = 0.0407, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.1547, Accuracy = 95.31%\n",
            "  Batch  34: Loss = 0.0582, Accuracy = 98.44%\n",
            "  Batch  35: Loss = 0.0629, Accuracy = 96.88%\n",
            "  Batch  36: Loss = 0.1021, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.0170, Accuracy = 100.00%\n",
            "  Batch  38: Loss = 0.0375, Accuracy = 98.44%\n",
            "  Batch  39: Loss = 0.0503, Accuracy = 98.44%\n",
            "  Batch  40: Loss = 0.0566, Accuracy = 96.88%\n",
            "  Batch  41: Loss = 0.0800, Accuracy = 98.44%\n",
            "  Batch  42: Loss = 0.1072, Accuracy = 95.31%\n",
            "  Batch  43: Loss = 0.0384, Accuracy = 100.00%\n",
            "  Batch  44: Loss = 0.0499, Accuracy = 98.44%\n",
            "  Batch  45: Loss = 0.0486, Accuracy = 96.88%\n",
            "  Batch  46: Loss = 0.2246, Accuracy = 95.31%\n",
            "  Batch  47: Loss = 0.0239, Accuracy = 100.00%\n",
            "  Batch  48: Loss = 0.1324, Accuracy = 95.31%\n",
            "  Batch  49: Loss = 0.0700, Accuracy = 96.88%\n",
            "  Batch  50: Loss = 0.0186, Accuracy = 100.00%\n",
            "  Batch  51: Loss = 0.1276, Accuracy = 95.31%\n",
            "  Batch  52: Loss = 0.1520, Accuracy = 96.88%\n",
            "  Batch  53: Loss = 0.0161, Accuracy = 100.00%\n",
            "  Batch  54: Loss = 0.0255, Accuracy = 98.44%\n",
            "  Batch  55: Loss = 0.0222, Accuracy = 100.00%\n",
            "  Batch  56: Loss = 0.0744, Accuracy = 96.88%\n",
            "  Batch  57: Loss = 0.0444, Accuracy = 100.00%\n",
            "  Batch  58: Loss = 0.1425, Accuracy = 95.31%\n",
            "  Batch  59: Loss = 0.0797, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.1188, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.0624, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.0273, Accuracy = 100.00%\n",
            "  Batch  63: Loss = 0.0548, Accuracy = 100.00%\n",
            "  Batch  64: Loss = 0.0273, Accuracy = 98.44%\n",
            "  Batch  65: Loss = 0.0733, Accuracy = 98.44%\n",
            "  Batch  66: Loss = 0.0157, Accuracy = 100.00%\n",
            "  Batch  67: Loss = 0.1648, Accuracy = 92.19%\n",
            "  Batch  68: Loss = 0.0758, Accuracy = 96.88%\n",
            "  Batch  69: Loss = 0.0630, Accuracy = 96.88%\n",
            "  Batch  70: Loss = 0.2955, Accuracy = 90.62%\n",
            "  Batch  71: Loss = 0.0696, Accuracy = 95.31%\n",
            "  Batch  72: Loss = 0.0823, Accuracy = 96.88%\n",
            "  Batch  73: Loss = 0.1915, Accuracy = 95.31%\n",
            "  Batch  74: Loss = 0.0234, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.0774, Accuracy = 98.44%\n",
            "  Batch  76: Loss = 0.1141, Accuracy = 96.88%\n",
            "  Batch  77: Loss = 0.0759, Accuracy = 96.88%\n",
            "  Batch  78: Loss = 0.1498, Accuracy = 96.88%\n",
            "  Batch  79: Loss = 0.1351, Accuracy = 95.31%\n",
            "  Batch  80: Loss = 0.0931, Accuracy = 95.31%\n",
            "  Batch  81: Loss = 0.0251, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.0330, Accuracy = 96.88%\n",
            "  Batch  83: Loss = 0.0488, Accuracy = 96.88%\n",
            "  Batch  84: Loss = 0.0761, Accuracy = 98.44%\n",
            "  Batch  85: Loss = 0.0378, Accuracy = 98.44%\n",
            "  Batch  86: Loss = 0.2136, Accuracy = 93.75%\n",
            "  Batch  87: Loss = 0.0239, Accuracy = 100.00%\n",
            "  Batch  88: Loss = 0.0410, Accuracy = 98.44%\n",
            "  Batch  89: Loss = 0.0760, Accuracy = 96.88%\n",
            "  Batch  90: Loss = 0.1203, Accuracy = 95.31%\n",
            "  Batch  91: Loss = 0.0520, Accuracy = 98.44%\n",
            "  Batch  92: Loss = 0.0180, Accuracy = 100.00%\n",
            "  Batch  93: Loss = 0.0336, Accuracy = 98.44%\n",
            "  Batch  94: Loss = 0.1501, Accuracy = 93.75%\n",
            "  Batch  95: Loss = 0.0618, Accuracy = 96.88%\n",
            "  Batch  96: Loss = 0.0433, Accuracy = 98.44%\n",
            "  Batch  97: Loss = 0.0389, Accuracy = 98.44%\n",
            "  Batch  98: Loss = 0.0943, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.0717, Accuracy = 96.88%\n",
            "  Batch 100: Loss = 0.0694, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.0069, Accuracy = 100.00%\n",
            "  Batch 102: Loss = 0.0941, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.0170, Accuracy = 100.00%\n",
            "  Batch 104: Loss = 0.1864, Accuracy = 96.88%\n",
            "  Batch 105: Loss = 0.0486, Accuracy = 96.88%\n",
            "  Batch 106: Loss = 0.0312, Accuracy = 100.00%\n",
            "  Batch 107: Loss = 0.1484, Accuracy = 92.19%\n",
            "  Batch 108: Loss = 0.1056, Accuracy = 96.88%\n",
            "  Batch 109: Loss = 0.0235, Accuracy = 98.44%\n",
            "  Batch 110: Loss = 0.0608, Accuracy = 96.88%\n",
            "  Batch 111: Loss = 0.1152, Accuracy = 95.31%\n",
            "  Batch 112: Loss = 0.1045, Accuracy = 96.88%\n",
            "  Batch 113: Loss = 0.0792, Accuracy = 95.31%\n",
            "  Batch 114: Loss = 0.1159, Accuracy = 93.75%\n",
            "  Batch 115: Loss = 0.0162, Accuracy = 100.00%\n",
            "  Batch 116: Loss = 0.0111, Accuracy = 100.00%\n",
            "  Batch 117: Loss = 0.0297, Accuracy = 100.00%\n",
            "  Batch 118: Loss = 0.0515, Accuracy = 98.44%\n",
            "  Batch 119: Loss = 0.0625, Accuracy = 96.88%\n",
            "  Batch 120: Loss = 0.0621, Accuracy = 96.88%\n",
            "  Batch 121: Loss = 0.1475, Accuracy = 95.31%\n",
            "  Batch 122: Loss = 0.0628, Accuracy = 98.44%\n",
            "  Batch 123: Loss = 0.0741, Accuracy = 96.88%\n",
            "  Batch 124: Loss = 0.0376, Accuracy = 100.00%\n",
            "  Batch 125: Loss = 0.1142, Accuracy = 95.31%\n",
            "  Batch 126: Loss = 0.0677, Accuracy = 98.44%\n",
            "  Batch 127: Loss = 0.0674, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0733, Accuracy = 96.88%\n",
            "  Batch 129: Loss = 0.0781, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.0497, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0135, Accuracy = 100.00%\n",
            "==> Epoch 82 Summary: Total Loss = 9.9405, Accuracy = 97.38%\n",
            "\n",
            "Epoch 83\n",
            "  Batch   1: Loss = 0.0760, Accuracy = 98.44%\n",
            "  Batch   2: Loss = 0.0603, Accuracy = 98.44%\n",
            "  Batch   3: Loss = 0.0666, Accuracy = 96.88%\n",
            "  Batch   4: Loss = 0.0139, Accuracy = 100.00%\n",
            "  Batch   5: Loss = 0.0202, Accuracy = 100.00%\n",
            "  Batch   6: Loss = 0.0163, Accuracy = 98.44%\n",
            "  Batch   7: Loss = 0.0898, Accuracy = 96.88%\n",
            "  Batch   8: Loss = 0.0504, Accuracy = 98.44%\n",
            "  Batch   9: Loss = 0.1876, Accuracy = 93.75%\n",
            "  Batch  10: Loss = 0.1277, Accuracy = 96.88%\n",
            "  Batch  11: Loss = 0.1953, Accuracy = 92.19%\n",
            "  Batch  12: Loss = 0.0483, Accuracy = 98.44%\n",
            "  Batch  13: Loss = 0.0487, Accuracy = 96.88%\n",
            "  Batch  14: Loss = 0.0317, Accuracy = 100.00%\n",
            "  Batch  15: Loss = 0.1683, Accuracy = 96.88%\n",
            "  Batch  16: Loss = 0.1320, Accuracy = 93.75%\n",
            "  Batch  17: Loss = 0.0741, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.0483, Accuracy = 96.88%\n",
            "  Batch  19: Loss = 0.0769, Accuracy = 98.44%\n",
            "  Batch  20: Loss = 0.0121, Accuracy = 100.00%\n",
            "  Batch  21: Loss = 0.1227, Accuracy = 95.31%\n",
            "  Batch  22: Loss = 0.1148, Accuracy = 96.88%\n",
            "  Batch  23: Loss = 0.0686, Accuracy = 98.44%\n",
            "  Batch  24: Loss = 0.0694, Accuracy = 96.88%\n",
            "  Batch  25: Loss = 0.1585, Accuracy = 95.31%\n",
            "  Batch  26: Loss = 0.1496, Accuracy = 95.31%\n",
            "  Batch  27: Loss = 0.0622, Accuracy = 96.88%\n",
            "  Batch  28: Loss = 0.0276, Accuracy = 98.44%\n",
            "  Batch  29: Loss = 0.0632, Accuracy = 95.31%\n",
            "  Batch  30: Loss = 0.0442, Accuracy = 98.44%\n",
            "  Batch  31: Loss = 0.0456, Accuracy = 98.44%\n",
            "  Batch  32: Loss = 0.0278, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.0650, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.1951, Accuracy = 93.75%\n",
            "  Batch  35: Loss = 0.1424, Accuracy = 98.44%\n",
            "  Batch  36: Loss = 0.0845, Accuracy = 93.75%\n",
            "  Batch  37: Loss = 0.0380, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.0338, Accuracy = 98.44%\n",
            "  Batch  39: Loss = 0.1155, Accuracy = 96.88%\n",
            "  Batch  40: Loss = 0.0715, Accuracy = 96.88%\n",
            "  Batch  41: Loss = 0.2130, Accuracy = 90.62%\n",
            "  Batch  42: Loss = 0.0639, Accuracy = 96.88%\n",
            "  Batch  43: Loss = 0.0504, Accuracy = 98.44%\n",
            "  Batch  44: Loss = 0.1155, Accuracy = 96.88%\n",
            "  Batch  45: Loss = 0.0393, Accuracy = 98.44%\n",
            "  Batch  46: Loss = 0.0246, Accuracy = 100.00%\n",
            "  Batch  47: Loss = 0.1261, Accuracy = 93.75%\n",
            "  Batch  48: Loss = 0.1829, Accuracy = 95.31%\n",
            "  Batch  49: Loss = 0.0338, Accuracy = 100.00%\n",
            "  Batch  50: Loss = 0.0562, Accuracy = 98.44%\n",
            "  Batch  51: Loss = 0.0221, Accuracy = 100.00%\n",
            "  Batch  52: Loss = 0.0967, Accuracy = 96.88%\n",
            "  Batch  53: Loss = 0.0925, Accuracy = 96.88%\n",
            "  Batch  54: Loss = 0.1105, Accuracy = 98.44%\n",
            "  Batch  55: Loss = 0.0601, Accuracy = 96.88%\n",
            "  Batch  56: Loss = 0.0580, Accuracy = 98.44%\n",
            "  Batch  57: Loss = 0.1118, Accuracy = 95.31%\n",
            "  Batch  58: Loss = 0.0849, Accuracy = 96.88%\n",
            "  Batch  59: Loss = 0.0423, Accuracy = 100.00%\n",
            "  Batch  60: Loss = 0.0831, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.0118, Accuracy = 100.00%\n",
            "  Batch  62: Loss = 0.0689, Accuracy = 98.44%\n",
            "  Batch  63: Loss = 0.0862, Accuracy = 98.44%\n",
            "  Batch  64: Loss = 0.0998, Accuracy = 95.31%\n",
            "  Batch  65: Loss = 0.0325, Accuracy = 98.44%\n",
            "  Batch  66: Loss = 0.0339, Accuracy = 100.00%\n",
            "  Batch  67: Loss = 0.0171, Accuracy = 100.00%\n",
            "  Batch  68: Loss = 0.0655, Accuracy = 96.88%\n",
            "  Batch  69: Loss = 0.0947, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.2437, Accuracy = 93.75%\n",
            "  Batch  71: Loss = 0.0116, Accuracy = 100.00%\n",
            "  Batch  72: Loss = 0.0208, Accuracy = 100.00%\n",
            "  Batch  73: Loss = 0.0790, Accuracy = 96.88%\n",
            "  Batch  74: Loss = 0.1435, Accuracy = 96.88%\n",
            "  Batch  75: Loss = 0.0480, Accuracy = 98.44%\n",
            "  Batch  76: Loss = 0.0585, Accuracy = 98.44%\n",
            "  Batch  77: Loss = 0.2406, Accuracy = 92.19%\n",
            "  Batch  78: Loss = 0.1687, Accuracy = 93.75%\n",
            "  Batch  79: Loss = 0.3372, Accuracy = 89.06%\n",
            "  Batch  80: Loss = 0.1132, Accuracy = 95.31%\n",
            "  Batch  81: Loss = 0.0433, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.1503, Accuracy = 96.88%\n",
            "  Batch  83: Loss = 0.1280, Accuracy = 96.88%\n",
            "  Batch  84: Loss = 0.1037, Accuracy = 93.75%\n",
            "  Batch  85: Loss = 0.0746, Accuracy = 95.31%\n",
            "  Batch  86: Loss = 0.0848, Accuracy = 96.88%\n",
            "  Batch  87: Loss = 0.0938, Accuracy = 96.88%\n",
            "  Batch  88: Loss = 0.1238, Accuracy = 93.75%\n",
            "  Batch  89: Loss = 0.0900, Accuracy = 96.88%\n",
            "  Batch  90: Loss = 0.0821, Accuracy = 96.88%\n",
            "  Batch  91: Loss = 0.1023, Accuracy = 96.88%\n",
            "  Batch  92: Loss = 0.0565, Accuracy = 98.44%\n",
            "  Batch  93: Loss = 0.0922, Accuracy = 96.88%\n",
            "  Batch  94: Loss = 0.1023, Accuracy = 95.31%\n",
            "  Batch  95: Loss = 0.0284, Accuracy = 100.00%\n",
            "  Batch  96: Loss = 0.0775, Accuracy = 98.44%\n",
            "  Batch  97: Loss = 0.0260, Accuracy = 100.00%\n",
            "  Batch  98: Loss = 0.1288, Accuracy = 95.31%\n",
            "  Batch  99: Loss = 0.2082, Accuracy = 90.62%\n",
            "  Batch 100: Loss = 0.0223, Accuracy = 100.00%\n",
            "  Batch 101: Loss = 0.0219, Accuracy = 100.00%\n",
            "  Batch 102: Loss = 0.1126, Accuracy = 95.31%\n",
            "  Batch 103: Loss = 0.0242, Accuracy = 100.00%\n",
            "  Batch 104: Loss = 0.0935, Accuracy = 96.88%\n",
            "  Batch 105: Loss = 0.0267, Accuracy = 98.44%\n",
            "  Batch 106: Loss = 0.1499, Accuracy = 92.19%\n",
            "  Batch 107: Loss = 0.0899, Accuracy = 95.31%\n",
            "  Batch 108: Loss = 0.1750, Accuracy = 95.31%\n",
            "  Batch 109: Loss = 0.1397, Accuracy = 96.88%\n",
            "  Batch 110: Loss = 0.0665, Accuracy = 96.88%\n",
            "  Batch 111: Loss = 0.1007, Accuracy = 96.88%\n",
            "  Batch 112: Loss = 0.2482, Accuracy = 92.19%\n",
            "  Batch 113: Loss = 0.0640, Accuracy = 96.88%\n",
            "  Batch 114: Loss = 0.1018, Accuracy = 96.88%\n",
            "  Batch 115: Loss = 0.0574, Accuracy = 98.44%\n",
            "  Batch 116: Loss = 0.0744, Accuracy = 98.44%\n",
            "  Batch 117: Loss = 0.1275, Accuracy = 98.44%\n",
            "  Batch 118: Loss = 0.1825, Accuracy = 95.31%\n",
            "  Batch 119: Loss = 0.1499, Accuracy = 92.19%\n",
            "  Batch 120: Loss = 0.1505, Accuracy = 93.75%\n",
            "  Batch 121: Loss = 0.1292, Accuracy = 93.75%\n",
            "  Batch 122: Loss = 0.0386, Accuracy = 98.44%\n",
            "  Batch 123: Loss = 0.0865, Accuracy = 98.44%\n",
            "  Batch 124: Loss = 0.0579, Accuracy = 98.44%\n",
            "  Batch 125: Loss = 0.0676, Accuracy = 98.44%\n",
            "  Batch 126: Loss = 0.0385, Accuracy = 100.00%\n",
            "  Batch 127: Loss = 0.0232, Accuracy = 100.00%\n",
            "  Batch 128: Loss = 0.0545, Accuracy = 96.88%\n",
            "  Batch 129: Loss = 0.0313, Accuracy = 100.00%\n",
            "  Batch 130: Loss = 0.0261, Accuracy = 100.00%\n",
            "  Batch 131: Loss = 0.0208, Accuracy = 100.00%\n",
            "==> Epoch 83 Summary: Total Loss = 11.4373, Accuracy = 97.08%\n",
            "\n",
            "Epoch 84\n",
            "  Batch   1: Loss = 0.0804, Accuracy = 96.88%\n",
            "  Batch   2: Loss = 0.0468, Accuracy = 98.44%\n",
            "  Batch   3: Loss = 0.0300, Accuracy = 100.00%\n",
            "  Batch   4: Loss = 0.0535, Accuracy = 96.88%\n",
            "  Batch   5: Loss = 0.0073, Accuracy = 100.00%\n",
            "  Batch   6: Loss = 0.0941, Accuracy = 96.88%\n",
            "  Batch   7: Loss = 0.0519, Accuracy = 96.88%\n",
            "  Batch   8: Loss = 0.1014, Accuracy = 95.31%\n",
            "  Batch   9: Loss = 0.2186, Accuracy = 92.19%\n",
            "  Batch  10: Loss = 0.1419, Accuracy = 95.31%\n",
            "  Batch  11: Loss = 0.0621, Accuracy = 96.88%\n",
            "  Batch  12: Loss = 0.0040, Accuracy = 100.00%\n",
            "  Batch  13: Loss = 0.0502, Accuracy = 96.88%\n",
            "  Batch  14: Loss = 0.0748, Accuracy = 98.44%\n",
            "  Batch  15: Loss = 0.0843, Accuracy = 96.88%\n",
            "  Batch  16: Loss = 0.0339, Accuracy = 100.00%\n",
            "  Batch  17: Loss = 0.0611, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.0265, Accuracy = 100.00%\n",
            "  Batch  19: Loss = 0.1056, Accuracy = 96.88%\n",
            "  Batch  20: Loss = 0.0877, Accuracy = 96.88%\n",
            "  Batch  21: Loss = 0.0601, Accuracy = 96.88%\n",
            "  Batch  22: Loss = 0.0305, Accuracy = 98.44%\n",
            "  Batch  23: Loss = 0.0514, Accuracy = 98.44%\n",
            "  Batch  24: Loss = 0.0059, Accuracy = 100.00%\n",
            "  Batch  25: Loss = 0.0208, Accuracy = 100.00%\n",
            "  Batch  26: Loss = 0.0277, Accuracy = 98.44%\n",
            "  Batch  27: Loss = 0.0118, Accuracy = 100.00%\n",
            "  Batch  28: Loss = 0.0679, Accuracy = 98.44%\n",
            "  Batch  29: Loss = 0.0519, Accuracy = 98.44%\n",
            "  Batch  30: Loss = 0.0412, Accuracy = 98.44%\n",
            "  Batch  31: Loss = 0.0465, Accuracy = 98.44%\n",
            "  Batch  32: Loss = 0.0474, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.0733, Accuracy = 96.88%\n",
            "  Batch  34: Loss = 0.2175, Accuracy = 95.31%\n",
            "  Batch  35: Loss = 0.1000, Accuracy = 95.31%\n",
            "  Batch  36: Loss = 0.0520, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.0184, Accuracy = 100.00%\n",
            "  Batch  38: Loss = 0.0188, Accuracy = 100.00%\n",
            "  Batch  39: Loss = 0.0186, Accuracy = 100.00%\n",
            "  Batch  40: Loss = 0.0491, Accuracy = 98.44%\n",
            "  Batch  41: Loss = 0.0927, Accuracy = 95.31%\n",
            "  Batch  42: Loss = 0.0840, Accuracy = 96.88%\n",
            "  Batch  43: Loss = 0.0407, Accuracy = 98.44%\n",
            "  Batch  44: Loss = 0.0299, Accuracy = 98.44%\n",
            "  Batch  45: Loss = 0.0462, Accuracy = 98.44%\n",
            "  Batch  46: Loss = 0.0608, Accuracy = 95.31%\n",
            "  Batch  47: Loss = 0.0390, Accuracy = 96.88%\n",
            "  Batch  48: Loss = 0.1586, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.0526, Accuracy = 98.44%\n",
            "  Batch  50: Loss = 0.0321, Accuracy = 98.44%\n",
            "  Batch  51: Loss = 0.0744, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.0197, Accuracy = 100.00%\n",
            "  Batch  53: Loss = 0.0836, Accuracy = 95.31%\n",
            "  Batch  54: Loss = 0.1574, Accuracy = 95.31%\n",
            "  Batch  55: Loss = 0.1282, Accuracy = 95.31%\n",
            "  Batch  56: Loss = 0.0598, Accuracy = 96.88%\n",
            "  Batch  57: Loss = 0.0693, Accuracy = 96.88%\n",
            "  Batch  58: Loss = 0.0301, Accuracy = 98.44%\n",
            "  Batch  59: Loss = 0.1278, Accuracy = 93.75%\n",
            "  Batch  60: Loss = 0.0517, Accuracy = 98.44%\n",
            "  Batch  61: Loss = 0.0537, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.0747, Accuracy = 95.31%\n",
            "  Batch  63: Loss = 0.0790, Accuracy = 95.31%\n",
            "  Batch  64: Loss = 0.0526, Accuracy = 98.44%\n",
            "  Batch  65: Loss = 0.0419, Accuracy = 98.44%\n",
            "  Batch  66: Loss = 0.0212, Accuracy = 100.00%\n",
            "  Batch  67: Loss = 0.0283, Accuracy = 100.00%\n",
            "  Batch  68: Loss = 0.0214, Accuracy = 100.00%\n",
            "  Batch  69: Loss = 0.0806, Accuracy = 95.31%\n",
            "  Batch  70: Loss = 0.0150, Accuracy = 100.00%\n",
            "  Batch  71: Loss = 0.1077, Accuracy = 95.31%\n",
            "  Batch  72: Loss = 0.0125, Accuracy = 100.00%\n",
            "  Batch  73: Loss = 0.0513, Accuracy = 98.44%\n",
            "  Batch  74: Loss = 0.0789, Accuracy = 96.88%\n",
            "  Batch  75: Loss = 0.0738, Accuracy = 98.44%\n",
            "  Batch  76: Loss = 0.0294, Accuracy = 98.44%\n",
            "  Batch  77: Loss = 0.0736, Accuracy = 98.44%\n",
            "  Batch  78: Loss = 0.0359, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.0289, Accuracy = 98.44%\n",
            "  Batch  80: Loss = 0.0712, Accuracy = 96.88%\n",
            "  Batch  81: Loss = 0.1746, Accuracy = 95.31%\n",
            "  Batch  82: Loss = 0.0489, Accuracy = 100.00%\n",
            "  Batch  83: Loss = 0.1628, Accuracy = 96.88%\n",
            "  Batch  84: Loss = 0.0301, Accuracy = 98.44%\n",
            "  Batch  85: Loss = 0.0694, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.0838, Accuracy = 98.44%\n",
            "  Batch  87: Loss = 0.0494, Accuracy = 96.88%\n",
            "  Batch  88: Loss = 0.1303, Accuracy = 95.31%\n",
            "  Batch  89: Loss = 0.0789, Accuracy = 96.88%\n",
            "  Batch  90: Loss = 0.1108, Accuracy = 95.31%\n",
            "  Batch  91: Loss = 0.0506, Accuracy = 98.44%\n",
            "  Batch  92: Loss = 0.0105, Accuracy = 100.00%\n",
            "  Batch  93: Loss = 0.0234, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.1270, Accuracy = 95.31%\n",
            "  Batch  95: Loss = 0.0116, Accuracy = 100.00%\n",
            "  Batch  96: Loss = 0.0156, Accuracy = 100.00%\n",
            "  Batch  97: Loss = 0.0215, Accuracy = 98.44%\n",
            "  Batch  98: Loss = 0.1068, Accuracy = 93.75%\n",
            "  Batch  99: Loss = 0.0342, Accuracy = 98.44%\n",
            "  Batch 100: Loss = 0.0671, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.0382, Accuracy = 100.00%\n",
            "  Batch 102: Loss = 0.0806, Accuracy = 95.31%\n",
            "  Batch 103: Loss = 0.0439, Accuracy = 98.44%\n",
            "  Batch 104: Loss = 0.1055, Accuracy = 96.88%\n",
            "  Batch 105: Loss = 0.0920, Accuracy = 95.31%\n",
            "  Batch 106: Loss = 0.1717, Accuracy = 93.75%\n",
            "  Batch 107: Loss = 0.0846, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.0280, Accuracy = 98.44%\n",
            "  Batch 109: Loss = 0.0265, Accuracy = 100.00%\n",
            "  Batch 110: Loss = 0.0212, Accuracy = 100.00%\n",
            "  Batch 111: Loss = 0.0294, Accuracy = 98.44%\n",
            "  Batch 112: Loss = 0.1706, Accuracy = 96.88%\n",
            "  Batch 113: Loss = 0.0571, Accuracy = 100.00%\n",
            "  Batch 114: Loss = 0.0494, Accuracy = 96.88%\n",
            "  Batch 115: Loss = 0.0577, Accuracy = 96.88%\n",
            "  Batch 116: Loss = 0.0541, Accuracy = 98.44%\n",
            "  Batch 117: Loss = 0.1471, Accuracy = 95.31%\n",
            "  Batch 118: Loss = 0.0879, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.0443, Accuracy = 98.44%\n",
            "  Batch 120: Loss = 0.1140, Accuracy = 95.31%\n",
            "  Batch 121: Loss = 0.0535, Accuracy = 98.44%\n",
            "  Batch 122: Loss = 0.1057, Accuracy = 98.44%\n",
            "  Batch 123: Loss = 0.0329, Accuracy = 98.44%\n",
            "  Batch 124: Loss = 0.0098, Accuracy = 100.00%\n",
            "  Batch 125: Loss = 0.0271, Accuracy = 100.00%\n",
            "  Batch 126: Loss = 0.1167, Accuracy = 96.88%\n",
            "  Batch 127: Loss = 0.0500, Accuracy = 96.88%\n",
            "  Batch 128: Loss = 0.1427, Accuracy = 95.31%\n",
            "  Batch 129: Loss = 0.0324, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.1560, Accuracy = 92.19%\n",
            "  Batch 131: Loss = 0.0062, Accuracy = 100.00%\n",
            "==> Epoch 84 Summary: Total Loss = 8.5414, Accuracy = 97.67%\n",
            "\n",
            "Epoch 85\n",
            "  Batch   1: Loss = 0.0272, Accuracy = 98.44%\n",
            "  Batch   2: Loss = 0.1083, Accuracy = 96.88%\n",
            "  Batch   3: Loss = 0.0403, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0261, Accuracy = 98.44%\n",
            "  Batch   5: Loss = 0.0612, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.1437, Accuracy = 93.75%\n",
            "  Batch   7: Loss = 0.1406, Accuracy = 95.31%\n",
            "  Batch   8: Loss = 0.0635, Accuracy = 98.44%\n",
            "  Batch   9: Loss = 0.0074, Accuracy = 100.00%\n",
            "  Batch  10: Loss = 0.0660, Accuracy = 98.44%\n",
            "  Batch  11: Loss = 0.0703, Accuracy = 98.44%\n",
            "  Batch  12: Loss = 0.0502, Accuracy = 96.88%\n",
            "  Batch  13: Loss = 0.0341, Accuracy = 98.44%\n",
            "  Batch  14: Loss = 0.0465, Accuracy = 98.44%\n",
            "  Batch  15: Loss = 0.0169, Accuracy = 100.00%\n",
            "  Batch  16: Loss = 0.1013, Accuracy = 95.31%\n",
            "  Batch  17: Loss = 0.0700, Accuracy = 96.88%\n",
            "  Batch  18: Loss = 0.0190, Accuracy = 100.00%\n",
            "  Batch  19: Loss = 0.0451, Accuracy = 98.44%\n",
            "  Batch  20: Loss = 0.0722, Accuracy = 96.88%\n",
            "  Batch  21: Loss = 0.0695, Accuracy = 95.31%\n",
            "  Batch  22: Loss = 0.0356, Accuracy = 98.44%\n",
            "  Batch  23: Loss = 0.0805, Accuracy = 98.44%\n",
            "  Batch  24: Loss = 0.1014, Accuracy = 93.75%\n",
            "  Batch  25: Loss = 0.0861, Accuracy = 96.88%\n",
            "  Batch  26: Loss = 0.1012, Accuracy = 95.31%\n",
            "  Batch  27: Loss = 0.0528, Accuracy = 96.88%\n",
            "  Batch  28: Loss = 0.0577, Accuracy = 98.44%\n",
            "  Batch  29: Loss = 0.0421, Accuracy = 100.00%\n",
            "  Batch  30: Loss = 0.0379, Accuracy = 98.44%\n",
            "  Batch  31: Loss = 0.0329, Accuracy = 96.88%\n",
            "  Batch  32: Loss = 0.0191, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.0630, Accuracy = 96.88%\n",
            "  Batch  34: Loss = 0.0890, Accuracy = 96.88%\n",
            "  Batch  35: Loss = 0.0212, Accuracy = 100.00%\n",
            "  Batch  36: Loss = 0.1166, Accuracy = 96.88%\n",
            "  Batch  37: Loss = 0.0503, Accuracy = 96.88%\n",
            "  Batch  38: Loss = 0.0396, Accuracy = 98.44%\n",
            "  Batch  39: Loss = 0.1143, Accuracy = 96.88%\n",
            "  Batch  40: Loss = 0.0723, Accuracy = 96.88%\n",
            "  Batch  41: Loss = 0.1510, Accuracy = 95.31%\n",
            "  Batch  42: Loss = 0.0514, Accuracy = 96.88%\n",
            "  Batch  43: Loss = 0.1224, Accuracy = 95.31%\n",
            "  Batch  44: Loss = 0.1278, Accuracy = 93.75%\n",
            "  Batch  45: Loss = 0.1082, Accuracy = 96.88%\n",
            "  Batch  46: Loss = 0.1796, Accuracy = 93.75%\n",
            "  Batch  47: Loss = 0.0148, Accuracy = 100.00%\n",
            "  Batch  48: Loss = 0.1060, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.0235, Accuracy = 100.00%\n",
            "  Batch  50: Loss = 0.2392, Accuracy = 92.19%\n",
            "  Batch  51: Loss = 0.0559, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.0501, Accuracy = 98.44%\n",
            "  Batch  53: Loss = 0.2195, Accuracy = 93.75%\n",
            "  Batch  54: Loss = 0.0191, Accuracy = 100.00%\n",
            "  Batch  55: Loss = 0.1759, Accuracy = 93.75%\n",
            "  Batch  56: Loss = 0.0857, Accuracy = 96.88%\n",
            "  Batch  57: Loss = 0.0514, Accuracy = 96.88%\n",
            "  Batch  58: Loss = 0.0408, Accuracy = 100.00%\n",
            "  Batch  59: Loss = 0.0151, Accuracy = 100.00%\n",
            "  Batch  60: Loss = 0.1147, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.2537, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.0886, Accuracy = 96.88%\n",
            "  Batch  63: Loss = 0.1396, Accuracy = 95.31%\n",
            "  Batch  64: Loss = 0.0582, Accuracy = 100.00%\n",
            "  Batch  65: Loss = 0.1129, Accuracy = 95.31%\n",
            "  Batch  66: Loss = 0.0800, Accuracy = 98.44%\n",
            "  Batch  67: Loss = 0.1111, Accuracy = 95.31%\n",
            "  Batch  68: Loss = 0.0259, Accuracy = 98.44%\n",
            "  Batch  69: Loss = 0.0504, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.0215, Accuracy = 100.00%\n",
            "  Batch  71: Loss = 0.1974, Accuracy = 93.75%\n",
            "  Batch  72: Loss = 0.1160, Accuracy = 98.44%\n",
            "  Batch  73: Loss = 0.0929, Accuracy = 96.88%\n",
            "  Batch  74: Loss = 0.1077, Accuracy = 96.88%\n",
            "  Batch  75: Loss = 0.1173, Accuracy = 95.31%\n",
            "  Batch  76: Loss = 0.1047, Accuracy = 96.88%\n",
            "  Batch  77: Loss = 0.0558, Accuracy = 96.88%\n",
            "  Batch  78: Loss = 0.0344, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.0163, Accuracy = 100.00%\n",
            "  Batch  80: Loss = 0.0285, Accuracy = 98.44%\n",
            "  Batch  81: Loss = 0.2262, Accuracy = 95.31%\n",
            "  Batch  82: Loss = 0.1368, Accuracy = 95.31%\n",
            "  Batch  83: Loss = 0.0443, Accuracy = 98.44%\n",
            "  Batch  84: Loss = 0.0682, Accuracy = 96.88%\n",
            "  Batch  85: Loss = 0.1181, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.0890, Accuracy = 96.88%\n",
            "  Batch  87: Loss = 0.0387, Accuracy = 96.88%\n",
            "  Batch  88: Loss = 0.0074, Accuracy = 100.00%\n",
            "  Batch  89: Loss = 0.0311, Accuracy = 98.44%\n",
            "  Batch  90: Loss = 0.0502, Accuracy = 96.88%\n",
            "  Batch  91: Loss = 0.0821, Accuracy = 95.31%\n",
            "  Batch  92: Loss = 0.0457, Accuracy = 98.44%\n",
            "  Batch  93: Loss = 0.0254, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.0231, Accuracy = 100.00%\n",
            "  Batch  95: Loss = 0.0998, Accuracy = 95.31%\n",
            "  Batch  96: Loss = 0.0455, Accuracy = 98.44%\n",
            "  Batch  97: Loss = 0.0215, Accuracy = 100.00%\n",
            "  Batch  98: Loss = 0.0429, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.1248, Accuracy = 95.31%\n",
            "  Batch 100: Loss = 0.1577, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.1412, Accuracy = 95.31%\n",
            "  Batch 102: Loss = 0.0412, Accuracy = 98.44%\n",
            "  Batch 103: Loss = 0.0425, Accuracy = 100.00%\n",
            "  Batch 104: Loss = 0.0290, Accuracy = 98.44%\n",
            "  Batch 105: Loss = 0.0629, Accuracy = 98.44%\n",
            "  Batch 106: Loss = 0.0135, Accuracy = 100.00%\n",
            "  Batch 107: Loss = 0.0442, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.0772, Accuracy = 95.31%\n",
            "  Batch 109: Loss = 0.0782, Accuracy = 96.88%\n",
            "  Batch 110: Loss = 0.1706, Accuracy = 93.75%\n",
            "  Batch 111: Loss = 0.0335, Accuracy = 100.00%\n",
            "  Batch 112: Loss = 0.0213, Accuracy = 100.00%\n",
            "  Batch 113: Loss = 0.0658, Accuracy = 96.88%\n",
            "  Batch 114: Loss = 0.0434, Accuracy = 98.44%\n",
            "  Batch 115: Loss = 0.0251, Accuracy = 98.44%\n",
            "  Batch 116: Loss = 0.0424, Accuracy = 98.44%\n",
            "  Batch 117: Loss = 0.0208, Accuracy = 100.00%\n",
            "  Batch 118: Loss = 0.0586, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.0559, Accuracy = 96.88%\n",
            "  Batch 120: Loss = 0.0935, Accuracy = 98.44%\n",
            "  Batch 121: Loss = 0.1048, Accuracy = 98.44%\n",
            "  Batch 122: Loss = 0.0842, Accuracy = 96.88%\n",
            "  Batch 123: Loss = 0.0648, Accuracy = 96.88%\n",
            "  Batch 124: Loss = 0.1352, Accuracy = 96.88%\n",
            "  Batch 125: Loss = 0.0132, Accuracy = 100.00%\n",
            "  Batch 126: Loss = 0.0829, Accuracy = 95.31%\n",
            "  Batch 127: Loss = 0.0506, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0318, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.0649, Accuracy = 95.31%\n",
            "  Batch 130: Loss = 0.0200, Accuracy = 98.44%\n",
            "  Batch 131: Loss = 0.0019, Accuracy = 100.00%\n",
            "==> Epoch 85 Summary: Total Loss = 9.5513, Accuracy = 97.45%\n",
            "\n",
            "Epoch 86\n",
            "  Batch   1: Loss = 0.0486, Accuracy = 98.44%\n",
            "  Batch   2: Loss = 0.1288, Accuracy = 92.19%\n",
            "  Batch   3: Loss = 0.0502, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0745, Accuracy = 98.44%\n",
            "  Batch   5: Loss = 0.0296, Accuracy = 98.44%\n",
            "  Batch   6: Loss = 0.1180, Accuracy = 95.31%\n",
            "  Batch   7: Loss = 0.0606, Accuracy = 95.31%\n",
            "  Batch   8: Loss = 0.0483, Accuracy = 98.44%\n",
            "  Batch   9: Loss = 0.0662, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.0904, Accuracy = 96.88%\n",
            "  Batch  11: Loss = 0.0372, Accuracy = 98.44%\n",
            "  Batch  12: Loss = 0.0247, Accuracy = 100.00%\n",
            "  Batch  13: Loss = 0.0709, Accuracy = 96.88%\n",
            "  Batch  14: Loss = 0.0562, Accuracy = 98.44%\n",
            "  Batch  15: Loss = 0.1663, Accuracy = 93.75%\n",
            "  Batch  16: Loss = 0.0729, Accuracy = 96.88%\n",
            "  Batch  17: Loss = 0.0396, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.1534, Accuracy = 95.31%\n",
            "  Batch  19: Loss = 0.0285, Accuracy = 98.44%\n",
            "  Batch  20: Loss = 0.0593, Accuracy = 96.88%\n",
            "  Batch  21: Loss = 0.0200, Accuracy = 100.00%\n",
            "  Batch  22: Loss = 0.0248, Accuracy = 100.00%\n",
            "  Batch  23: Loss = 0.0705, Accuracy = 98.44%\n",
            "  Batch  24: Loss = 0.0863, Accuracy = 96.88%\n",
            "  Batch  25: Loss = 0.0625, Accuracy = 95.31%\n",
            "  Batch  26: Loss = 0.0291, Accuracy = 100.00%\n",
            "  Batch  27: Loss = 0.0412, Accuracy = 100.00%\n",
            "  Batch  28: Loss = 0.1024, Accuracy = 96.88%\n",
            "  Batch  29: Loss = 0.0320, Accuracy = 98.44%\n",
            "  Batch  30: Loss = 0.1006, Accuracy = 93.75%\n",
            "  Batch  31: Loss = 0.0194, Accuracy = 98.44%\n",
            "  Batch  32: Loss = 0.0817, Accuracy = 96.88%\n",
            "  Batch  33: Loss = 0.0299, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.0302, Accuracy = 100.00%\n",
            "  Batch  35: Loss = 0.2131, Accuracy = 89.06%\n",
            "  Batch  36: Loss = 0.0167, Accuracy = 100.00%\n",
            "  Batch  37: Loss = 0.0359, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.0600, Accuracy = 96.88%\n",
            "  Batch  39: Loss = 0.0351, Accuracy = 100.00%\n",
            "  Batch  40: Loss = 0.1021, Accuracy = 95.31%\n",
            "  Batch  41: Loss = 0.0707, Accuracy = 95.31%\n",
            "  Batch  42: Loss = 0.0649, Accuracy = 96.88%\n",
            "  Batch  43: Loss = 0.0695, Accuracy = 96.88%\n",
            "  Batch  44: Loss = 0.0321, Accuracy = 100.00%\n",
            "  Batch  45: Loss = 0.0957, Accuracy = 95.31%\n",
            "  Batch  46: Loss = 0.0678, Accuracy = 96.88%\n",
            "  Batch  47: Loss = 0.0619, Accuracy = 98.44%\n",
            "  Batch  48: Loss = 0.0890, Accuracy = 98.44%\n",
            "  Batch  49: Loss = 0.0895, Accuracy = 96.88%\n",
            "  Batch  50: Loss = 0.0763, Accuracy = 98.44%\n",
            "  Batch  51: Loss = 0.0821, Accuracy = 96.88%\n",
            "  Batch  52: Loss = 0.1462, Accuracy = 93.75%\n",
            "  Batch  53: Loss = 0.0732, Accuracy = 96.88%\n",
            "  Batch  54: Loss = 0.0422, Accuracy = 98.44%\n",
            "  Batch  55: Loss = 0.0345, Accuracy = 98.44%\n",
            "  Batch  56: Loss = 0.0425, Accuracy = 98.44%\n",
            "  Batch  57: Loss = 0.1109, Accuracy = 95.31%\n",
            "  Batch  58: Loss = 0.0894, Accuracy = 95.31%\n",
            "  Batch  59: Loss = 0.1136, Accuracy = 92.19%\n",
            "  Batch  60: Loss = 0.0776, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.0560, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.1211, Accuracy = 93.75%\n",
            "  Batch  63: Loss = 0.0968, Accuracy = 98.44%\n",
            "  Batch  64: Loss = 0.0127, Accuracy = 100.00%\n",
            "  Batch  65: Loss = 0.1230, Accuracy = 95.31%\n",
            "  Batch  66: Loss = 0.0994, Accuracy = 98.44%\n",
            "  Batch  67: Loss = 0.1523, Accuracy = 95.31%\n",
            "  Batch  68: Loss = 0.0332, Accuracy = 100.00%\n",
            "  Batch  69: Loss = 0.0850, Accuracy = 96.88%\n",
            "  Batch  70: Loss = 0.0766, Accuracy = 98.44%\n",
            "  Batch  71: Loss = 0.0401, Accuracy = 98.44%\n",
            "  Batch  72: Loss = 0.0991, Accuracy = 95.31%\n",
            "  Batch  73: Loss = 0.0567, Accuracy = 98.44%\n",
            "  Batch  74: Loss = 0.0328, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.0100, Accuracy = 100.00%\n",
            "  Batch  76: Loss = 0.0165, Accuracy = 100.00%\n",
            "  Batch  77: Loss = 0.0586, Accuracy = 98.44%\n",
            "  Batch  78: Loss = 0.1667, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.0991, Accuracy = 96.88%\n",
            "  Batch  80: Loss = 0.0143, Accuracy = 100.00%\n",
            "  Batch  81: Loss = 0.0262, Accuracy = 100.00%\n",
            "  Batch  82: Loss = 0.1446, Accuracy = 95.31%\n",
            "  Batch  83: Loss = 0.0055, Accuracy = 100.00%\n",
            "  Batch  84: Loss = 0.0829, Accuracy = 98.44%\n",
            "  Batch  85: Loss = 0.0230, Accuracy = 100.00%\n",
            "  Batch  86: Loss = 0.0380, Accuracy = 98.44%\n",
            "  Batch  87: Loss = 0.0849, Accuracy = 93.75%\n",
            "  Batch  88: Loss = 0.0122, Accuracy = 100.00%\n",
            "  Batch  89: Loss = 0.0779, Accuracy = 95.31%\n",
            "  Batch  90: Loss = 0.0483, Accuracy = 98.44%\n",
            "  Batch  91: Loss = 0.0776, Accuracy = 98.44%\n",
            "  Batch  92: Loss = 0.0281, Accuracy = 100.00%\n",
            "  Batch  93: Loss = 0.0384, Accuracy = 98.44%\n",
            "  Batch  94: Loss = 0.1211, Accuracy = 95.31%\n",
            "  Batch  95: Loss = 0.0456, Accuracy = 96.88%\n",
            "  Batch  96: Loss = 0.0891, Accuracy = 98.44%\n",
            "  Batch  97: Loss = 0.0576, Accuracy = 98.44%\n",
            "  Batch  98: Loss = 0.0565, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.0870, Accuracy = 98.44%\n",
            "  Batch 100: Loss = 0.0234, Accuracy = 100.00%\n",
            "  Batch 101: Loss = 0.1440, Accuracy = 96.88%\n",
            "  Batch 102: Loss = 0.0512, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.0821, Accuracy = 96.88%\n",
            "  Batch 104: Loss = 0.2014, Accuracy = 92.19%\n",
            "  Batch 105: Loss = 0.0687, Accuracy = 98.44%\n",
            "  Batch 106: Loss = 0.0171, Accuracy = 100.00%\n",
            "  Batch 107: Loss = 0.0064, Accuracy = 100.00%\n",
            "  Batch 108: Loss = 0.0318, Accuracy = 100.00%\n",
            "  Batch 109: Loss = 0.0124, Accuracy = 100.00%\n",
            "  Batch 110: Loss = 0.0600, Accuracy = 96.88%\n",
            "  Batch 111: Loss = 0.0666, Accuracy = 98.44%\n",
            "  Batch 112: Loss = 0.0820, Accuracy = 95.31%\n",
            "  Batch 113: Loss = 0.0802, Accuracy = 96.88%\n",
            "  Batch 114: Loss = 0.0506, Accuracy = 96.88%\n",
            "  Batch 115: Loss = 0.0635, Accuracy = 98.44%\n",
            "  Batch 116: Loss = 0.0398, Accuracy = 98.44%\n",
            "  Batch 117: Loss = 0.0387, Accuracy = 98.44%\n",
            "  Batch 118: Loss = 0.1148, Accuracy = 95.31%\n",
            "  Batch 119: Loss = 0.0393, Accuracy = 98.44%\n",
            "  Batch 120: Loss = 0.2432, Accuracy = 92.19%\n",
            "  Batch 121: Loss = 0.1539, Accuracy = 95.31%\n",
            "  Batch 122: Loss = 0.0304, Accuracy = 100.00%\n",
            "  Batch 123: Loss = 0.1242, Accuracy = 95.31%\n",
            "  Batch 124: Loss = 0.1251, Accuracy = 95.31%\n",
            "  Batch 125: Loss = 0.1208, Accuracy = 96.88%\n",
            "  Batch 126: Loss = 0.0966, Accuracy = 95.31%\n",
            "  Batch 127: Loss = 0.0267, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0342, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.1763, Accuracy = 93.75%\n",
            "  Batch 130: Loss = 0.0412, Accuracy = 98.44%\n",
            "  Batch 131: Loss = 0.0677, Accuracy = 100.00%\n",
            "==> Epoch 86 Summary: Total Loss = 9.2564, Accuracy = 97.44%\n",
            "\n",
            "Epoch 87\n",
            "  Batch   1: Loss = 0.0207, Accuracy = 100.00%\n",
            "  Batch   2: Loss = 0.2334, Accuracy = 87.50%\n",
            "  Batch   3: Loss = 0.2079, Accuracy = 92.19%\n",
            "  Batch   4: Loss = 0.0554, Accuracy = 98.44%\n",
            "  Batch   5: Loss = 0.0339, Accuracy = 98.44%\n",
            "  Batch   6: Loss = 0.0604, Accuracy = 96.88%\n",
            "  Batch   7: Loss = 0.0419, Accuracy = 98.44%\n",
            "  Batch   8: Loss = 0.0588, Accuracy = 96.88%\n",
            "  Batch   9: Loss = 0.1917, Accuracy = 93.75%\n",
            "  Batch  10: Loss = 0.0500, Accuracy = 98.44%\n",
            "  Batch  11: Loss = 0.0454, Accuracy = 96.88%\n",
            "  Batch  12: Loss = 0.0622, Accuracy = 96.88%\n",
            "  Batch  13: Loss = 0.0336, Accuracy = 98.44%\n",
            "  Batch  14: Loss = 0.0720, Accuracy = 96.88%\n",
            "  Batch  15: Loss = 0.0504, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.0391, Accuracy = 98.44%\n",
            "  Batch  17: Loss = 0.0884, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.0614, Accuracy = 98.44%\n",
            "  Batch  19: Loss = 0.2042, Accuracy = 93.75%\n",
            "  Batch  20: Loss = 0.0366, Accuracy = 98.44%\n",
            "  Batch  21: Loss = 0.0368, Accuracy = 98.44%\n",
            "  Batch  22: Loss = 0.0990, Accuracy = 96.88%\n",
            "  Batch  23: Loss = 0.0510, Accuracy = 98.44%\n",
            "  Batch  24: Loss = 0.0089, Accuracy = 100.00%\n",
            "  Batch  25: Loss = 0.0255, Accuracy = 100.00%\n",
            "  Batch  26: Loss = 0.1738, Accuracy = 90.62%\n",
            "  Batch  27: Loss = 0.0411, Accuracy = 100.00%\n",
            "  Batch  28: Loss = 0.0500, Accuracy = 96.88%\n",
            "  Batch  29: Loss = 0.0594, Accuracy = 98.44%\n",
            "  Batch  30: Loss = 0.0733, Accuracy = 96.88%\n",
            "  Batch  31: Loss = 0.0583, Accuracy = 96.88%\n",
            "  Batch  32: Loss = 0.0274, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.0336, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.1242, Accuracy = 96.88%\n",
            "  Batch  35: Loss = 0.0554, Accuracy = 96.88%\n",
            "  Batch  36: Loss = 0.0381, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.1251, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.2008, Accuracy = 95.31%\n",
            "  Batch  39: Loss = 0.0584, Accuracy = 96.88%\n",
            "  Batch  40: Loss = 0.1021, Accuracy = 95.31%\n",
            "  Batch  41: Loss = 0.0321, Accuracy = 98.44%\n",
            "  Batch  42: Loss = 0.0471, Accuracy = 100.00%\n",
            "  Batch  43: Loss = 0.1837, Accuracy = 98.44%\n",
            "  Batch  44: Loss = 0.0513, Accuracy = 98.44%\n",
            "  Batch  45: Loss = 0.0681, Accuracy = 98.44%\n",
            "  Batch  46: Loss = 0.0831, Accuracy = 96.88%\n",
            "  Batch  47: Loss = 0.0433, Accuracy = 98.44%\n",
            "  Batch  48: Loss = 0.0180, Accuracy = 100.00%\n",
            "  Batch  49: Loss = 0.0628, Accuracy = 98.44%\n",
            "  Batch  50: Loss = 0.0970, Accuracy = 96.88%\n",
            "  Batch  51: Loss = 0.0771, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.0173, Accuracy = 100.00%\n",
            "  Batch  53: Loss = 0.0901, Accuracy = 96.88%\n",
            "  Batch  54: Loss = 0.1624, Accuracy = 93.75%\n",
            "  Batch  55: Loss = 0.2068, Accuracy = 92.19%\n",
            "  Batch  56: Loss = 0.0353, Accuracy = 96.88%\n",
            "  Batch  57: Loss = 0.0536, Accuracy = 95.31%\n",
            "  Batch  58: Loss = 0.0808, Accuracy = 96.88%\n",
            "  Batch  59: Loss = 0.0965, Accuracy = 95.31%\n",
            "  Batch  60: Loss = 0.0681, Accuracy = 95.31%\n",
            "  Batch  61: Loss = 0.0476, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.0232, Accuracy = 100.00%\n",
            "  Batch  63: Loss = 0.1240, Accuracy = 95.31%\n",
            "  Batch  64: Loss = 0.0195, Accuracy = 100.00%\n",
            "  Batch  65: Loss = 0.0632, Accuracy = 96.88%\n",
            "  Batch  66: Loss = 0.1302, Accuracy = 92.19%\n",
            "  Batch  67: Loss = 0.0443, Accuracy = 100.00%\n",
            "  Batch  68: Loss = 0.0667, Accuracy = 98.44%\n",
            "  Batch  69: Loss = 0.0739, Accuracy = 96.88%\n",
            "  Batch  70: Loss = 0.0805, Accuracy = 95.31%\n",
            "  Batch  71: Loss = 0.0135, Accuracy = 100.00%\n",
            "  Batch  72: Loss = 0.0602, Accuracy = 96.88%\n",
            "  Batch  73: Loss = 0.1415, Accuracy = 95.31%\n",
            "  Batch  74: Loss = 0.0416, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.0925, Accuracy = 95.31%\n",
            "  Batch  76: Loss = 0.0323, Accuracy = 98.44%\n",
            "  Batch  77: Loss = 0.0159, Accuracy = 100.00%\n",
            "  Batch  78: Loss = 0.0300, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.0249, Accuracy = 100.00%\n",
            "  Batch  80: Loss = 0.0702, Accuracy = 95.31%\n",
            "  Batch  81: Loss = 0.0491, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.0448, Accuracy = 96.88%\n",
            "  Batch  83: Loss = 0.0723, Accuracy = 95.31%\n",
            "  Batch  84: Loss = 0.0217, Accuracy = 100.00%\n",
            "  Batch  85: Loss = 0.0315, Accuracy = 98.44%\n",
            "  Batch  86: Loss = 0.1760, Accuracy = 95.31%\n",
            "  Batch  87: Loss = 0.0351, Accuracy = 98.44%\n",
            "  Batch  88: Loss = 0.1053, Accuracy = 95.31%\n",
            "  Batch  89: Loss = 0.0350, Accuracy = 98.44%\n",
            "  Batch  90: Loss = 0.0496, Accuracy = 96.88%\n",
            "  Batch  91: Loss = 0.0926, Accuracy = 95.31%\n",
            "  Batch  92: Loss = 0.0239, Accuracy = 98.44%\n",
            "  Batch  93: Loss = 0.0992, Accuracy = 96.88%\n",
            "  Batch  94: Loss = 0.1026, Accuracy = 98.44%\n",
            "  Batch  95: Loss = 0.0806, Accuracy = 96.88%\n",
            "  Batch  96: Loss = 0.1798, Accuracy = 93.75%\n",
            "  Batch  97: Loss = 0.0268, Accuracy = 98.44%\n",
            "  Batch  98: Loss = 0.1234, Accuracy = 96.88%\n",
            "  Batch  99: Loss = 0.0647, Accuracy = 96.88%\n",
            "  Batch 100: Loss = 0.0834, Accuracy = 95.31%\n",
            "  Batch 101: Loss = 0.1805, Accuracy = 92.19%\n",
            "  Batch 102: Loss = 0.1888, Accuracy = 95.31%\n",
            "  Batch 103: Loss = 0.0543, Accuracy = 96.88%\n",
            "  Batch 104: Loss = 0.0220, Accuracy = 100.00%\n",
            "  Batch 105: Loss = 0.1378, Accuracy = 96.88%\n",
            "  Batch 106: Loss = 0.0508, Accuracy = 98.44%\n",
            "  Batch 107: Loss = 0.1430, Accuracy = 93.75%\n",
            "  Batch 108: Loss = 0.0124, Accuracy = 100.00%\n",
            "  Batch 109: Loss = 0.1320, Accuracy = 95.31%\n",
            "  Batch 110: Loss = 0.2109, Accuracy = 95.31%\n",
            "  Batch 111: Loss = 0.0093, Accuracy = 100.00%\n",
            "  Batch 112: Loss = 0.0677, Accuracy = 98.44%\n",
            "  Batch 113: Loss = 0.1602, Accuracy = 95.31%\n",
            "  Batch 114: Loss = 0.0375, Accuracy = 98.44%\n",
            "  Batch 115: Loss = 0.1234, Accuracy = 93.75%\n",
            "  Batch 116: Loss = 0.0924, Accuracy = 96.88%\n",
            "  Batch 117: Loss = 0.1445, Accuracy = 93.75%\n",
            "  Batch 118: Loss = 0.0743, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.0400, Accuracy = 100.00%\n",
            "  Batch 120: Loss = 0.1247, Accuracy = 96.88%\n",
            "  Batch 121: Loss = 0.0723, Accuracy = 96.88%\n",
            "  Batch 122: Loss = 0.0815, Accuracy = 95.31%\n",
            "  Batch 123: Loss = 0.0615, Accuracy = 98.44%\n",
            "  Batch 124: Loss = 0.0348, Accuracy = 98.44%\n",
            "  Batch 125: Loss = 0.0334, Accuracy = 96.88%\n",
            "  Batch 126: Loss = 0.0168, Accuracy = 100.00%\n",
            "  Batch 127: Loss = 0.0298, Accuracy = 100.00%\n",
            "  Batch 128: Loss = 0.0546, Accuracy = 96.88%\n",
            "  Batch 129: Loss = 0.0619, Accuracy = 96.88%\n",
            "  Batch 130: Loss = 0.0849, Accuracy = 98.44%\n",
            "  Batch 131: Loss = 0.0003, Accuracy = 100.00%\n",
            "==> Epoch 87 Summary: Total Loss = 9.8957, Accuracy = 97.18%\n",
            "\n",
            "Epoch 88\n",
            "  Batch   1: Loss = 0.0326, Accuracy = 98.44%\n",
            "  Batch   2: Loss = 0.0260, Accuracy = 100.00%\n",
            "  Batch   3: Loss = 0.2103, Accuracy = 92.19%\n",
            "  Batch   4: Loss = 0.2286, Accuracy = 95.31%\n",
            "  Batch   5: Loss = 0.0809, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.0896, Accuracy = 96.88%\n",
            "  Batch   7: Loss = 0.0970, Accuracy = 93.75%\n",
            "  Batch   8: Loss = 0.0272, Accuracy = 100.00%\n",
            "  Batch   9: Loss = 0.1402, Accuracy = 95.31%\n",
            "  Batch  10: Loss = 0.0718, Accuracy = 98.44%\n",
            "  Batch  11: Loss = 0.0298, Accuracy = 98.44%\n",
            "  Batch  12: Loss = 0.0581, Accuracy = 96.88%\n",
            "  Batch  13: Loss = 0.0209, Accuracy = 100.00%\n",
            "  Batch  14: Loss = 0.0842, Accuracy = 95.31%\n",
            "  Batch  15: Loss = 0.1685, Accuracy = 96.88%\n",
            "  Batch  16: Loss = 0.1546, Accuracy = 96.88%\n",
            "  Batch  17: Loss = 0.0978, Accuracy = 96.88%\n",
            "  Batch  18: Loss = 0.0375, Accuracy = 98.44%\n",
            "  Batch  19: Loss = 0.0881, Accuracy = 96.88%\n",
            "  Batch  20: Loss = 0.1099, Accuracy = 96.88%\n",
            "  Batch  21: Loss = 0.1488, Accuracy = 95.31%\n",
            "  Batch  22: Loss = 0.0809, Accuracy = 95.31%\n",
            "  Batch  23: Loss = 0.0247, Accuracy = 100.00%\n",
            "  Batch  24: Loss = 0.0439, Accuracy = 96.88%\n",
            "  Batch  25: Loss = 0.0372, Accuracy = 100.00%\n",
            "  Batch  26: Loss = 0.0639, Accuracy = 98.44%\n",
            "  Batch  27: Loss = 0.0924, Accuracy = 93.75%\n",
            "  Batch  28: Loss = 0.0381, Accuracy = 98.44%\n",
            "  Batch  29: Loss = 0.0539, Accuracy = 98.44%\n",
            "  Batch  30: Loss = 0.1223, Accuracy = 92.19%\n",
            "  Batch  31: Loss = 0.0374, Accuracy = 98.44%\n",
            "  Batch  32: Loss = 0.1239, Accuracy = 96.88%\n",
            "  Batch  33: Loss = 0.0232, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.0812, Accuracy = 93.75%\n",
            "  Batch  35: Loss = 0.0151, Accuracy = 100.00%\n",
            "  Batch  36: Loss = 0.0815, Accuracy = 96.88%\n",
            "  Batch  37: Loss = 0.0517, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.0746, Accuracy = 95.31%\n",
            "  Batch  39: Loss = 0.1281, Accuracy = 96.88%\n",
            "  Batch  40: Loss = 0.0285, Accuracy = 100.00%\n",
            "  Batch  41: Loss = 0.0485, Accuracy = 96.88%\n",
            "  Batch  42: Loss = 0.1029, Accuracy = 96.88%\n",
            "  Batch  43: Loss = 0.0672, Accuracy = 96.88%\n",
            "  Batch  44: Loss = 0.1258, Accuracy = 93.75%\n",
            "  Batch  45: Loss = 0.0781, Accuracy = 96.88%\n",
            "  Batch  46: Loss = 0.0466, Accuracy = 96.88%\n",
            "  Batch  47: Loss = 0.0360, Accuracy = 98.44%\n",
            "  Batch  48: Loss = 0.0511, Accuracy = 100.00%\n",
            "  Batch  49: Loss = 0.0415, Accuracy = 98.44%\n",
            "  Batch  50: Loss = 0.0485, Accuracy = 96.88%\n",
            "  Batch  51: Loss = 0.0584, Accuracy = 96.88%\n",
            "  Batch  52: Loss = 0.0517, Accuracy = 98.44%\n",
            "  Batch  53: Loss = 0.0176, Accuracy = 100.00%\n",
            "  Batch  54: Loss = 0.0662, Accuracy = 98.44%\n",
            "  Batch  55: Loss = 0.0374, Accuracy = 98.44%\n",
            "  Batch  56: Loss = 0.0701, Accuracy = 98.44%\n",
            "  Batch  57: Loss = 0.0730, Accuracy = 95.31%\n",
            "  Batch  58: Loss = 0.0268, Accuracy = 100.00%\n",
            "  Batch  59: Loss = 0.0288, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.1374, Accuracy = 95.31%\n",
            "  Batch  61: Loss = 0.0542, Accuracy = 98.44%\n",
            "  Batch  62: Loss = 0.1588, Accuracy = 93.75%\n",
            "  Batch  63: Loss = 0.0842, Accuracy = 98.44%\n",
            "  Batch  64: Loss = 0.1137, Accuracy = 93.75%\n",
            "  Batch  65: Loss = 0.0793, Accuracy = 98.44%\n",
            "  Batch  66: Loss = 0.0616, Accuracy = 98.44%\n",
            "  Batch  67: Loss = 0.0253, Accuracy = 98.44%\n",
            "  Batch  68: Loss = 0.0155, Accuracy = 100.00%\n",
            "  Batch  69: Loss = 0.0197, Accuracy = 100.00%\n",
            "  Batch  70: Loss = 0.1028, Accuracy = 96.88%\n",
            "  Batch  71: Loss = 0.0755, Accuracy = 95.31%\n",
            "  Batch  72: Loss = 0.0727, Accuracy = 98.44%\n",
            "  Batch  73: Loss = 0.0213, Accuracy = 100.00%\n",
            "  Batch  74: Loss = 0.0649, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.0964, Accuracy = 96.88%\n",
            "  Batch  76: Loss = 0.0203, Accuracy = 100.00%\n",
            "  Batch  77: Loss = 0.0862, Accuracy = 96.88%\n",
            "  Batch  78: Loss = 0.0616, Accuracy = 96.88%\n",
            "  Batch  79: Loss = 0.0439, Accuracy = 96.88%\n",
            "  Batch  80: Loss = 0.0288, Accuracy = 100.00%\n",
            "  Batch  81: Loss = 0.1155, Accuracy = 95.31%\n",
            "  Batch  82: Loss = 0.0485, Accuracy = 98.44%\n",
            "  Batch  83: Loss = 0.0912, Accuracy = 95.31%\n",
            "  Batch  84: Loss = 0.0905, Accuracy = 98.44%\n",
            "  Batch  85: Loss = 0.0077, Accuracy = 100.00%\n",
            "  Batch  86: Loss = 0.0511, Accuracy = 98.44%\n",
            "  Batch  87: Loss = 0.0234, Accuracy = 100.00%\n",
            "  Batch  88: Loss = 0.0265, Accuracy = 100.00%\n",
            "  Batch  89: Loss = 0.1521, Accuracy = 98.44%\n",
            "  Batch  90: Loss = 0.1529, Accuracy = 95.31%\n",
            "  Batch  91: Loss = 0.0274, Accuracy = 100.00%\n",
            "  Batch  92: Loss = 0.0927, Accuracy = 96.88%\n",
            "  Batch  93: Loss = 0.0665, Accuracy = 98.44%\n",
            "  Batch  94: Loss = 0.1226, Accuracy = 95.31%\n",
            "  Batch  95: Loss = 0.0221, Accuracy = 100.00%\n",
            "  Batch  96: Loss = 0.0796, Accuracy = 95.31%\n",
            "  Batch  97: Loss = 0.0695, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.0449, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.0578, Accuracy = 98.44%\n",
            "  Batch 100: Loss = 0.0558, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.0356, Accuracy = 98.44%\n",
            "  Batch 102: Loss = 0.0577, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.0954, Accuracy = 96.88%\n",
            "  Batch 104: Loss = 0.1019, Accuracy = 96.88%\n",
            "  Batch 105: Loss = 0.0414, Accuracy = 96.88%\n",
            "  Batch 106: Loss = 0.0185, Accuracy = 98.44%\n",
            "  Batch 107: Loss = 0.0493, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.0480, Accuracy = 96.88%\n",
            "  Batch 109: Loss = 0.1291, Accuracy = 93.75%\n",
            "  Batch 110: Loss = 0.1563, Accuracy = 95.31%\n",
            "  Batch 111: Loss = 0.1041, Accuracy = 98.44%\n",
            "  Batch 112: Loss = 0.0360, Accuracy = 98.44%\n",
            "  Batch 113: Loss = 0.1326, Accuracy = 95.31%\n",
            "  Batch 114: Loss = 0.0506, Accuracy = 96.88%\n",
            "  Batch 115: Loss = 0.0766, Accuracy = 95.31%\n",
            "  Batch 116: Loss = 0.0062, Accuracy = 100.00%\n",
            "  Batch 117: Loss = 0.0221, Accuracy = 100.00%\n",
            "  Batch 118: Loss = 0.0959, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.0714, Accuracy = 98.44%\n",
            "  Batch 120: Loss = 0.0358, Accuracy = 98.44%\n",
            "  Batch 121: Loss = 0.0150, Accuracy = 100.00%\n",
            "  Batch 122: Loss = 0.0401, Accuracy = 98.44%\n",
            "  Batch 123: Loss = 0.0577, Accuracy = 96.88%\n",
            "  Batch 124: Loss = 0.0546, Accuracy = 96.88%\n",
            "  Batch 125: Loss = 0.0145, Accuracy = 100.00%\n",
            "  Batch 126: Loss = 0.0126, Accuracy = 100.00%\n",
            "  Batch 127: Loss = 0.1047, Accuracy = 96.88%\n",
            "  Batch 128: Loss = 0.0302, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.0420, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.1033, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0006, Accuracy = 100.00%\n",
            "==> Epoch 88 Summary: Total Loss = 8.9435, Accuracy = 97.53%\n",
            "\n",
            "Epoch 89\n",
            "  Batch   1: Loss = 0.1137, Accuracy = 93.75%\n",
            "  Batch   2: Loss = 0.1117, Accuracy = 95.31%\n",
            "  Batch   3: Loss = 0.0380, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0674, Accuracy = 98.44%\n",
            "  Batch   5: Loss = 0.0439, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.0289, Accuracy = 100.00%\n",
            "  Batch   7: Loss = 0.0239, Accuracy = 100.00%\n",
            "  Batch   8: Loss = 0.0383, Accuracy = 98.44%\n",
            "  Batch   9: Loss = 0.0998, Accuracy = 96.88%\n",
            "  Batch  10: Loss = 0.1294, Accuracy = 95.31%\n",
            "  Batch  11: Loss = 0.0986, Accuracy = 98.44%\n",
            "  Batch  12: Loss = 0.0800, Accuracy = 98.44%\n",
            "  Batch  13: Loss = 0.0800, Accuracy = 98.44%\n",
            "  Batch  14: Loss = 0.0192, Accuracy = 100.00%\n",
            "  Batch  15: Loss = 0.0609, Accuracy = 96.88%\n",
            "  Batch  16: Loss = 0.1219, Accuracy = 95.31%\n",
            "  Batch  17: Loss = 0.1592, Accuracy = 96.88%\n",
            "  Batch  18: Loss = 0.0154, Accuracy = 100.00%\n",
            "  Batch  19: Loss = 0.2599, Accuracy = 89.06%\n",
            "  Batch  20: Loss = 0.0891, Accuracy = 96.88%\n",
            "  Batch  21: Loss = 0.1150, Accuracy = 96.88%\n",
            "  Batch  22: Loss = 0.1718, Accuracy = 95.31%\n",
            "  Batch  23: Loss = 0.0554, Accuracy = 96.88%\n",
            "  Batch  24: Loss = 0.0624, Accuracy = 96.88%\n",
            "  Batch  25: Loss = 0.1276, Accuracy = 95.31%\n",
            "  Batch  26: Loss = 0.0843, Accuracy = 95.31%\n",
            "  Batch  27: Loss = 0.1657, Accuracy = 95.31%\n",
            "  Batch  28: Loss = 0.0894, Accuracy = 95.31%\n",
            "  Batch  29: Loss = 0.0144, Accuracy = 100.00%\n",
            "  Batch  30: Loss = 0.0868, Accuracy = 95.31%\n",
            "  Batch  31: Loss = 0.0224, Accuracy = 100.00%\n",
            "  Batch  32: Loss = 0.0667, Accuracy = 96.88%\n",
            "  Batch  33: Loss = 0.0704, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.0517, Accuracy = 96.88%\n",
            "  Batch  35: Loss = 0.0902, Accuracy = 96.88%\n",
            "  Batch  36: Loss = 0.1618, Accuracy = 95.31%\n",
            "  Batch  37: Loss = 0.0961, Accuracy = 96.88%\n",
            "  Batch  38: Loss = 0.0136, Accuracy = 100.00%\n",
            "  Batch  39: Loss = 0.0382, Accuracy = 100.00%\n",
            "  Batch  40: Loss = 0.0708, Accuracy = 98.44%\n",
            "  Batch  41: Loss = 0.0108, Accuracy = 100.00%\n",
            "  Batch  42: Loss = 0.0609, Accuracy = 98.44%\n",
            "  Batch  43: Loss = 0.0287, Accuracy = 100.00%\n",
            "  Batch  44: Loss = 0.0382, Accuracy = 100.00%\n",
            "  Batch  45: Loss = 0.2612, Accuracy = 92.19%\n",
            "  Batch  46: Loss = 0.0618, Accuracy = 98.44%\n",
            "  Batch  47: Loss = 0.0959, Accuracy = 93.75%\n",
            "  Batch  48: Loss = 0.0730, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.0907, Accuracy = 95.31%\n",
            "  Batch  50: Loss = 0.0321, Accuracy = 100.00%\n",
            "  Batch  51: Loss = 0.0109, Accuracy = 100.00%\n",
            "  Batch  52: Loss = 0.0239, Accuracy = 98.44%\n",
            "  Batch  53: Loss = 0.0643, Accuracy = 98.44%\n",
            "  Batch  54: Loss = 0.1492, Accuracy = 93.75%\n",
            "  Batch  55: Loss = 0.1749, Accuracy = 96.88%\n",
            "  Batch  56: Loss = 0.1162, Accuracy = 95.31%\n",
            "  Batch  57: Loss = 0.0444, Accuracy = 100.00%\n",
            "  Batch  58: Loss = 0.0257, Accuracy = 98.44%\n",
            "  Batch  59: Loss = 0.0995, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.0331, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.1150, Accuracy = 93.75%\n",
            "  Batch  62: Loss = 0.0159, Accuracy = 100.00%\n",
            "  Batch  63: Loss = 0.0987, Accuracy = 96.88%\n",
            "  Batch  64: Loss = 0.1191, Accuracy = 93.75%\n",
            "  Batch  65: Loss = 0.0553, Accuracy = 98.44%\n",
            "  Batch  66: Loss = 0.0653, Accuracy = 95.31%\n",
            "  Batch  67: Loss = 0.0544, Accuracy = 95.31%\n",
            "  Batch  68: Loss = 0.0308, Accuracy = 98.44%\n",
            "  Batch  69: Loss = 0.0746, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.2019, Accuracy = 95.31%\n",
            "  Batch  71: Loss = 0.0697, Accuracy = 98.44%\n",
            "  Batch  72: Loss = 0.0980, Accuracy = 95.31%\n",
            "  Batch  73: Loss = 0.0834, Accuracy = 95.31%\n",
            "  Batch  74: Loss = 0.0495, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.1388, Accuracy = 95.31%\n",
            "  Batch  76: Loss = 0.0201, Accuracy = 100.00%\n",
            "  Batch  77: Loss = 0.0343, Accuracy = 98.44%\n",
            "  Batch  78: Loss = 0.0310, Accuracy = 100.00%\n",
            "  Batch  79: Loss = 0.0648, Accuracy = 98.44%\n",
            "  Batch  80: Loss = 0.0576, Accuracy = 98.44%\n",
            "  Batch  81: Loss = 0.0884, Accuracy = 93.75%\n",
            "  Batch  82: Loss = 0.2218, Accuracy = 92.19%\n",
            "  Batch  83: Loss = 0.0149, Accuracy = 100.00%\n",
            "  Batch  84: Loss = 0.0379, Accuracy = 98.44%\n",
            "  Batch  85: Loss = 0.0580, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.1268, Accuracy = 95.31%\n",
            "  Batch  87: Loss = 0.0259, Accuracy = 100.00%\n",
            "  Batch  88: Loss = 0.0459, Accuracy = 96.88%\n",
            "  Batch  89: Loss = 0.0359, Accuracy = 98.44%\n",
            "  Batch  90: Loss = 0.0916, Accuracy = 98.44%\n",
            "  Batch  91: Loss = 0.0490, Accuracy = 96.88%\n",
            "  Batch  92: Loss = 0.0544, Accuracy = 100.00%\n",
            "  Batch  93: Loss = 0.0330, Accuracy = 98.44%\n",
            "  Batch  94: Loss = 0.0279, Accuracy = 98.44%\n",
            "  Batch  95: Loss = 0.0164, Accuracy = 100.00%\n",
            "  Batch  96: Loss = 0.0333, Accuracy = 98.44%\n",
            "  Batch  97: Loss = 0.0827, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.0458, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.0554, Accuracy = 98.44%\n",
            "  Batch 100: Loss = 0.0474, Accuracy = 98.44%\n",
            "  Batch 101: Loss = 0.0131, Accuracy = 100.00%\n",
            "  Batch 102: Loss = 0.0488, Accuracy = 98.44%\n",
            "  Batch 103: Loss = 0.1066, Accuracy = 95.31%\n",
            "  Batch 104: Loss = 0.0940, Accuracy = 95.31%\n",
            "  Batch 105: Loss = 0.0982, Accuracy = 96.88%\n",
            "  Batch 106: Loss = 0.0821, Accuracy = 95.31%\n",
            "  Batch 107: Loss = 0.0167, Accuracy = 100.00%\n",
            "  Batch 108: Loss = 0.0294, Accuracy = 98.44%\n",
            "  Batch 109: Loss = 0.2152, Accuracy = 92.19%\n",
            "  Batch 110: Loss = 0.0978, Accuracy = 93.75%\n",
            "  Batch 111: Loss = 0.0216, Accuracy = 100.00%\n",
            "  Batch 112: Loss = 0.0885, Accuracy = 96.88%\n",
            "  Batch 113: Loss = 0.0518, Accuracy = 98.44%\n",
            "  Batch 114: Loss = 0.0248, Accuracy = 100.00%\n",
            "  Batch 115: Loss = 0.1224, Accuracy = 96.88%\n",
            "  Batch 116: Loss = 0.0859, Accuracy = 96.88%\n",
            "  Batch 117: Loss = 0.0423, Accuracy = 98.44%\n",
            "  Batch 118: Loss = 0.0389, Accuracy = 98.44%\n",
            "  Batch 119: Loss = 0.1171, Accuracy = 93.75%\n",
            "  Batch 120: Loss = 0.0467, Accuracy = 96.88%\n",
            "  Batch 121: Loss = 0.0339, Accuracy = 100.00%\n",
            "  Batch 122: Loss = 0.0867, Accuracy = 98.44%\n",
            "  Batch 123: Loss = 0.0248, Accuracy = 100.00%\n",
            "  Batch 124: Loss = 0.0350, Accuracy = 96.88%\n",
            "  Batch 125: Loss = 0.1328, Accuracy = 92.19%\n",
            "  Batch 126: Loss = 0.0884, Accuracy = 98.44%\n",
            "  Batch 127: Loss = 0.0258, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.1541, Accuracy = 95.31%\n",
            "  Batch 129: Loss = 0.0603, Accuracy = 96.88%\n",
            "  Batch 130: Loss = 0.0886, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0257, Accuracy = 100.00%\n",
            "==> Epoch 89 Summary: Total Loss = 9.6980, Accuracy = 97.32%\n",
            "\n",
            "Epoch 90\n",
            "  Batch   1: Loss = 0.1614, Accuracy = 93.75%\n",
            "  Batch   2: Loss = 0.0610, Accuracy = 98.44%\n",
            "  Batch   3: Loss = 0.0193, Accuracy = 100.00%\n",
            "  Batch   4: Loss = 0.0896, Accuracy = 98.44%\n",
            "  Batch   5: Loss = 0.1768, Accuracy = 95.31%\n",
            "  Batch   6: Loss = 0.0722, Accuracy = 96.88%\n",
            "  Batch   7: Loss = 0.1405, Accuracy = 96.88%\n",
            "  Batch   8: Loss = 0.0578, Accuracy = 96.88%\n",
            "  Batch   9: Loss = 0.0495, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.0354, Accuracy = 98.44%\n",
            "  Batch  11: Loss = 0.0638, Accuracy = 98.44%\n",
            "  Batch  12: Loss = 0.1523, Accuracy = 95.31%\n",
            "  Batch  13: Loss = 0.0552, Accuracy = 96.88%\n",
            "  Batch  14: Loss = 0.1592, Accuracy = 95.31%\n",
            "  Batch  15: Loss = 0.1166, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.1274, Accuracy = 96.88%\n",
            "  Batch  17: Loss = 0.0989, Accuracy = 96.88%\n",
            "  Batch  18: Loss = 0.0327, Accuracy = 98.44%\n",
            "  Batch  19: Loss = 0.1435, Accuracy = 96.88%\n",
            "  Batch  20: Loss = 0.0333, Accuracy = 98.44%\n",
            "  Batch  21: Loss = 0.0177, Accuracy = 100.00%\n",
            "  Batch  22: Loss = 0.0318, Accuracy = 100.00%\n",
            "  Batch  23: Loss = 0.1677, Accuracy = 95.31%\n",
            "  Batch  24: Loss = 0.0750, Accuracy = 95.31%\n",
            "  Batch  25: Loss = 0.0123, Accuracy = 100.00%\n",
            "  Batch  26: Loss = 0.1664, Accuracy = 95.31%\n",
            "  Batch  27: Loss = 0.0182, Accuracy = 100.00%\n",
            "  Batch  28: Loss = 0.1062, Accuracy = 95.31%\n",
            "  Batch  29: Loss = 0.0717, Accuracy = 95.31%\n",
            "  Batch  30: Loss = 0.0777, Accuracy = 96.88%\n",
            "  Batch  31: Loss = 0.1670, Accuracy = 95.31%\n",
            "  Batch  32: Loss = 0.0370, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.0907, Accuracy = 95.31%\n",
            "  Batch  34: Loss = 0.0930, Accuracy = 95.31%\n",
            "  Batch  35: Loss = 0.0379, Accuracy = 98.44%\n",
            "  Batch  36: Loss = 0.0421, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.0728, Accuracy = 95.31%\n",
            "  Batch  38: Loss = 0.0521, Accuracy = 98.44%\n",
            "  Batch  39: Loss = 0.1944, Accuracy = 92.19%\n",
            "  Batch  40: Loss = 0.0432, Accuracy = 98.44%\n",
            "  Batch  41: Loss = 0.1129, Accuracy = 95.31%\n",
            "  Batch  42: Loss = 0.0743, Accuracy = 98.44%\n",
            "  Batch  43: Loss = 0.1865, Accuracy = 92.19%\n",
            "  Batch  44: Loss = 0.0517, Accuracy = 98.44%\n",
            "  Batch  45: Loss = 0.0352, Accuracy = 98.44%\n",
            "  Batch  46: Loss = 0.0355, Accuracy = 96.88%\n",
            "  Batch  47: Loss = 0.0246, Accuracy = 100.00%\n",
            "  Batch  48: Loss = 0.0222, Accuracy = 100.00%\n",
            "  Batch  49: Loss = 0.0511, Accuracy = 98.44%\n",
            "  Batch  50: Loss = 0.0588, Accuracy = 98.44%\n",
            "  Batch  51: Loss = 0.0099, Accuracy = 100.00%\n",
            "  Batch  52: Loss = 0.1589, Accuracy = 95.31%\n",
            "  Batch  53: Loss = 0.1348, Accuracy = 98.44%\n",
            "  Batch  54: Loss = 0.0841, Accuracy = 96.88%\n",
            "  Batch  55: Loss = 0.0225, Accuracy = 100.00%\n",
            "  Batch  56: Loss = 0.0312, Accuracy = 100.00%\n",
            "  Batch  57: Loss = 0.1210, Accuracy = 95.31%\n",
            "  Batch  58: Loss = 0.1976, Accuracy = 92.19%\n",
            "  Batch  59: Loss = 0.0977, Accuracy = 96.88%\n",
            "  Batch  60: Loss = 0.0154, Accuracy = 100.00%\n",
            "  Batch  61: Loss = 0.0681, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.1742, Accuracy = 93.75%\n",
            "  Batch  63: Loss = 0.0787, Accuracy = 98.44%\n",
            "  Batch  64: Loss = 0.0914, Accuracy = 96.88%\n",
            "  Batch  65: Loss = 0.0483, Accuracy = 96.88%\n",
            "  Batch  66: Loss = 0.0097, Accuracy = 100.00%\n",
            "  Batch  67: Loss = 0.1545, Accuracy = 93.75%\n",
            "  Batch  68: Loss = 0.0438, Accuracy = 98.44%\n",
            "  Batch  69: Loss = 0.0392, Accuracy = 100.00%\n",
            "  Batch  70: Loss = 0.1345, Accuracy = 95.31%\n",
            "  Batch  71: Loss = 0.0501, Accuracy = 100.00%\n",
            "  Batch  72: Loss = 0.0953, Accuracy = 98.44%\n",
            "  Batch  73: Loss = 0.0481, Accuracy = 98.44%\n",
            "  Batch  74: Loss = 0.2466, Accuracy = 92.19%\n",
            "  Batch  75: Loss = 0.3532, Accuracy = 87.50%\n",
            "  Batch  76: Loss = 0.0751, Accuracy = 98.44%\n",
            "  Batch  77: Loss = 0.1283, Accuracy = 96.88%\n",
            "  Batch  78: Loss = 0.0700, Accuracy = 96.88%\n",
            "  Batch  79: Loss = 0.0330, Accuracy = 100.00%\n",
            "  Batch  80: Loss = 0.1567, Accuracy = 96.88%\n",
            "  Batch  81: Loss = 0.1711, Accuracy = 95.31%\n",
            "  Batch  82: Loss = 0.2477, Accuracy = 93.75%\n",
            "  Batch  83: Loss = 0.1508, Accuracy = 95.31%\n",
            "  Batch  84: Loss = 0.2218, Accuracy = 95.31%\n",
            "  Batch  85: Loss = 0.0568, Accuracy = 98.44%\n",
            "  Batch  86: Loss = 0.0712, Accuracy = 98.44%\n",
            "  Batch  87: Loss = 0.0970, Accuracy = 95.31%\n",
            "  Batch  88: Loss = 0.1132, Accuracy = 93.75%\n",
            "  Batch  89: Loss = 0.0386, Accuracy = 98.44%\n",
            "  Batch  90: Loss = 0.0556, Accuracy = 98.44%\n",
            "  Batch  91: Loss = 0.1274, Accuracy = 95.31%\n",
            "  Batch  92: Loss = 0.1002, Accuracy = 96.88%\n",
            "  Batch  93: Loss = 0.1185, Accuracy = 96.88%\n",
            "  Batch  94: Loss = 0.0808, Accuracy = 95.31%\n",
            "  Batch  95: Loss = 0.0700, Accuracy = 95.31%\n",
            "  Batch  96: Loss = 0.2569, Accuracy = 95.31%\n",
            "  Batch  97: Loss = 0.0772, Accuracy = 98.44%\n",
            "  Batch  98: Loss = 0.0856, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.0131, Accuracy = 100.00%\n",
            "  Batch 100: Loss = 0.0799, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.0392, Accuracy = 98.44%\n",
            "  Batch 102: Loss = 0.1652, Accuracy = 90.62%\n",
            "  Batch 103: Loss = 0.0638, Accuracy = 95.31%\n",
            "  Batch 104: Loss = 0.2401, Accuracy = 89.06%\n",
            "  Batch 105: Loss = 0.0532, Accuracy = 98.44%\n",
            "  Batch 106: Loss = 0.0585, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.1083, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.0379, Accuracy = 100.00%\n",
            "  Batch 109: Loss = 0.0983, Accuracy = 96.88%\n",
            "  Batch 110: Loss = 0.0229, Accuracy = 100.00%\n",
            "  Batch 111: Loss = 0.1133, Accuracy = 95.31%\n",
            "  Batch 112: Loss = 0.1112, Accuracy = 95.31%\n",
            "  Batch 113: Loss = 0.0640, Accuracy = 95.31%\n",
            "  Batch 114: Loss = 0.1575, Accuracy = 96.88%\n",
            "  Batch 115: Loss = 0.0212, Accuracy = 100.00%\n",
            "  Batch 116: Loss = 0.0416, Accuracy = 98.44%\n",
            "  Batch 117: Loss = 0.1773, Accuracy = 95.31%\n",
            "  Batch 118: Loss = 0.0825, Accuracy = 98.44%\n",
            "  Batch 119: Loss = 0.0138, Accuracy = 100.00%\n",
            "  Batch 120: Loss = 0.0537, Accuracy = 96.88%\n",
            "  Batch 121: Loss = 0.0656, Accuracy = 95.31%\n",
            "  Batch 122: Loss = 0.0683, Accuracy = 96.88%\n",
            "  Batch 123: Loss = 0.0510, Accuracy = 98.44%\n",
            "  Batch 124: Loss = 0.1398, Accuracy = 95.31%\n",
            "  Batch 125: Loss = 0.0602, Accuracy = 98.44%\n",
            "  Batch 126: Loss = 0.0840, Accuracy = 98.44%\n",
            "  Batch 127: Loss = 0.0336, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0144, Accuracy = 100.00%\n",
            "  Batch 129: Loss = 0.0495, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.1200, Accuracy = 95.31%\n",
            "  Batch 131: Loss = 0.0368, Accuracy = 100.00%\n",
            "==> Epoch 90 Summary: Total Loss = 11.7252, Accuracy = 97.03%\n",
            "\n",
            "Epoch 91\n",
            "  Batch   1: Loss = 0.0641, Accuracy = 96.88%\n",
            "  Batch   2: Loss = 0.0511, Accuracy = 96.88%\n",
            "  Batch   3: Loss = 0.0269, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0472, Accuracy = 96.88%\n",
            "  Batch   5: Loss = 0.0504, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.1107, Accuracy = 93.75%\n",
            "  Batch   7: Loss = 0.2073, Accuracy = 95.31%\n",
            "  Batch   8: Loss = 0.1226, Accuracy = 93.75%\n",
            "  Batch   9: Loss = 0.0445, Accuracy = 96.88%\n",
            "  Batch  10: Loss = 0.0505, Accuracy = 98.44%\n",
            "  Batch  11: Loss = 0.0221, Accuracy = 100.00%\n",
            "  Batch  12: Loss = 0.0555, Accuracy = 96.88%\n",
            "  Batch  13: Loss = 0.1698, Accuracy = 93.75%\n",
            "  Batch  14: Loss = 0.0178, Accuracy = 100.00%\n",
            "  Batch  15: Loss = 0.0677, Accuracy = 96.88%\n",
            "  Batch  16: Loss = 0.0770, Accuracy = 96.88%\n",
            "  Batch  17: Loss = 0.1079, Accuracy = 95.31%\n",
            "  Batch  18: Loss = 0.0549, Accuracy = 96.88%\n",
            "  Batch  19: Loss = 0.0218, Accuracy = 100.00%\n",
            "  Batch  20: Loss = 0.1545, Accuracy = 93.75%\n",
            "  Batch  21: Loss = 0.0416, Accuracy = 98.44%\n",
            "  Batch  22: Loss = 0.2388, Accuracy = 93.75%\n",
            "  Batch  23: Loss = 0.0888, Accuracy = 95.31%\n",
            "  Batch  24: Loss = 0.0692, Accuracy = 98.44%\n",
            "  Batch  25: Loss = 0.0519, Accuracy = 98.44%\n",
            "  Batch  26: Loss = 0.0678, Accuracy = 95.31%\n",
            "  Batch  27: Loss = 0.1055, Accuracy = 95.31%\n",
            "  Batch  28: Loss = 0.1920, Accuracy = 95.31%\n",
            "  Batch  29: Loss = 0.1876, Accuracy = 93.75%\n",
            "  Batch  30: Loss = 0.0128, Accuracy = 100.00%\n",
            "  Batch  31: Loss = 0.0864, Accuracy = 95.31%\n",
            "  Batch  32: Loss = 0.0067, Accuracy = 100.00%\n",
            "  Batch  33: Loss = 0.0661, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.0468, Accuracy = 98.44%\n",
            "  Batch  35: Loss = 0.0644, Accuracy = 96.88%\n",
            "  Batch  36: Loss = 0.0597, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.0872, Accuracy = 95.31%\n",
            "  Batch  38: Loss = 0.0741, Accuracy = 96.88%\n",
            "  Batch  39: Loss = 0.0639, Accuracy = 96.88%\n",
            "  Batch  40: Loss = 0.0591, Accuracy = 98.44%\n",
            "  Batch  41: Loss = 0.0343, Accuracy = 98.44%\n",
            "  Batch  42: Loss = 0.0484, Accuracy = 98.44%\n",
            "  Batch  43: Loss = 0.0903, Accuracy = 95.31%\n",
            "  Batch  44: Loss = 0.0190, Accuracy = 100.00%\n",
            "  Batch  45: Loss = 0.0211, Accuracy = 100.00%\n",
            "  Batch  46: Loss = 0.0782, Accuracy = 95.31%\n",
            "  Batch  47: Loss = 0.0207, Accuracy = 98.44%\n",
            "  Batch  48: Loss = 0.1492, Accuracy = 93.75%\n",
            "  Batch  49: Loss = 0.0649, Accuracy = 98.44%\n",
            "  Batch  50: Loss = 0.0527, Accuracy = 98.44%\n",
            "  Batch  51: Loss = 0.0463, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.1354, Accuracy = 95.31%\n",
            "  Batch  53: Loss = 0.0726, Accuracy = 96.88%\n",
            "  Batch  54: Loss = 0.0406, Accuracy = 98.44%\n",
            "  Batch  55: Loss = 0.0544, Accuracy = 96.88%\n",
            "  Batch  56: Loss = 0.0152, Accuracy = 100.00%\n",
            "  Batch  57: Loss = 0.0866, Accuracy = 96.88%\n",
            "  Batch  58: Loss = 0.1818, Accuracy = 93.75%\n",
            "  Batch  59: Loss = 0.0472, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.0421, Accuracy = 98.44%\n",
            "  Batch  61: Loss = 0.0856, Accuracy = 98.44%\n",
            "  Batch  62: Loss = 0.0742, Accuracy = 96.88%\n",
            "  Batch  63: Loss = 0.0305, Accuracy = 100.00%\n",
            "  Batch  64: Loss = 0.0573, Accuracy = 100.00%\n",
            "  Batch  65: Loss = 0.1361, Accuracy = 95.31%\n",
            "  Batch  66: Loss = 0.0659, Accuracy = 96.88%\n",
            "  Batch  67: Loss = 0.0734, Accuracy = 95.31%\n",
            "  Batch  68: Loss = 0.0889, Accuracy = 96.88%\n",
            "  Batch  69: Loss = 0.1131, Accuracy = 93.75%\n",
            "  Batch  70: Loss = 0.0219, Accuracy = 100.00%\n",
            "  Batch  71: Loss = 0.0519, Accuracy = 98.44%\n",
            "  Batch  72: Loss = 0.1020, Accuracy = 96.88%\n",
            "  Batch  73: Loss = 0.0370, Accuracy = 98.44%\n",
            "  Batch  74: Loss = 0.1512, Accuracy = 92.19%\n",
            "  Batch  75: Loss = 0.0797, Accuracy = 98.44%\n",
            "  Batch  76: Loss = 0.1676, Accuracy = 95.31%\n",
            "  Batch  77: Loss = 0.1506, Accuracy = 93.75%\n",
            "  Batch  78: Loss = 0.0664, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.0691, Accuracy = 96.88%\n",
            "  Batch  80: Loss = 0.0802, Accuracy = 98.44%\n",
            "  Batch  81: Loss = 0.0414, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.0758, Accuracy = 95.31%\n",
            "  Batch  83: Loss = 0.1049, Accuracy = 96.88%\n",
            "  Batch  84: Loss = 0.1191, Accuracy = 96.88%\n",
            "  Batch  85: Loss = 0.0384, Accuracy = 98.44%\n",
            "  Batch  86: Loss = 0.0461, Accuracy = 98.44%\n",
            "  Batch  87: Loss = 0.0499, Accuracy = 98.44%\n",
            "  Batch  88: Loss = 0.0236, Accuracy = 100.00%\n",
            "  Batch  89: Loss = 0.0170, Accuracy = 100.00%\n",
            "  Batch  90: Loss = 0.1388, Accuracy = 95.31%\n",
            "  Batch  91: Loss = 0.2081, Accuracy = 93.75%\n",
            "  Batch  92: Loss = 0.0700, Accuracy = 98.44%\n",
            "  Batch  93: Loss = 0.0232, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.0678, Accuracy = 98.44%\n",
            "  Batch  95: Loss = 0.1563, Accuracy = 96.88%\n",
            "  Batch  96: Loss = 0.0908, Accuracy = 96.88%\n",
            "  Batch  97: Loss = 0.1246, Accuracy = 95.31%\n",
            "  Batch  98: Loss = 0.0679, Accuracy = 96.88%\n",
            "  Batch  99: Loss = 0.1159, Accuracy = 95.31%\n",
            "  Batch 100: Loss = 0.0327, Accuracy = 98.44%\n",
            "  Batch 101: Loss = 0.1549, Accuracy = 93.75%\n",
            "  Batch 102: Loss = 0.1392, Accuracy = 95.31%\n",
            "  Batch 103: Loss = 0.0534, Accuracy = 96.88%\n",
            "  Batch 104: Loss = 0.0347, Accuracy = 98.44%\n",
            "  Batch 105: Loss = 0.1066, Accuracy = 95.31%\n",
            "  Batch 106: Loss = 0.0513, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.0115, Accuracy = 100.00%\n",
            "  Batch 108: Loss = 0.0285, Accuracy = 98.44%\n",
            "  Batch 109: Loss = 0.0994, Accuracy = 95.31%\n",
            "  Batch 110: Loss = 0.0481, Accuracy = 96.88%\n",
            "  Batch 111: Loss = 0.0058, Accuracy = 100.00%\n",
            "  Batch 112: Loss = 0.1074, Accuracy = 95.31%\n",
            "  Batch 113: Loss = 0.0586, Accuracy = 96.88%\n",
            "  Batch 114: Loss = 0.1351, Accuracy = 96.88%\n",
            "  Batch 115: Loss = 0.1020, Accuracy = 95.31%\n",
            "  Batch 116: Loss = 0.2597, Accuracy = 89.06%\n",
            "  Batch 117: Loss = 0.0479, Accuracy = 98.44%\n",
            "  Batch 118: Loss = 0.0385, Accuracy = 98.44%\n",
            "  Batch 119: Loss = 0.0059, Accuracy = 100.00%\n",
            "  Batch 120: Loss = 0.0293, Accuracy = 98.44%\n",
            "  Batch 121: Loss = 0.0761, Accuracy = 95.31%\n",
            "  Batch 122: Loss = 0.1107, Accuracy = 96.88%\n",
            "  Batch 123: Loss = 0.1667, Accuracy = 96.88%\n",
            "  Batch 124: Loss = 0.2073, Accuracy = 92.19%\n",
            "  Batch 125: Loss = 0.1178, Accuracy = 96.88%\n",
            "  Batch 126: Loss = 0.1849, Accuracy = 93.75%\n",
            "  Batch 127: Loss = 0.0435, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.1197, Accuracy = 96.88%\n",
            "  Batch 129: Loss = 0.1920, Accuracy = 93.75%\n",
            "  Batch 130: Loss = 0.0532, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0160, Accuracy = 100.00%\n",
            "==> Epoch 91 Summary: Total Loss = 10.6165, Accuracy = 96.97%\n",
            "\n",
            "Epoch 92\n",
            "  Batch   1: Loss = 0.0514, Accuracy = 98.44%\n",
            "  Batch   2: Loss = 0.1756, Accuracy = 95.31%\n",
            "  Batch   3: Loss = 0.0991, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0776, Accuracy = 98.44%\n",
            "  Batch   5: Loss = 0.0706, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.0335, Accuracy = 98.44%\n",
            "  Batch   7: Loss = 0.0238, Accuracy = 100.00%\n",
            "  Batch   8: Loss = 0.1930, Accuracy = 92.19%\n",
            "  Batch   9: Loss = 0.4003, Accuracy = 92.19%\n",
            "  Batch  10: Loss = 0.0916, Accuracy = 98.44%\n",
            "  Batch  11: Loss = 0.0146, Accuracy = 100.00%\n",
            "  Batch  12: Loss = 0.1515, Accuracy = 92.19%\n",
            "  Batch  13: Loss = 0.0489, Accuracy = 98.44%\n",
            "  Batch  14: Loss = 0.1868, Accuracy = 93.75%\n",
            "  Batch  15: Loss = 0.1152, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.0542, Accuracy = 96.88%\n",
            "  Batch  17: Loss = 0.1097, Accuracy = 95.31%\n",
            "  Batch  18: Loss = 0.0969, Accuracy = 93.75%\n",
            "  Batch  19: Loss = 0.0919, Accuracy = 95.31%\n",
            "  Batch  20: Loss = 0.0596, Accuracy = 98.44%\n",
            "  Batch  21: Loss = 0.1434, Accuracy = 93.75%\n",
            "  Batch  22: Loss = 0.1184, Accuracy = 95.31%\n",
            "  Batch  23: Loss = 0.1872, Accuracy = 93.75%\n",
            "  Batch  24: Loss = 0.0240, Accuracy = 100.00%\n",
            "  Batch  25: Loss = 0.0953, Accuracy = 96.88%\n",
            "  Batch  26: Loss = 0.0442, Accuracy = 98.44%\n",
            "  Batch  27: Loss = 0.0686, Accuracy = 95.31%\n",
            "  Batch  28: Loss = 0.1481, Accuracy = 96.88%\n",
            "  Batch  29: Loss = 0.1978, Accuracy = 93.75%\n",
            "  Batch  30: Loss = 0.1704, Accuracy = 93.75%\n",
            "  Batch  31: Loss = 0.0954, Accuracy = 93.75%\n",
            "  Batch  32: Loss = 0.0770, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.1183, Accuracy = 93.75%\n",
            "  Batch  34: Loss = 0.0468, Accuracy = 100.00%\n",
            "  Batch  35: Loss = 0.0402, Accuracy = 100.00%\n",
            "  Batch  36: Loss = 0.0521, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.0215, Accuracy = 100.00%\n",
            "  Batch  38: Loss = 0.1145, Accuracy = 95.31%\n",
            "  Batch  39: Loss = 0.0459, Accuracy = 98.44%\n",
            "  Batch  40: Loss = 0.0177, Accuracy = 100.00%\n",
            "  Batch  41: Loss = 0.0263, Accuracy = 100.00%\n",
            "  Batch  42: Loss = 0.1211, Accuracy = 95.31%\n",
            "  Batch  43: Loss = 0.1070, Accuracy = 95.31%\n",
            "  Batch  44: Loss = 0.0145, Accuracy = 100.00%\n",
            "  Batch  45: Loss = 0.0328, Accuracy = 98.44%\n",
            "  Batch  46: Loss = 0.0549, Accuracy = 96.88%\n",
            "  Batch  47: Loss = 0.1205, Accuracy = 93.75%\n",
            "  Batch  48: Loss = 0.0366, Accuracy = 98.44%\n",
            "  Batch  49: Loss = 0.1094, Accuracy = 96.88%\n",
            "  Batch  50: Loss = 0.1116, Accuracy = 96.88%\n",
            "  Batch  51: Loss = 0.2845, Accuracy = 90.62%\n",
            "  Batch  52: Loss = 0.0753, Accuracy = 96.88%\n",
            "  Batch  53: Loss = 0.0745, Accuracy = 96.88%\n",
            "  Batch  54: Loss = 0.1368, Accuracy = 95.31%\n",
            "  Batch  55: Loss = 0.0332, Accuracy = 100.00%\n",
            "  Batch  56: Loss = 0.1016, Accuracy = 96.88%\n",
            "  Batch  57: Loss = 0.2090, Accuracy = 90.62%\n",
            "  Batch  58: Loss = 0.0653, Accuracy = 98.44%\n",
            "  Batch  59: Loss = 0.0616, Accuracy = 96.88%\n",
            "  Batch  60: Loss = 0.1156, Accuracy = 98.44%\n",
            "  Batch  61: Loss = 0.0887, Accuracy = 98.44%\n",
            "  Batch  62: Loss = 0.0648, Accuracy = 96.88%\n",
            "  Batch  63: Loss = 0.1366, Accuracy = 95.31%\n",
            "  Batch  64: Loss = 0.1491, Accuracy = 93.75%\n",
            "  Batch  65: Loss = 0.0601, Accuracy = 100.00%\n",
            "  Batch  66: Loss = 0.1836, Accuracy = 93.75%\n",
            "  Batch  67: Loss = 0.2320, Accuracy = 96.88%\n",
            "  Batch  68: Loss = 0.0472, Accuracy = 98.44%\n",
            "  Batch  69: Loss = 0.0292, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.0304, Accuracy = 100.00%\n",
            "  Batch  71: Loss = 0.0121, Accuracy = 100.00%\n",
            "  Batch  72: Loss = 0.0661, Accuracy = 96.88%\n",
            "  Batch  73: Loss = 0.0586, Accuracy = 98.44%\n",
            "  Batch  74: Loss = 0.0575, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.1256, Accuracy = 95.31%\n",
            "  Batch  76: Loss = 0.0527, Accuracy = 98.44%\n",
            "  Batch  77: Loss = 0.1122, Accuracy = 93.75%\n",
            "  Batch  78: Loss = 0.0226, Accuracy = 100.00%\n",
            "  Batch  79: Loss = 0.0783, Accuracy = 96.88%\n",
            "  Batch  80: Loss = 0.0516, Accuracy = 98.44%\n",
            "  Batch  81: Loss = 0.0691, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.0207, Accuracy = 100.00%\n",
            "  Batch  83: Loss = 0.0183, Accuracy = 100.00%\n",
            "  Batch  84: Loss = 0.0392, Accuracy = 100.00%\n",
            "  Batch  85: Loss = 0.0225, Accuracy = 100.00%\n",
            "  Batch  86: Loss = 0.0470, Accuracy = 98.44%\n",
            "  Batch  87: Loss = 0.0746, Accuracy = 96.88%\n",
            "  Batch  88: Loss = 0.0301, Accuracy = 98.44%\n",
            "  Batch  89: Loss = 0.0962, Accuracy = 96.88%\n",
            "  Batch  90: Loss = 0.0187, Accuracy = 100.00%\n",
            "  Batch  91: Loss = 0.0489, Accuracy = 98.44%\n",
            "  Batch  92: Loss = 0.0397, Accuracy = 96.88%\n",
            "  Batch  93: Loss = 0.0560, Accuracy = 96.88%\n",
            "  Batch  94: Loss = 0.0576, Accuracy = 98.44%\n",
            "  Batch  95: Loss = 0.0358, Accuracy = 98.44%\n",
            "  Batch  96: Loss = 0.1408, Accuracy = 95.31%\n",
            "  Batch  97: Loss = 0.0666, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.0241, Accuracy = 100.00%\n",
            "  Batch  99: Loss = 0.1902, Accuracy = 95.31%\n",
            "  Batch 100: Loss = 0.0310, Accuracy = 98.44%\n",
            "  Batch 101: Loss = 0.1438, Accuracy = 95.31%\n",
            "  Batch 102: Loss = 0.0605, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.0605, Accuracy = 96.88%\n",
            "  Batch 104: Loss = 0.1201, Accuracy = 93.75%\n",
            "  Batch 105: Loss = 0.0950, Accuracy = 96.88%\n",
            "  Batch 106: Loss = 0.0669, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.0162, Accuracy = 100.00%\n",
            "  Batch 108: Loss = 0.1155, Accuracy = 93.75%\n",
            "  Batch 109: Loss = 0.0225, Accuracy = 100.00%\n",
            "  Batch 110: Loss = 0.0291, Accuracy = 98.44%\n",
            "  Batch 111: Loss = 0.0204, Accuracy = 100.00%\n",
            "  Batch 112: Loss = 0.0253, Accuracy = 100.00%\n",
            "  Batch 113: Loss = 0.0728, Accuracy = 96.88%\n",
            "  Batch 114: Loss = 0.0399, Accuracy = 96.88%\n",
            "  Batch 115: Loss = 0.0091, Accuracy = 100.00%\n",
            "  Batch 116: Loss = 0.0717, Accuracy = 96.88%\n",
            "  Batch 117: Loss = 0.0623, Accuracy = 98.44%\n",
            "  Batch 118: Loss = 0.1630, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.0690, Accuracy = 98.44%\n",
            "  Batch 120: Loss = 0.0604, Accuracy = 98.44%\n",
            "  Batch 121: Loss = 0.1094, Accuracy = 95.31%\n",
            "  Batch 122: Loss = 0.0679, Accuracy = 95.31%\n",
            "  Batch 123: Loss = 0.0049, Accuracy = 100.00%\n",
            "  Batch 124: Loss = 0.0883, Accuracy = 96.88%\n",
            "  Batch 125: Loss = 0.0880, Accuracy = 96.88%\n",
            "  Batch 126: Loss = 0.1206, Accuracy = 96.88%\n",
            "  Batch 127: Loss = 0.0303, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0269, Accuracy = 100.00%\n",
            "  Batch 129: Loss = 0.0337, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.0435, Accuracy = 98.44%\n",
            "  Batch 131: Loss = 0.0024, Accuracy = 100.00%\n",
            "==> Epoch 92 Summary: Total Loss = 10.5901, Accuracy = 97.24%\n",
            "\n",
            "Epoch 93\n",
            "  Batch   1: Loss = 0.0545, Accuracy = 98.44%\n",
            "  Batch   2: Loss = 0.0237, Accuracy = 100.00%\n",
            "  Batch   3: Loss = 0.0195, Accuracy = 100.00%\n",
            "  Batch   4: Loss = 0.0420, Accuracy = 98.44%\n",
            "  Batch   5: Loss = 0.0407, Accuracy = 98.44%\n",
            "  Batch   6: Loss = 0.0344, Accuracy = 98.44%\n",
            "  Batch   7: Loss = 0.0400, Accuracy = 98.44%\n",
            "  Batch   8: Loss = 0.0920, Accuracy = 96.88%\n",
            "  Batch   9: Loss = 0.0876, Accuracy = 96.88%\n",
            "  Batch  10: Loss = 0.0230, Accuracy = 100.00%\n",
            "  Batch  11: Loss = 0.0657, Accuracy = 96.88%\n",
            "  Batch  12: Loss = 0.0133, Accuracy = 100.00%\n",
            "  Batch  13: Loss = 0.0146, Accuracy = 100.00%\n",
            "  Batch  14: Loss = 0.0116, Accuracy = 100.00%\n",
            "  Batch  15: Loss = 0.0149, Accuracy = 100.00%\n",
            "  Batch  16: Loss = 0.0424, Accuracy = 98.44%\n",
            "  Batch  17: Loss = 0.0434, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.2227, Accuracy = 96.88%\n",
            "  Batch  19: Loss = 0.0372, Accuracy = 98.44%\n",
            "  Batch  20: Loss = 0.0328, Accuracy = 98.44%\n",
            "  Batch  21: Loss = 0.0750, Accuracy = 95.31%\n",
            "  Batch  22: Loss = 0.0396, Accuracy = 98.44%\n",
            "  Batch  23: Loss = 0.0051, Accuracy = 100.00%\n",
            "  Batch  24: Loss = 0.0460, Accuracy = 98.44%\n",
            "  Batch  25: Loss = 0.0368, Accuracy = 100.00%\n",
            "  Batch  26: Loss = 0.0923, Accuracy = 98.44%\n",
            "  Batch  27: Loss = 0.0946, Accuracy = 96.88%\n",
            "  Batch  28: Loss = 0.0825, Accuracy = 96.88%\n",
            "  Batch  29: Loss = 0.0574, Accuracy = 98.44%\n",
            "  Batch  30: Loss = 0.0967, Accuracy = 95.31%\n",
            "  Batch  31: Loss = 0.0263, Accuracy = 100.00%\n",
            "  Batch  32: Loss = 0.0609, Accuracy = 96.88%\n",
            "  Batch  33: Loss = 0.0424, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.0653, Accuracy = 96.88%\n",
            "  Batch  35: Loss = 0.0106, Accuracy = 100.00%\n",
            "  Batch  36: Loss = 0.1089, Accuracy = 95.31%\n",
            "  Batch  37: Loss = 0.0803, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.1627, Accuracy = 95.31%\n",
            "  Batch  39: Loss = 0.2018, Accuracy = 92.19%\n",
            "  Batch  40: Loss = 0.0464, Accuracy = 98.44%\n",
            "  Batch  41: Loss = 0.0503, Accuracy = 98.44%\n",
            "  Batch  42: Loss = 0.1991, Accuracy = 93.75%\n",
            "  Batch  43: Loss = 0.1555, Accuracy = 95.31%\n",
            "  Batch  44: Loss = 0.1137, Accuracy = 95.31%\n",
            "  Batch  45: Loss = 0.1059, Accuracy = 96.88%\n",
            "  Batch  46: Loss = 0.0794, Accuracy = 98.44%\n",
            "  Batch  47: Loss = 0.0862, Accuracy = 96.88%\n",
            "  Batch  48: Loss = 0.1186, Accuracy = 98.44%\n",
            "  Batch  49: Loss = 0.0927, Accuracy = 95.31%\n",
            "  Batch  50: Loss = 0.0835, Accuracy = 96.88%\n",
            "  Batch  51: Loss = 0.0770, Accuracy = 96.88%\n",
            "  Batch  52: Loss = 0.0690, Accuracy = 98.44%\n",
            "  Batch  53: Loss = 0.0206, Accuracy = 100.00%\n",
            "  Batch  54: Loss = 0.0920, Accuracy = 95.31%\n",
            "  Batch  55: Loss = 0.1153, Accuracy = 96.88%\n",
            "  Batch  56: Loss = 0.0676, Accuracy = 96.88%\n",
            "  Batch  57: Loss = 0.0851, Accuracy = 98.44%\n",
            "  Batch  58: Loss = 0.0455, Accuracy = 98.44%\n",
            "  Batch  59: Loss = 0.0183, Accuracy = 100.00%\n",
            "  Batch  60: Loss = 0.0700, Accuracy = 95.31%\n",
            "  Batch  61: Loss = 0.0540, Accuracy = 98.44%\n",
            "  Batch  62: Loss = 0.0959, Accuracy = 98.44%\n",
            "  Batch  63: Loss = 0.0338, Accuracy = 98.44%\n",
            "  Batch  64: Loss = 0.0814, Accuracy = 98.44%\n",
            "  Batch  65: Loss = 0.0124, Accuracy = 100.00%\n",
            "  Batch  66: Loss = 0.0682, Accuracy = 98.44%\n",
            "  Batch  67: Loss = 0.0704, Accuracy = 96.88%\n",
            "  Batch  68: Loss = 0.0943, Accuracy = 95.31%\n",
            "  Batch  69: Loss = 0.0503, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.0988, Accuracy = 95.31%\n",
            "  Batch  71: Loss = 0.0713, Accuracy = 98.44%\n",
            "  Batch  72: Loss = 0.1136, Accuracy = 96.88%\n",
            "  Batch  73: Loss = 0.0661, Accuracy = 96.88%\n",
            "  Batch  74: Loss = 0.0641, Accuracy = 96.88%\n",
            "  Batch  75: Loss = 0.0985, Accuracy = 96.88%\n",
            "  Batch  76: Loss = 0.0812, Accuracy = 96.88%\n",
            "  Batch  77: Loss = 0.0248, Accuracy = 100.00%\n",
            "  Batch  78: Loss = 0.0547, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.1096, Accuracy = 95.31%\n",
            "  Batch  80: Loss = 0.0781, Accuracy = 96.88%\n",
            "  Batch  81: Loss = 0.1058, Accuracy = 96.88%\n",
            "  Batch  82: Loss = 0.0338, Accuracy = 98.44%\n",
            "  Batch  83: Loss = 0.0439, Accuracy = 96.88%\n",
            "  Batch  84: Loss = 0.0960, Accuracy = 95.31%\n",
            "  Batch  85: Loss = 0.0302, Accuracy = 100.00%\n",
            "  Batch  86: Loss = 0.0937, Accuracy = 93.75%\n",
            "  Batch  87: Loss = 0.0127, Accuracy = 100.00%\n",
            "  Batch  88: Loss = 0.0506, Accuracy = 98.44%\n",
            "  Batch  89: Loss = 0.1603, Accuracy = 95.31%\n",
            "  Batch  90: Loss = 0.0929, Accuracy = 98.44%\n",
            "  Batch  91: Loss = 0.0380, Accuracy = 96.88%\n",
            "  Batch  92: Loss = 0.1237, Accuracy = 95.31%\n",
            "  Batch  93: Loss = 0.0307, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.0139, Accuracy = 100.00%\n",
            "  Batch  95: Loss = 0.2686, Accuracy = 90.62%\n",
            "  Batch  96: Loss = 0.0166, Accuracy = 100.00%\n",
            "  Batch  97: Loss = 0.1218, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.0349, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.0528, Accuracy = 96.88%\n",
            "  Batch 100: Loss = 0.0425, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.0915, Accuracy = 95.31%\n",
            "  Batch 102: Loss = 0.0244, Accuracy = 100.00%\n",
            "  Batch 103: Loss = 0.0860, Accuracy = 98.44%\n",
            "  Batch 104: Loss = 0.1057, Accuracy = 96.88%\n",
            "  Batch 105: Loss = 0.0627, Accuracy = 98.44%\n",
            "  Batch 106: Loss = 0.0599, Accuracy = 98.44%\n",
            "  Batch 107: Loss = 0.1004, Accuracy = 95.31%\n",
            "  Batch 108: Loss = 0.0251, Accuracy = 100.00%\n",
            "  Batch 109: Loss = 0.1796, Accuracy = 95.31%\n",
            "  Batch 110: Loss = 0.1959, Accuracy = 95.31%\n",
            "  Batch 111: Loss = 0.1475, Accuracy = 96.88%\n",
            "  Batch 112: Loss = 0.0114, Accuracy = 100.00%\n",
            "  Batch 113: Loss = 0.0109, Accuracy = 100.00%\n",
            "  Batch 114: Loss = 0.0273, Accuracy = 100.00%\n",
            "  Batch 115: Loss = 0.1434, Accuracy = 93.75%\n",
            "  Batch 116: Loss = 0.0219, Accuracy = 100.00%\n",
            "  Batch 117: Loss = 0.0305, Accuracy = 98.44%\n",
            "  Batch 118: Loss = 0.0748, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.0797, Accuracy = 96.88%\n",
            "  Batch 120: Loss = 0.0177, Accuracy = 100.00%\n",
            "  Batch 121: Loss = 0.0494, Accuracy = 98.44%\n",
            "  Batch 122: Loss = 0.0663, Accuracy = 96.88%\n",
            "  Batch 123: Loss = 0.0412, Accuracy = 98.44%\n",
            "  Batch 124: Loss = 0.1271, Accuracy = 96.88%\n",
            "  Batch 125: Loss = 0.0177, Accuracy = 100.00%\n",
            "  Batch 126: Loss = 0.1277, Accuracy = 96.88%\n",
            "  Batch 127: Loss = 0.0532, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0530, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.0234, Accuracy = 100.00%\n",
            "  Batch 130: Loss = 0.0119, Accuracy = 100.00%\n",
            "  Batch 131: Loss = 0.0003, Accuracy = 100.00%\n",
            "==> Epoch 93 Summary: Total Loss = 9.0851, Accuracy = 97.72%\n",
            "\n",
            "Epoch 94\n",
            "  Batch   1: Loss = 0.1774, Accuracy = 96.88%\n",
            "  Batch   2: Loss = 0.0705, Accuracy = 96.88%\n",
            "  Batch   3: Loss = 0.0272, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0176, Accuracy = 100.00%\n",
            "  Batch   5: Loss = 0.1254, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.0772, Accuracy = 96.88%\n",
            "  Batch   7: Loss = 0.0427, Accuracy = 98.44%\n",
            "  Batch   8: Loss = 0.0183, Accuracy = 98.44%\n",
            "  Batch   9: Loss = 0.0283, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.0676, Accuracy = 96.88%\n",
            "  Batch  11: Loss = 0.0718, Accuracy = 96.88%\n",
            "  Batch  12: Loss = 0.0492, Accuracy = 96.88%\n",
            "  Batch  13: Loss = 0.0581, Accuracy = 96.88%\n",
            "  Batch  14: Loss = 0.0345, Accuracy = 100.00%\n",
            "  Batch  15: Loss = 0.0415, Accuracy = 100.00%\n",
            "  Batch  16: Loss = 0.1429, Accuracy = 93.75%\n",
            "  Batch  17: Loss = 0.0539, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.1134, Accuracy = 96.88%\n",
            "  Batch  19: Loss = 0.0397, Accuracy = 98.44%\n",
            "  Batch  20: Loss = 0.0443, Accuracy = 98.44%\n",
            "  Batch  21: Loss = 0.1711, Accuracy = 92.19%\n",
            "  Batch  22: Loss = 0.1208, Accuracy = 95.31%\n",
            "  Batch  23: Loss = 0.0478, Accuracy = 98.44%\n",
            "  Batch  24: Loss = 0.0433, Accuracy = 98.44%\n",
            "  Batch  25: Loss = 0.1117, Accuracy = 98.44%\n",
            "  Batch  26: Loss = 0.1040, Accuracy = 98.44%\n",
            "  Batch  27: Loss = 0.0184, Accuracy = 100.00%\n",
            "  Batch  28: Loss = 0.0426, Accuracy = 98.44%\n",
            "  Batch  29: Loss = 0.1113, Accuracy = 96.88%\n",
            "  Batch  30: Loss = 0.0798, Accuracy = 96.88%\n",
            "  Batch  31: Loss = 0.0535, Accuracy = 98.44%\n",
            "  Batch  32: Loss = 0.1257, Accuracy = 95.31%\n",
            "  Batch  33: Loss = 0.0326, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.2421, Accuracy = 92.19%\n",
            "  Batch  35: Loss = 0.0204, Accuracy = 100.00%\n",
            "  Batch  36: Loss = 0.0249, Accuracy = 100.00%\n",
            "  Batch  37: Loss = 0.0378, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.1109, Accuracy = 96.88%\n",
            "  Batch  39: Loss = 0.0429, Accuracy = 98.44%\n",
            "  Batch  40: Loss = 0.0555, Accuracy = 98.44%\n",
            "  Batch  41: Loss = 0.0571, Accuracy = 98.44%\n",
            "  Batch  42: Loss = 0.0310, Accuracy = 98.44%\n",
            "  Batch  43: Loss = 0.0571, Accuracy = 98.44%\n",
            "  Batch  44: Loss = 0.0189, Accuracy = 100.00%\n",
            "  Batch  45: Loss = 0.0235, Accuracy = 100.00%\n",
            "  Batch  46: Loss = 0.0249, Accuracy = 98.44%\n",
            "  Batch  47: Loss = 0.1367, Accuracy = 98.44%\n",
            "  Batch  48: Loss = 0.0850, Accuracy = 98.44%\n",
            "  Batch  49: Loss = 0.0142, Accuracy = 100.00%\n",
            "  Batch  50: Loss = 0.0121, Accuracy = 100.00%\n",
            "  Batch  51: Loss = 0.0433, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.0080, Accuracy = 100.00%\n",
            "  Batch  53: Loss = 0.0665, Accuracy = 96.88%\n",
            "  Batch  54: Loss = 0.0683, Accuracy = 96.88%\n",
            "  Batch  55: Loss = 0.1574, Accuracy = 96.88%\n",
            "  Batch  56: Loss = 0.0642, Accuracy = 98.44%\n",
            "  Batch  57: Loss = 0.0555, Accuracy = 96.88%\n",
            "  Batch  58: Loss = 0.0143, Accuracy = 100.00%\n",
            "  Batch  59: Loss = 0.0572, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.1072, Accuracy = 96.88%\n",
            "  Batch  61: Loss = 0.0810, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.0930, Accuracy = 96.88%\n",
            "  Batch  63: Loss = 0.1194, Accuracy = 95.31%\n",
            "  Batch  64: Loss = 0.1035, Accuracy = 95.31%\n",
            "  Batch  65: Loss = 0.0143, Accuracy = 100.00%\n",
            "  Batch  66: Loss = 0.1260, Accuracy = 95.31%\n",
            "  Batch  67: Loss = 0.0186, Accuracy = 100.00%\n",
            "  Batch  68: Loss = 0.0161, Accuracy = 100.00%\n",
            "  Batch  69: Loss = 0.0505, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.1316, Accuracy = 95.31%\n",
            "  Batch  71: Loss = 0.1157, Accuracy = 96.88%\n",
            "  Batch  72: Loss = 0.1053, Accuracy = 96.88%\n",
            "  Batch  73: Loss = 0.0261, Accuracy = 98.44%\n",
            "  Batch  74: Loss = 0.1767, Accuracy = 95.31%\n",
            "  Batch  75: Loss = 0.0842, Accuracy = 98.44%\n",
            "  Batch  76: Loss = 0.0679, Accuracy = 96.88%\n",
            "  Batch  77: Loss = 0.0551, Accuracy = 98.44%\n",
            "  Batch  78: Loss = 0.0347, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.0767, Accuracy = 98.44%\n",
            "  Batch  80: Loss = 0.1001, Accuracy = 93.75%\n",
            "  Batch  81: Loss = 0.2126, Accuracy = 90.62%\n",
            "  Batch  82: Loss = 0.0504, Accuracy = 98.44%\n",
            "  Batch  83: Loss = 0.0227, Accuracy = 100.00%\n",
            "  Batch  84: Loss = 0.1680, Accuracy = 95.31%\n",
            "  Batch  85: Loss = 0.0729, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.1100, Accuracy = 95.31%\n",
            "  Batch  87: Loss = 0.0319, Accuracy = 100.00%\n",
            "  Batch  88: Loss = 0.0174, Accuracy = 100.00%\n",
            "  Batch  89: Loss = 0.1273, Accuracy = 95.31%\n",
            "  Batch  90: Loss = 0.1142, Accuracy = 96.88%\n",
            "  Batch  91: Loss = 0.3085, Accuracy = 92.19%\n",
            "  Batch  92: Loss = 0.1950, Accuracy = 92.19%\n",
            "  Batch  93: Loss = 0.0195, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.0309, Accuracy = 98.44%\n",
            "  Batch  95: Loss = 0.1754, Accuracy = 95.31%\n",
            "  Batch  96: Loss = 0.0344, Accuracy = 98.44%\n",
            "  Batch  97: Loss = 0.0160, Accuracy = 100.00%\n",
            "  Batch  98: Loss = 0.0273, Accuracy = 100.00%\n",
            "  Batch  99: Loss = 0.0381, Accuracy = 100.00%\n",
            "  Batch 100: Loss = 0.0675, Accuracy = 96.88%\n",
            "  Batch 101: Loss = 0.1238, Accuracy = 95.31%\n",
            "  Batch 102: Loss = 0.1113, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.1894, Accuracy = 93.75%\n",
            "  Batch 104: Loss = 0.1335, Accuracy = 93.75%\n",
            "  Batch 105: Loss = 0.0954, Accuracy = 98.44%\n",
            "  Batch 106: Loss = 0.0188, Accuracy = 100.00%\n",
            "  Batch 107: Loss = 0.0989, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.0368, Accuracy = 98.44%\n",
            "  Batch 109: Loss = 0.0173, Accuracy = 100.00%\n",
            "  Batch 110: Loss = 0.0614, Accuracy = 96.88%\n",
            "  Batch 111: Loss = 0.0498, Accuracy = 98.44%\n",
            "  Batch 112: Loss = 0.0154, Accuracy = 100.00%\n",
            "  Batch 113: Loss = 0.0476, Accuracy = 96.88%\n",
            "  Batch 114: Loss = 0.0653, Accuracy = 95.31%\n",
            "  Batch 115: Loss = 0.0191, Accuracy = 100.00%\n",
            "  Batch 116: Loss = 0.0930, Accuracy = 98.44%\n",
            "  Batch 117: Loss = 0.0892, Accuracy = 96.88%\n",
            "  Batch 118: Loss = 0.0503, Accuracy = 98.44%\n",
            "  Batch 119: Loss = 0.1451, Accuracy = 95.31%\n",
            "  Batch 120: Loss = 0.1182, Accuracy = 95.31%\n",
            "  Batch 121: Loss = 0.0620, Accuracy = 98.44%\n",
            "  Batch 122: Loss = 0.1719, Accuracy = 96.88%\n",
            "  Batch 123: Loss = 0.0036, Accuracy = 100.00%\n",
            "  Batch 124: Loss = 0.1251, Accuracy = 93.75%\n",
            "  Batch 125: Loss = 0.0758, Accuracy = 96.88%\n",
            "  Batch 126: Loss = 0.0278, Accuracy = 98.44%\n",
            "  Batch 127: Loss = 0.0473, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.1444, Accuracy = 95.31%\n",
            "  Batch 129: Loss = 0.1759, Accuracy = 93.75%\n",
            "  Batch 130: Loss = 0.0579, Accuracy = 98.44%\n",
            "  Batch 131: Loss = 0.0979, Accuracy = 100.00%\n",
            "==> Epoch 94 Summary: Total Loss = 10.0068, Accuracy = 97.54%\n",
            "\n",
            "Epoch 95\n",
            "  Batch   1: Loss = 0.1864, Accuracy = 93.75%\n",
            "  Batch   2: Loss = 0.0575, Accuracy = 100.00%\n",
            "  Batch   3: Loss = 0.1387, Accuracy = 95.31%\n",
            "  Batch   4: Loss = 0.0144, Accuracy = 100.00%\n",
            "  Batch   5: Loss = 0.0310, Accuracy = 98.44%\n",
            "  Batch   6: Loss = 0.0707, Accuracy = 96.88%\n",
            "  Batch   7: Loss = 0.0485, Accuracy = 100.00%\n",
            "  Batch   8: Loss = 0.0338, Accuracy = 98.44%\n",
            "  Batch   9: Loss = 0.1090, Accuracy = 95.31%\n",
            "  Batch  10: Loss = 0.0914, Accuracy = 96.88%\n",
            "  Batch  11: Loss = 0.2218, Accuracy = 92.19%\n",
            "  Batch  12: Loss = 0.0831, Accuracy = 96.88%\n",
            "  Batch  13: Loss = 0.1575, Accuracy = 95.31%\n",
            "  Batch  14: Loss = 0.0247, Accuracy = 100.00%\n",
            "  Batch  15: Loss = 0.1280, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.0144, Accuracy = 100.00%\n",
            "  Batch  17: Loss = 0.0872, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.1313, Accuracy = 95.31%\n",
            "  Batch  19: Loss = 0.0679, Accuracy = 96.88%\n",
            "  Batch  20: Loss = 0.3472, Accuracy = 90.62%\n",
            "  Batch  21: Loss = 0.0906, Accuracy = 95.31%\n",
            "  Batch  22: Loss = 0.1597, Accuracy = 92.19%\n",
            "  Batch  23: Loss = 0.2579, Accuracy = 93.75%\n",
            "  Batch  24: Loss = 0.1253, Accuracy = 95.31%\n",
            "  Batch  25: Loss = 0.1284, Accuracy = 95.31%\n",
            "  Batch  26: Loss = 0.1761, Accuracy = 96.88%\n",
            "  Batch  27: Loss = 0.1743, Accuracy = 92.19%\n",
            "  Batch  28: Loss = 0.1233, Accuracy = 95.31%\n",
            "  Batch  29: Loss = 0.1590, Accuracy = 93.75%\n",
            "  Batch  30: Loss = 0.0224, Accuracy = 100.00%\n",
            "  Batch  31: Loss = 0.0344, Accuracy = 98.44%\n",
            "  Batch  32: Loss = 0.1295, Accuracy = 95.31%\n",
            "  Batch  33: Loss = 0.0624, Accuracy = 96.88%\n",
            "  Batch  34: Loss = 0.0994, Accuracy = 96.88%\n",
            "  Batch  35: Loss = 0.0793, Accuracy = 96.88%\n",
            "  Batch  36: Loss = 0.0755, Accuracy = 96.88%\n",
            "  Batch  37: Loss = 0.2618, Accuracy = 89.06%\n",
            "  Batch  38: Loss = 0.0935, Accuracy = 95.31%\n",
            "  Batch  39: Loss = 0.0125, Accuracy = 100.00%\n",
            "  Batch  40: Loss = 0.0848, Accuracy = 96.88%\n",
            "  Batch  41: Loss = 0.1228, Accuracy = 98.44%\n",
            "  Batch  42: Loss = 0.2724, Accuracy = 93.75%\n",
            "  Batch  43: Loss = 0.2253, Accuracy = 93.75%\n",
            "  Batch  44: Loss = 0.0651, Accuracy = 96.88%\n",
            "  Batch  45: Loss = 0.1237, Accuracy = 95.31%\n",
            "  Batch  46: Loss = 0.0378, Accuracy = 100.00%\n",
            "  Batch  47: Loss = 0.0512, Accuracy = 98.44%\n",
            "  Batch  48: Loss = 0.0154, Accuracy = 100.00%\n",
            "  Batch  49: Loss = 0.0729, Accuracy = 98.44%\n",
            "  Batch  50: Loss = 0.1221, Accuracy = 95.31%\n",
            "  Batch  51: Loss = 0.0093, Accuracy = 100.00%\n",
            "  Batch  52: Loss = 0.0279, Accuracy = 100.00%\n",
            "  Batch  53: Loss = 0.0222, Accuracy = 100.00%\n",
            "  Batch  54: Loss = 0.0788, Accuracy = 98.44%\n",
            "  Batch  55: Loss = 0.0465, Accuracy = 98.44%\n",
            "  Batch  56: Loss = 0.1591, Accuracy = 95.31%\n",
            "  Batch  57: Loss = 0.0639, Accuracy = 98.44%\n",
            "  Batch  58: Loss = 0.1275, Accuracy = 95.31%\n",
            "  Batch  59: Loss = 0.0862, Accuracy = 95.31%\n",
            "  Batch  60: Loss = 0.0895, Accuracy = 95.31%\n",
            "  Batch  61: Loss = 0.0679, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.0800, Accuracy = 96.88%\n",
            "  Batch  63: Loss = 0.0597, Accuracy = 98.44%\n",
            "  Batch  64: Loss = 0.1391, Accuracy = 93.75%\n",
            "  Batch  65: Loss = 0.0897, Accuracy = 96.88%\n",
            "  Batch  66: Loss = 0.0177, Accuracy = 100.00%\n",
            "  Batch  67: Loss = 0.1107, Accuracy = 93.75%\n",
            "  Batch  68: Loss = 0.0302, Accuracy = 100.00%\n",
            "  Batch  69: Loss = 0.0489, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.1036, Accuracy = 96.88%\n",
            "  Batch  71: Loss = 0.0614, Accuracy = 98.44%\n",
            "  Batch  72: Loss = 0.0529, Accuracy = 98.44%\n",
            "  Batch  73: Loss = 0.0698, Accuracy = 98.44%\n",
            "  Batch  74: Loss = 0.0966, Accuracy = 95.31%\n",
            "  Batch  75: Loss = 0.1121, Accuracy = 95.31%\n",
            "  Batch  76: Loss = 0.1052, Accuracy = 96.88%\n",
            "  Batch  77: Loss = 0.0517, Accuracy = 96.88%\n",
            "  Batch  78: Loss = 0.0327, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.0531, Accuracy = 96.88%\n",
            "  Batch  80: Loss = 0.0993, Accuracy = 95.31%\n",
            "  Batch  81: Loss = 0.1919, Accuracy = 95.31%\n",
            "  Batch  82: Loss = 0.0909, Accuracy = 96.88%\n",
            "  Batch  83: Loss = 0.0563, Accuracy = 98.44%\n",
            "  Batch  84: Loss = 0.1248, Accuracy = 95.31%\n",
            "  Batch  85: Loss = 0.0597, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.0203, Accuracy = 100.00%\n",
            "  Batch  87: Loss = 0.2095, Accuracy = 95.31%\n",
            "  Batch  88: Loss = 0.0809, Accuracy = 95.31%\n",
            "  Batch  89: Loss = 0.0186, Accuracy = 100.00%\n",
            "  Batch  90: Loss = 0.0696, Accuracy = 96.88%\n",
            "  Batch  91: Loss = 0.0724, Accuracy = 98.44%\n",
            "  Batch  92: Loss = 0.0695, Accuracy = 98.44%\n",
            "  Batch  93: Loss = 0.2037, Accuracy = 92.19%\n",
            "  Batch  94: Loss = 0.0566, Accuracy = 96.88%\n",
            "  Batch  95: Loss = 0.1519, Accuracy = 96.88%\n",
            "  Batch  96: Loss = 0.0447, Accuracy = 98.44%\n",
            "  Batch  97: Loss = 0.0813, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.0772, Accuracy = 96.88%\n",
            "  Batch  99: Loss = 0.0345, Accuracy = 100.00%\n",
            "  Batch 100: Loss = 0.1314, Accuracy = 95.31%\n",
            "  Batch 101: Loss = 0.1338, Accuracy = 93.75%\n",
            "  Batch 102: Loss = 0.0374, Accuracy = 98.44%\n",
            "  Batch 103: Loss = 0.0396, Accuracy = 100.00%\n",
            "  Batch 104: Loss = 0.0437, Accuracy = 96.88%\n",
            "  Batch 105: Loss = 0.0578, Accuracy = 98.44%\n",
            "  Batch 106: Loss = 0.0507, Accuracy = 98.44%\n",
            "  Batch 107: Loss = 0.0240, Accuracy = 100.00%\n",
            "  Batch 108: Loss = 0.0287, Accuracy = 98.44%\n",
            "  Batch 109: Loss = 0.0371, Accuracy = 98.44%\n",
            "  Batch 110: Loss = 0.0640, Accuracy = 95.31%\n",
            "  Batch 111: Loss = 0.1293, Accuracy = 95.31%\n",
            "  Batch 112: Loss = 0.0706, Accuracy = 98.44%\n",
            "  Batch 113: Loss = 0.0610, Accuracy = 98.44%\n",
            "  Batch 114: Loss = 0.1270, Accuracy = 93.75%\n",
            "  Batch 115: Loss = 0.0286, Accuracy = 100.00%\n",
            "  Batch 116: Loss = 0.0413, Accuracy = 98.44%\n",
            "  Batch 117: Loss = 0.0338, Accuracy = 98.44%\n",
            "  Batch 118: Loss = 0.0295, Accuracy = 98.44%\n",
            "  Batch 119: Loss = 0.1035, Accuracy = 95.31%\n",
            "  Batch 120: Loss = 0.0374, Accuracy = 98.44%\n",
            "  Batch 121: Loss = 0.0657, Accuracy = 98.44%\n",
            "  Batch 122: Loss = 0.0170, Accuracy = 100.00%\n",
            "  Batch 123: Loss = 0.0373, Accuracy = 98.44%\n",
            "  Batch 124: Loss = 0.1132, Accuracy = 96.88%\n",
            "  Batch 125: Loss = 0.0207, Accuracy = 100.00%\n",
            "  Batch 126: Loss = 0.0514, Accuracy = 98.44%\n",
            "  Batch 127: Loss = 0.0399, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0558, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.0428, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.0482, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0028, Accuracy = 100.00%\n",
            "==> Epoch 95 Summary: Total Loss = 11.2154, Accuracy = 97.07%\n",
            "\n",
            "Epoch 96\n",
            "  Batch   1: Loss = 0.1751, Accuracy = 95.31%\n",
            "  Batch   2: Loss = 0.0881, Accuracy = 98.44%\n",
            "  Batch   3: Loss = 0.0389, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0096, Accuracy = 100.00%\n",
            "  Batch   5: Loss = 0.0784, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.1857, Accuracy = 95.31%\n",
            "  Batch   7: Loss = 0.0642, Accuracy = 98.44%\n",
            "  Batch   8: Loss = 0.0164, Accuracy = 100.00%\n",
            "  Batch   9: Loss = 0.0610, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.0706, Accuracy = 95.31%\n",
            "  Batch  11: Loss = 0.0252, Accuracy = 98.44%\n",
            "  Batch  12: Loss = 0.1141, Accuracy = 95.31%\n",
            "  Batch  13: Loss = 0.1685, Accuracy = 92.19%\n",
            "  Batch  14: Loss = 0.0212, Accuracy = 100.00%\n",
            "  Batch  15: Loss = 0.0908, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.0535, Accuracy = 98.44%\n",
            "  Batch  17: Loss = 0.0354, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.0734, Accuracy = 98.44%\n",
            "  Batch  19: Loss = 0.0671, Accuracy = 98.44%\n",
            "  Batch  20: Loss = 0.0235, Accuracy = 100.00%\n",
            "  Batch  21: Loss = 0.2270, Accuracy = 90.62%\n",
            "  Batch  22: Loss = 0.0183, Accuracy = 98.44%\n",
            "  Batch  23: Loss = 0.0694, Accuracy = 98.44%\n",
            "  Batch  24: Loss = 0.1391, Accuracy = 95.31%\n",
            "  Batch  25: Loss = 0.0566, Accuracy = 96.88%\n",
            "  Batch  26: Loss = 0.0267, Accuracy = 98.44%\n",
            "  Batch  27: Loss = 0.0545, Accuracy = 98.44%\n",
            "  Batch  28: Loss = 0.0595, Accuracy = 98.44%\n",
            "  Batch  29: Loss = 0.1194, Accuracy = 96.88%\n",
            "  Batch  30: Loss = 0.0925, Accuracy = 95.31%\n",
            "  Batch  31: Loss = 0.0253, Accuracy = 100.00%\n",
            "  Batch  32: Loss = 0.1308, Accuracy = 95.31%\n",
            "  Batch  33: Loss = 0.0185, Accuracy = 100.00%\n",
            "  Batch  34: Loss = 0.0187, Accuracy = 100.00%\n",
            "  Batch  35: Loss = 0.0765, Accuracy = 96.88%\n",
            "  Batch  36: Loss = 0.0359, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.1116, Accuracy = 95.31%\n",
            "  Batch  38: Loss = 0.0838, Accuracy = 96.88%\n",
            "  Batch  39: Loss = 0.0344, Accuracy = 98.44%\n",
            "  Batch  40: Loss = 0.0370, Accuracy = 98.44%\n",
            "  Batch  41: Loss = 0.0310, Accuracy = 96.88%\n",
            "  Batch  42: Loss = 0.0258, Accuracy = 100.00%\n",
            "  Batch  43: Loss = 0.0336, Accuracy = 98.44%\n",
            "  Batch  44: Loss = 0.0278, Accuracy = 98.44%\n",
            "  Batch  45: Loss = 0.0133, Accuracy = 100.00%\n",
            "  Batch  46: Loss = 0.0233, Accuracy = 100.00%\n",
            "  Batch  47: Loss = 0.1008, Accuracy = 95.31%\n",
            "  Batch  48: Loss = 0.1057, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.0426, Accuracy = 98.44%\n",
            "  Batch  50: Loss = 0.0109, Accuracy = 100.00%\n",
            "  Batch  51: Loss = 0.0382, Accuracy = 100.00%\n",
            "  Batch  52: Loss = 0.0759, Accuracy = 93.75%\n",
            "  Batch  53: Loss = 0.0589, Accuracy = 98.44%\n",
            "  Batch  54: Loss = 0.0729, Accuracy = 93.75%\n",
            "  Batch  55: Loss = 0.0246, Accuracy = 100.00%\n",
            "  Batch  56: Loss = 0.0495, Accuracy = 98.44%\n",
            "  Batch  57: Loss = 0.0401, Accuracy = 98.44%\n",
            "  Batch  58: Loss = 0.0148, Accuracy = 100.00%\n",
            "  Batch  59: Loss = 0.0355, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.1376, Accuracy = 98.44%\n",
            "  Batch  61: Loss = 0.0528, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.0707, Accuracy = 95.31%\n",
            "  Batch  63: Loss = 0.0566, Accuracy = 96.88%\n",
            "  Batch  64: Loss = 0.0952, Accuracy = 95.31%\n",
            "  Batch  65: Loss = 0.0702, Accuracy = 95.31%\n",
            "  Batch  66: Loss = 0.0358, Accuracy = 98.44%\n",
            "  Batch  67: Loss = 0.0633, Accuracy = 98.44%\n",
            "  Batch  68: Loss = 0.1054, Accuracy = 95.31%\n",
            "  Batch  69: Loss = 0.0375, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.0529, Accuracy = 98.44%\n",
            "  Batch  71: Loss = 0.1302, Accuracy = 93.75%\n",
            "  Batch  72: Loss = 0.0127, Accuracy = 100.00%\n",
            "  Batch  73: Loss = 0.0601, Accuracy = 98.44%\n",
            "  Batch  74: Loss = 0.1931, Accuracy = 96.88%\n",
            "  Batch  75: Loss = 0.0466, Accuracy = 98.44%\n",
            "  Batch  76: Loss = 0.1297, Accuracy = 96.88%\n",
            "  Batch  77: Loss = 0.0922, Accuracy = 95.31%\n",
            "  Batch  78: Loss = 0.1291, Accuracy = 95.31%\n",
            "  Batch  79: Loss = 0.0216, Accuracy = 100.00%\n",
            "  Batch  80: Loss = 0.0351, Accuracy = 98.44%\n",
            "  Batch  81: Loss = 0.0354, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.0739, Accuracy = 96.88%\n",
            "  Batch  83: Loss = 0.0487, Accuracy = 98.44%\n",
            "  Batch  84: Loss = 0.0834, Accuracy = 96.88%\n",
            "  Batch  85: Loss = 0.0761, Accuracy = 96.88%\n",
            "  Batch  86: Loss = 0.0536, Accuracy = 98.44%\n",
            "  Batch  87: Loss = 0.1115, Accuracy = 95.31%\n",
            "  Batch  88: Loss = 0.1247, Accuracy = 93.75%\n",
            "  Batch  89: Loss = 0.1027, Accuracy = 96.88%\n",
            "  Batch  90: Loss = 0.0916, Accuracy = 96.88%\n",
            "  Batch  91: Loss = 0.0985, Accuracy = 95.31%\n",
            "  Batch  92: Loss = 0.0099, Accuracy = 100.00%\n",
            "  Batch  93: Loss = 0.1919, Accuracy = 96.88%\n",
            "  Batch  94: Loss = 0.1053, Accuracy = 96.88%\n",
            "  Batch  95: Loss = 0.0763, Accuracy = 96.88%\n",
            "  Batch  96: Loss = 0.0682, Accuracy = 96.88%\n",
            "  Batch  97: Loss = 0.0503, Accuracy = 98.44%\n",
            "  Batch  98: Loss = 0.1312, Accuracy = 95.31%\n",
            "  Batch  99: Loss = 0.0566, Accuracy = 96.88%\n",
            "  Batch 100: Loss = 0.1074, Accuracy = 95.31%\n",
            "  Batch 101: Loss = 0.0090, Accuracy = 100.00%\n",
            "  Batch 102: Loss = 0.1211, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.0857, Accuracy = 96.88%\n",
            "  Batch 104: Loss = 0.0248, Accuracy = 100.00%\n",
            "  Batch 105: Loss = 0.1633, Accuracy = 95.31%\n",
            "  Batch 106: Loss = 0.0214, Accuracy = 100.00%\n",
            "  Batch 107: Loss = 0.0887, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.2245, Accuracy = 93.75%\n",
            "  Batch 109: Loss = 0.0285, Accuracy = 98.44%\n",
            "  Batch 110: Loss = 0.0374, Accuracy = 98.44%\n",
            "  Batch 111: Loss = 0.0472, Accuracy = 96.88%\n",
            "  Batch 112: Loss = 0.0885, Accuracy = 95.31%\n",
            "  Batch 113: Loss = 0.0682, Accuracy = 96.88%\n",
            "  Batch 114: Loss = 0.0188, Accuracy = 100.00%\n",
            "  Batch 115: Loss = 0.0644, Accuracy = 98.44%\n",
            "  Batch 116: Loss = 0.0604, Accuracy = 96.88%\n",
            "  Batch 117: Loss = 0.0820, Accuracy = 95.31%\n",
            "  Batch 118: Loss = 0.0500, Accuracy = 98.44%\n",
            "  Batch 119: Loss = 0.1002, Accuracy = 98.44%\n",
            "  Batch 120: Loss = 0.0428, Accuracy = 98.44%\n",
            "  Batch 121: Loss = 0.0288, Accuracy = 98.44%\n",
            "  Batch 122: Loss = 0.0857, Accuracy = 95.31%\n",
            "  Batch 123: Loss = 0.1211, Accuracy = 95.31%\n",
            "  Batch 124: Loss = 0.0176, Accuracy = 100.00%\n",
            "  Batch 125: Loss = 0.0386, Accuracy = 96.88%\n",
            "  Batch 126: Loss = 0.0785, Accuracy = 95.31%\n",
            "  Batch 127: Loss = 0.0262, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.1371, Accuracy = 95.31%\n",
            "  Batch 129: Loss = 0.1950, Accuracy = 92.19%\n",
            "  Batch 130: Loss = 0.0690, Accuracy = 98.44%\n",
            "  Batch 131: Loss = 0.0005, Accuracy = 100.00%\n",
            "==> Epoch 96 Summary: Total Loss = 9.2719, Accuracy = 97.42%\n",
            "\n",
            "Epoch 97\n",
            "  Batch   1: Loss = 0.0467, Accuracy = 98.44%\n",
            "  Batch   2: Loss = 0.1179, Accuracy = 95.31%\n",
            "  Batch   3: Loss = 0.0218, Accuracy = 100.00%\n",
            "  Batch   4: Loss = 0.0349, Accuracy = 100.00%\n",
            "  Batch   5: Loss = 0.1879, Accuracy = 92.19%\n",
            "  Batch   6: Loss = 0.1441, Accuracy = 95.31%\n",
            "  Batch   7: Loss = 0.0401, Accuracy = 98.44%\n",
            "  Batch   8: Loss = 0.0896, Accuracy = 96.88%\n",
            "  Batch   9: Loss = 0.0441, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.1504, Accuracy = 93.75%\n",
            "  Batch  11: Loss = 0.0821, Accuracy = 95.31%\n",
            "  Batch  12: Loss = 0.0136, Accuracy = 100.00%\n",
            "  Batch  13: Loss = 0.0248, Accuracy = 98.44%\n",
            "  Batch  14: Loss = 0.0869, Accuracy = 96.88%\n",
            "  Batch  15: Loss = 0.1535, Accuracy = 95.31%\n",
            "  Batch  16: Loss = 0.0664, Accuracy = 96.88%\n",
            "  Batch  17: Loss = 0.0292, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.0884, Accuracy = 95.31%\n",
            "  Batch  19: Loss = 0.0102, Accuracy = 100.00%\n",
            "  Batch  20: Loss = 0.3369, Accuracy = 90.62%\n",
            "  Batch  21: Loss = 0.0513, Accuracy = 96.88%\n",
            "  Batch  22: Loss = 0.0526, Accuracy = 98.44%\n",
            "  Batch  23: Loss = 0.0985, Accuracy = 96.88%\n",
            "  Batch  24: Loss = 0.1031, Accuracy = 95.31%\n",
            "  Batch  25: Loss = 0.1283, Accuracy = 92.19%\n",
            "  Batch  26: Loss = 0.0972, Accuracy = 95.31%\n",
            "  Batch  27: Loss = 0.2464, Accuracy = 96.88%\n",
            "  Batch  28: Loss = 0.0933, Accuracy = 96.88%\n",
            "  Batch  29: Loss = 0.1797, Accuracy = 93.75%\n",
            "  Batch  30: Loss = 0.0248, Accuracy = 100.00%\n",
            "  Batch  31: Loss = 0.0556, Accuracy = 98.44%\n",
            "  Batch  32: Loss = 0.1251, Accuracy = 93.75%\n",
            "  Batch  33: Loss = 0.1122, Accuracy = 93.75%\n",
            "  Batch  34: Loss = 0.0552, Accuracy = 96.88%\n",
            "  Batch  35: Loss = 0.1080, Accuracy = 95.31%\n",
            "  Batch  36: Loss = 0.1577, Accuracy = 96.88%\n",
            "  Batch  37: Loss = 0.0946, Accuracy = 95.31%\n",
            "  Batch  38: Loss = 0.0332, Accuracy = 100.00%\n",
            "  Batch  39: Loss = 0.1125, Accuracy = 96.88%\n",
            "  Batch  40: Loss = 0.0526, Accuracy = 98.44%\n",
            "  Batch  41: Loss = 0.0386, Accuracy = 100.00%\n",
            "  Batch  42: Loss = 0.0381, Accuracy = 98.44%\n",
            "  Batch  43: Loss = 0.0676, Accuracy = 98.44%\n",
            "  Batch  44: Loss = 0.1238, Accuracy = 95.31%\n",
            "  Batch  45: Loss = 0.0274, Accuracy = 100.00%\n",
            "  Batch  46: Loss = 0.1794, Accuracy = 95.31%\n",
            "  Batch  47: Loss = 0.0573, Accuracy = 98.44%\n",
            "  Batch  48: Loss = 0.0851, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.1611, Accuracy = 96.88%\n",
            "  Batch  50: Loss = 0.0483, Accuracy = 98.44%\n",
            "  Batch  51: Loss = 0.0732, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.0361, Accuracy = 100.00%\n",
            "  Batch  53: Loss = 0.1030, Accuracy = 95.31%\n",
            "  Batch  54: Loss = 0.0242, Accuracy = 100.00%\n",
            "  Batch  55: Loss = 0.0313, Accuracy = 98.44%\n",
            "  Batch  56: Loss = 0.0261, Accuracy = 100.00%\n",
            "  Batch  57: Loss = 0.0472, Accuracy = 98.44%\n",
            "  Batch  58: Loss = 0.1057, Accuracy = 98.44%\n",
            "  Batch  59: Loss = 0.0112, Accuracy = 100.00%\n",
            "  Batch  60: Loss = 0.1642, Accuracy = 95.31%\n",
            "  Batch  61: Loss = 0.0934, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.0193, Accuracy = 100.00%\n",
            "  Batch  63: Loss = 0.1123, Accuracy = 98.44%\n",
            "  Batch  64: Loss = 0.0607, Accuracy = 96.88%\n",
            "  Batch  65: Loss = 0.1465, Accuracy = 95.31%\n",
            "  Batch  66: Loss = 0.0204, Accuracy = 100.00%\n",
            "  Batch  67: Loss = 0.0704, Accuracy = 95.31%\n",
            "  Batch  68: Loss = 0.0674, Accuracy = 98.44%\n",
            "  Batch  69: Loss = 0.0708, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.0782, Accuracy = 96.88%\n",
            "  Batch  71: Loss = 0.0256, Accuracy = 98.44%\n",
            "  Batch  72: Loss = 0.0064, Accuracy = 100.00%\n",
            "  Batch  73: Loss = 0.1038, Accuracy = 96.88%\n",
            "  Batch  74: Loss = 0.0443, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.1221, Accuracy = 96.88%\n",
            "  Batch  76: Loss = 0.0809, Accuracy = 96.88%\n",
            "  Batch  77: Loss = 0.2005, Accuracy = 95.31%\n",
            "  Batch  78: Loss = 0.1063, Accuracy = 96.88%\n",
            "  Batch  79: Loss = 0.0061, Accuracy = 100.00%\n",
            "  Batch  80: Loss = 0.0743, Accuracy = 98.44%\n",
            "  Batch  81: Loss = 0.0712, Accuracy = 96.88%\n",
            "  Batch  82: Loss = 0.0312, Accuracy = 98.44%\n",
            "  Batch  83: Loss = 0.0194, Accuracy = 100.00%\n",
            "  Batch  84: Loss = 0.0383, Accuracy = 98.44%\n",
            "  Batch  85: Loss = 0.0735, Accuracy = 98.44%\n",
            "  Batch  86: Loss = 0.0790, Accuracy = 96.88%\n",
            "  Batch  87: Loss = 0.0326, Accuracy = 98.44%\n",
            "  Batch  88: Loss = 0.0195, Accuracy = 100.00%\n",
            "  Batch  89: Loss = 0.0505, Accuracy = 96.88%\n",
            "  Batch  90: Loss = 0.0736, Accuracy = 96.88%\n",
            "  Batch  91: Loss = 0.0498, Accuracy = 98.44%\n",
            "  Batch  92: Loss = 0.1173, Accuracy = 98.44%\n",
            "  Batch  93: Loss = 0.0600, Accuracy = 98.44%\n",
            "  Batch  94: Loss = 0.1603, Accuracy = 95.31%\n",
            "  Batch  95: Loss = 0.0331, Accuracy = 98.44%\n",
            "  Batch  96: Loss = 0.0278, Accuracy = 100.00%\n",
            "  Batch  97: Loss = 0.0222, Accuracy = 100.00%\n",
            "  Batch  98: Loss = 0.0533, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.0626, Accuracy = 96.88%\n",
            "  Batch 100: Loss = 0.0593, Accuracy = 98.44%\n",
            "  Batch 101: Loss = 0.0271, Accuracy = 98.44%\n",
            "  Batch 102: Loss = 0.0064, Accuracy = 100.00%\n",
            "  Batch 103: Loss = 0.0358, Accuracy = 98.44%\n",
            "  Batch 104: Loss = 0.1285, Accuracy = 95.31%\n",
            "  Batch 105: Loss = 0.1158, Accuracy = 93.75%\n",
            "  Batch 106: Loss = 0.0574, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.0214, Accuracy = 100.00%\n",
            "  Batch 108: Loss = 0.0438, Accuracy = 98.44%\n",
            "  Batch 109: Loss = 0.0349, Accuracy = 98.44%\n",
            "  Batch 110: Loss = 0.0457, Accuracy = 98.44%\n",
            "  Batch 111: Loss = 0.0966, Accuracy = 96.88%\n",
            "  Batch 112: Loss = 0.1135, Accuracy = 93.75%\n",
            "  Batch 113: Loss = 0.0218, Accuracy = 100.00%\n",
            "  Batch 114: Loss = 0.0577, Accuracy = 96.88%\n",
            "  Batch 115: Loss = 0.0195, Accuracy = 98.44%\n",
            "  Batch 116: Loss = 0.0220, Accuracy = 100.00%\n",
            "  Batch 117: Loss = 0.0124, Accuracy = 100.00%\n",
            "  Batch 118: Loss = 0.0559, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.1333, Accuracy = 95.31%\n",
            "  Batch 120: Loss = 0.0608, Accuracy = 98.44%\n",
            "  Batch 121: Loss = 0.0406, Accuracy = 98.44%\n",
            "  Batch 122: Loss = 0.1441, Accuracy = 95.31%\n",
            "  Batch 123: Loss = 0.0235, Accuracy = 98.44%\n",
            "  Batch 124: Loss = 0.0803, Accuracy = 96.88%\n",
            "  Batch 125: Loss = 0.0785, Accuracy = 98.44%\n",
            "  Batch 126: Loss = 0.0688, Accuracy = 96.88%\n",
            "  Batch 127: Loss = 0.0778, Accuracy = 95.31%\n",
            "  Batch 128: Loss = 0.0277, Accuracy = 100.00%\n",
            "  Batch 129: Loss = 0.0378, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.1522, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0023, Accuracy = 100.00%\n",
            "==> Epoch 97 Summary: Total Loss = 9.6881, Accuracy = 97.50%\n",
            "\n",
            "Epoch 98\n",
            "  Batch   1: Loss = 0.0727, Accuracy = 98.44%\n",
            "  Batch   2: Loss = 0.0314, Accuracy = 98.44%\n",
            "  Batch   3: Loss = 0.0892, Accuracy = 96.88%\n",
            "  Batch   4: Loss = 0.0209, Accuracy = 100.00%\n",
            "  Batch   5: Loss = 0.0098, Accuracy = 100.00%\n",
            "  Batch   6: Loss = 0.0631, Accuracy = 98.44%\n",
            "  Batch   7: Loss = 0.0451, Accuracy = 98.44%\n",
            "  Batch   8: Loss = 0.0040, Accuracy = 100.00%\n",
            "  Batch   9: Loss = 0.1247, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.1063, Accuracy = 96.88%\n",
            "  Batch  11: Loss = 0.0380, Accuracy = 98.44%\n",
            "  Batch  12: Loss = 0.0869, Accuracy = 96.88%\n",
            "  Batch  13: Loss = 0.0467, Accuracy = 98.44%\n",
            "  Batch  14: Loss = 0.0264, Accuracy = 100.00%\n",
            "  Batch  15: Loss = 0.0089, Accuracy = 100.00%\n",
            "  Batch  16: Loss = 0.0376, Accuracy = 100.00%\n",
            "  Batch  17: Loss = 0.0556, Accuracy = 96.88%\n",
            "  Batch  18: Loss = 0.0276, Accuracy = 98.44%\n",
            "  Batch  19: Loss = 0.0267, Accuracy = 98.44%\n",
            "  Batch  20: Loss = 0.0450, Accuracy = 98.44%\n",
            "  Batch  21: Loss = 0.0085, Accuracy = 100.00%\n",
            "  Batch  22: Loss = 0.1252, Accuracy = 96.88%\n",
            "  Batch  23: Loss = 0.0085, Accuracy = 100.00%\n",
            "  Batch  24: Loss = 0.0110, Accuracy = 100.00%\n",
            "  Batch  25: Loss = 0.0518, Accuracy = 96.88%\n",
            "  Batch  26: Loss = 0.0562, Accuracy = 98.44%\n",
            "  Batch  27: Loss = 0.0801, Accuracy = 98.44%\n",
            "  Batch  28: Loss = 0.0884, Accuracy = 96.88%\n",
            "  Batch  29: Loss = 0.1797, Accuracy = 93.75%\n",
            "  Batch  30: Loss = 0.0157, Accuracy = 100.00%\n",
            "  Batch  31: Loss = 0.0426, Accuracy = 100.00%\n",
            "  Batch  32: Loss = 0.0558, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.0429, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.0214, Accuracy = 98.44%\n",
            "  Batch  35: Loss = 0.0467, Accuracy = 98.44%\n",
            "  Batch  36: Loss = 0.0770, Accuracy = 98.44%\n",
            "  Batch  37: Loss = 0.0439, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.0266, Accuracy = 100.00%\n",
            "  Batch  39: Loss = 0.0166, Accuracy = 100.00%\n",
            "  Batch  40: Loss = 0.0051, Accuracy = 100.00%\n",
            "  Batch  41: Loss = 0.0268, Accuracy = 100.00%\n",
            "  Batch  42: Loss = 0.0885, Accuracy = 96.88%\n",
            "  Batch  43: Loss = 0.2216, Accuracy = 93.75%\n",
            "  Batch  44: Loss = 0.1556, Accuracy = 96.88%\n",
            "  Batch  45: Loss = 0.0779, Accuracy = 95.31%\n",
            "  Batch  46: Loss = 0.0088, Accuracy = 100.00%\n",
            "  Batch  47: Loss = 0.0991, Accuracy = 93.75%\n",
            "  Batch  48: Loss = 0.0674, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.1190, Accuracy = 95.31%\n",
            "  Batch  50: Loss = 0.0761, Accuracy = 98.44%\n",
            "  Batch  51: Loss = 0.0708, Accuracy = 96.88%\n",
            "  Batch  52: Loss = 0.0261, Accuracy = 98.44%\n",
            "  Batch  53: Loss = 0.0666, Accuracy = 98.44%\n",
            "  Batch  54: Loss = 0.0444, Accuracy = 98.44%\n",
            "  Batch  55: Loss = 0.1339, Accuracy = 98.44%\n",
            "  Batch  56: Loss = 0.0564, Accuracy = 96.88%\n",
            "  Batch  57: Loss = 0.0355, Accuracy = 98.44%\n",
            "  Batch  58: Loss = 0.1704, Accuracy = 96.88%\n",
            "  Batch  59: Loss = 0.0486, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.0400, Accuracy = 100.00%\n",
            "  Batch  61: Loss = 0.0310, Accuracy = 100.00%\n",
            "  Batch  62: Loss = 0.1687, Accuracy = 95.31%\n",
            "  Batch  63: Loss = 0.0084, Accuracy = 100.00%\n",
            "  Batch  64: Loss = 0.0894, Accuracy = 96.88%\n",
            "  Batch  65: Loss = 0.0348, Accuracy = 98.44%\n",
            "  Batch  66: Loss = 0.0503, Accuracy = 98.44%\n",
            "  Batch  67: Loss = 0.0111, Accuracy = 100.00%\n",
            "  Batch  68: Loss = 0.1363, Accuracy = 93.75%\n",
            "  Batch  69: Loss = 0.0233, Accuracy = 100.00%\n",
            "  Batch  70: Loss = 0.0766, Accuracy = 98.44%\n",
            "  Batch  71: Loss = 0.0435, Accuracy = 100.00%\n",
            "  Batch  72: Loss = 0.0596, Accuracy = 96.88%\n",
            "  Batch  73: Loss = 0.0485, Accuracy = 96.88%\n",
            "  Batch  74: Loss = 0.0870, Accuracy = 98.44%\n",
            "  Batch  75: Loss = 0.0223, Accuracy = 98.44%\n",
            "  Batch  76: Loss = 0.1932, Accuracy = 92.19%\n",
            "  Batch  77: Loss = 0.0165, Accuracy = 98.44%\n",
            "  Batch  78: Loss = 0.0289, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.1950, Accuracy = 93.75%\n",
            "  Batch  80: Loss = 0.0945, Accuracy = 95.31%\n",
            "  Batch  81: Loss = 0.1281, Accuracy = 96.88%\n",
            "  Batch  82: Loss = 0.1094, Accuracy = 95.31%\n",
            "  Batch  83: Loss = 0.0693, Accuracy = 96.88%\n",
            "  Batch  84: Loss = 0.0625, Accuracy = 96.88%\n",
            "  Batch  85: Loss = 0.0824, Accuracy = 98.44%\n",
            "  Batch  86: Loss = 0.0462, Accuracy = 98.44%\n",
            "  Batch  87: Loss = 0.1555, Accuracy = 95.31%\n",
            "  Batch  88: Loss = 0.0854, Accuracy = 98.44%\n",
            "  Batch  89: Loss = 0.0385, Accuracy = 98.44%\n",
            "  Batch  90: Loss = 0.0615, Accuracy = 96.88%\n",
            "  Batch  91: Loss = 0.1223, Accuracy = 92.19%\n",
            "  Batch  92: Loss = 0.0523, Accuracy = 98.44%\n",
            "  Batch  93: Loss = 0.1378, Accuracy = 95.31%\n",
            "  Batch  94: Loss = 0.1422, Accuracy = 95.31%\n",
            "  Batch  95: Loss = 0.0530, Accuracy = 100.00%\n",
            "  Batch  96: Loss = 0.0293, Accuracy = 100.00%\n",
            "  Batch  97: Loss = 0.1213, Accuracy = 96.88%\n",
            "  Batch  98: Loss = 0.0665, Accuracy = 96.88%\n",
            "  Batch  99: Loss = 0.0874, Accuracy = 96.88%\n",
            "  Batch 100: Loss = 0.0073, Accuracy = 100.00%\n",
            "  Batch 101: Loss = 0.0869, Accuracy = 96.88%\n",
            "  Batch 102: Loss = 0.1095, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.0647, Accuracy = 98.44%\n",
            "  Batch 104: Loss = 0.0209, Accuracy = 100.00%\n",
            "  Batch 105: Loss = 0.0690, Accuracy = 96.88%\n",
            "  Batch 106: Loss = 0.0812, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.0246, Accuracy = 100.00%\n",
            "  Batch 108: Loss = 0.0281, Accuracy = 98.44%\n",
            "  Batch 109: Loss = 0.0212, Accuracy = 100.00%\n",
            "  Batch 110: Loss = 0.0493, Accuracy = 98.44%\n",
            "  Batch 111: Loss = 0.0582, Accuracy = 98.44%\n",
            "  Batch 112: Loss = 0.0401, Accuracy = 100.00%\n",
            "  Batch 113: Loss = 0.0502, Accuracy = 98.44%\n",
            "  Batch 114: Loss = 0.0613, Accuracy = 98.44%\n",
            "  Batch 115: Loss = 0.0335, Accuracy = 98.44%\n",
            "  Batch 116: Loss = 0.0131, Accuracy = 100.00%\n",
            "  Batch 117: Loss = 0.0175, Accuracy = 100.00%\n",
            "  Batch 118: Loss = 0.0651, Accuracy = 96.88%\n",
            "  Batch 119: Loss = 0.0722, Accuracy = 96.88%\n",
            "  Batch 120: Loss = 0.0161, Accuracy = 100.00%\n",
            "  Batch 121: Loss = 0.0127, Accuracy = 100.00%\n",
            "  Batch 122: Loss = 0.0747, Accuracy = 96.88%\n",
            "  Batch 123: Loss = 0.0714, Accuracy = 95.31%\n",
            "  Batch 124: Loss = 0.0125, Accuracy = 100.00%\n",
            "  Batch 125: Loss = 0.0247, Accuracy = 98.44%\n",
            "  Batch 126: Loss = 0.0396, Accuracy = 98.44%\n",
            "  Batch 127: Loss = 0.0113, Accuracy = 100.00%\n",
            "  Batch 128: Loss = 0.0582, Accuracy = 98.44%\n",
            "  Batch 129: Loss = 0.0250, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.0401, Accuracy = 98.44%\n",
            "  Batch 131: Loss = 0.0009, Accuracy = 100.00%\n",
            "==> Epoch 98 Summary: Total Loss = 8.0345, Accuracy = 98.01%\n",
            "\n",
            "Epoch 99\n",
            "  Batch   1: Loss = 0.0329, Accuracy = 98.44%\n",
            "  Batch   2: Loss = 0.1448, Accuracy = 96.88%\n",
            "  Batch   3: Loss = 0.0438, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0158, Accuracy = 100.00%\n",
            "  Batch   5: Loss = 0.0717, Accuracy = 96.88%\n",
            "  Batch   6: Loss = 0.0291, Accuracy = 98.44%\n",
            "  Batch   7: Loss = 0.1561, Accuracy = 95.31%\n",
            "  Batch   8: Loss = 0.0299, Accuracy = 100.00%\n",
            "  Batch   9: Loss = 0.0598, Accuracy = 98.44%\n",
            "  Batch  10: Loss = 0.0251, Accuracy = 100.00%\n",
            "  Batch  11: Loss = 0.1138, Accuracy = 96.88%\n",
            "  Batch  12: Loss = 0.0747, Accuracy = 95.31%\n",
            "  Batch  13: Loss = 0.1332, Accuracy = 96.88%\n",
            "  Batch  14: Loss = 0.0872, Accuracy = 96.88%\n",
            "  Batch  15: Loss = 0.0419, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.0199, Accuracy = 98.44%\n",
            "  Batch  17: Loss = 0.0740, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.0185, Accuracy = 100.00%\n",
            "  Batch  19: Loss = 0.0928, Accuracy = 96.88%\n",
            "  Batch  20: Loss = 0.0487, Accuracy = 96.88%\n",
            "  Batch  21: Loss = 0.0434, Accuracy = 100.00%\n",
            "  Batch  22: Loss = 0.0120, Accuracy = 100.00%\n",
            "  Batch  23: Loss = 0.0828, Accuracy = 95.31%\n",
            "  Batch  24: Loss = 0.0200, Accuracy = 100.00%\n",
            "  Batch  25: Loss = 0.0842, Accuracy = 96.88%\n",
            "  Batch  26: Loss = 0.0988, Accuracy = 95.31%\n",
            "  Batch  27: Loss = 0.1018, Accuracy = 96.88%\n",
            "  Batch  28: Loss = 0.0120, Accuracy = 100.00%\n",
            "  Batch  29: Loss = 0.0397, Accuracy = 98.44%\n",
            "  Batch  30: Loss = 0.0376, Accuracy = 98.44%\n",
            "  Batch  31: Loss = 0.0849, Accuracy = 96.88%\n",
            "  Batch  32: Loss = 0.0613, Accuracy = 96.88%\n",
            "  Batch  33: Loss = 0.0727, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.0299, Accuracy = 98.44%\n",
            "  Batch  35: Loss = 0.0265, Accuracy = 98.44%\n",
            "  Batch  36: Loss = 0.0090, Accuracy = 100.00%\n",
            "  Batch  37: Loss = 0.0505, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.0332, Accuracy = 98.44%\n",
            "  Batch  39: Loss = 0.1309, Accuracy = 95.31%\n",
            "  Batch  40: Loss = 0.0456, Accuracy = 98.44%\n",
            "  Batch  41: Loss = 0.0144, Accuracy = 100.00%\n",
            "  Batch  42: Loss = 0.0448, Accuracy = 98.44%\n",
            "  Batch  43: Loss = 0.0260, Accuracy = 100.00%\n",
            "  Batch  44: Loss = 0.1030, Accuracy = 98.44%\n",
            "  Batch  45: Loss = 0.1185, Accuracy = 95.31%\n",
            "  Batch  46: Loss = 0.1078, Accuracy = 98.44%\n",
            "  Batch  47: Loss = 0.0106, Accuracy = 100.00%\n",
            "  Batch  48: Loss = 0.0831, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.0179, Accuracy = 100.00%\n",
            "  Batch  50: Loss = 0.0804, Accuracy = 98.44%\n",
            "  Batch  51: Loss = 0.0505, Accuracy = 98.44%\n",
            "  Batch  52: Loss = 0.0293, Accuracy = 98.44%\n",
            "  Batch  53: Loss = 0.0288, Accuracy = 100.00%\n",
            "  Batch  54: Loss = 0.0150, Accuracy = 100.00%\n",
            "  Batch  55: Loss = 0.0404, Accuracy = 98.44%\n",
            "  Batch  56: Loss = 0.0357, Accuracy = 98.44%\n",
            "  Batch  57: Loss = 0.0545, Accuracy = 96.88%\n",
            "  Batch  58: Loss = 0.0354, Accuracy = 98.44%\n",
            "  Batch  59: Loss = 0.0626, Accuracy = 98.44%\n",
            "  Batch  60: Loss = 0.0328, Accuracy = 100.00%\n",
            "  Batch  61: Loss = 0.0498, Accuracy = 98.44%\n",
            "  Batch  62: Loss = 0.0494, Accuracy = 98.44%\n",
            "  Batch  63: Loss = 0.0593, Accuracy = 96.88%\n",
            "  Batch  64: Loss = 0.0334, Accuracy = 100.00%\n",
            "  Batch  65: Loss = 0.1177, Accuracy = 96.88%\n",
            "  Batch  66: Loss = 0.0352, Accuracy = 98.44%\n",
            "  Batch  67: Loss = 0.0709, Accuracy = 98.44%\n",
            "  Batch  68: Loss = 0.0384, Accuracy = 98.44%\n",
            "  Batch  69: Loss = 0.0746, Accuracy = 98.44%\n",
            "  Batch  70: Loss = 0.0245, Accuracy = 98.44%\n",
            "  Batch  71: Loss = 0.0285, Accuracy = 100.00%\n",
            "  Batch  72: Loss = 0.0846, Accuracy = 96.88%\n",
            "  Batch  73: Loss = 0.0051, Accuracy = 100.00%\n",
            "  Batch  74: Loss = 0.0191, Accuracy = 100.00%\n",
            "  Batch  75: Loss = 0.0751, Accuracy = 98.44%\n",
            "  Batch  76: Loss = 0.1245, Accuracy = 95.31%\n",
            "  Batch  77: Loss = 0.0774, Accuracy = 96.88%\n",
            "  Batch  78: Loss = 0.0523, Accuracy = 98.44%\n",
            "  Batch  79: Loss = 0.0940, Accuracy = 98.44%\n",
            "  Batch  80: Loss = 0.0598, Accuracy = 96.88%\n",
            "  Batch  81: Loss = 0.0949, Accuracy = 98.44%\n",
            "  Batch  82: Loss = 0.0541, Accuracy = 98.44%\n",
            "  Batch  83: Loss = 0.0255, Accuracy = 98.44%\n",
            "  Batch  84: Loss = 0.0043, Accuracy = 100.00%\n",
            "  Batch  85: Loss = 0.0306, Accuracy = 100.00%\n",
            "  Batch  86: Loss = 0.0362, Accuracy = 96.88%\n",
            "  Batch  87: Loss = 0.0350, Accuracy = 98.44%\n",
            "  Batch  88: Loss = 0.0946, Accuracy = 95.31%\n",
            "  Batch  89: Loss = 0.1109, Accuracy = 95.31%\n",
            "  Batch  90: Loss = 0.0557, Accuracy = 98.44%\n",
            "  Batch  91: Loss = 0.0383, Accuracy = 98.44%\n",
            "  Batch  92: Loss = 0.0662, Accuracy = 96.88%\n",
            "  Batch  93: Loss = 0.0235, Accuracy = 100.00%\n",
            "  Batch  94: Loss = 0.0142, Accuracy = 100.00%\n",
            "  Batch  95: Loss = 0.0678, Accuracy = 96.88%\n",
            "  Batch  96: Loss = 0.0741, Accuracy = 98.44%\n",
            "  Batch  97: Loss = 0.0251, Accuracy = 98.44%\n",
            "  Batch  98: Loss = 0.0604, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.0524, Accuracy = 98.44%\n",
            "  Batch 100: Loss = 0.2223, Accuracy = 93.75%\n",
            "  Batch 101: Loss = 0.0413, Accuracy = 96.88%\n",
            "  Batch 102: Loss = 0.0628, Accuracy = 96.88%\n",
            "  Batch 103: Loss = 0.1743, Accuracy = 95.31%\n",
            "  Batch 104: Loss = 0.0322, Accuracy = 100.00%\n",
            "  Batch 105: Loss = 0.1288, Accuracy = 93.75%\n",
            "  Batch 106: Loss = 0.0349, Accuracy = 98.44%\n",
            "  Batch 107: Loss = 0.0730, Accuracy = 95.31%\n",
            "  Batch 108: Loss = 0.0976, Accuracy = 98.44%\n",
            "  Batch 109: Loss = 0.0754, Accuracy = 96.88%\n",
            "  Batch 110: Loss = 0.0053, Accuracy = 100.00%\n",
            "  Batch 111: Loss = 0.0551, Accuracy = 96.88%\n",
            "  Batch 112: Loss = 0.2188, Accuracy = 93.75%\n",
            "  Batch 113: Loss = 0.0478, Accuracy = 98.44%\n",
            "  Batch 114: Loss = 0.0963, Accuracy = 95.31%\n",
            "  Batch 115: Loss = 0.0792, Accuracy = 98.44%\n",
            "  Batch 116: Loss = 0.1173, Accuracy = 96.88%\n",
            "  Batch 117: Loss = 0.0654, Accuracy = 98.44%\n",
            "  Batch 118: Loss = 0.1042, Accuracy = 98.44%\n",
            "  Batch 119: Loss = 0.0064, Accuracy = 100.00%\n",
            "  Batch 120: Loss = 0.0612, Accuracy = 98.44%\n",
            "  Batch 121: Loss = 0.0091, Accuracy = 100.00%\n",
            "  Batch 122: Loss = 0.1034, Accuracy = 95.31%\n",
            "  Batch 123: Loss = 0.0967, Accuracy = 93.75%\n",
            "  Batch 124: Loss = 0.1266, Accuracy = 93.75%\n",
            "  Batch 125: Loss = 0.0817, Accuracy = 95.31%\n",
            "  Batch 126: Loss = 0.0398, Accuracy = 100.00%\n",
            "  Batch 127: Loss = 0.0402, Accuracy = 98.44%\n",
            "  Batch 128: Loss = 0.0765, Accuracy = 96.88%\n",
            "  Batch 129: Loss = 0.1628, Accuracy = 89.06%\n",
            "  Batch 130: Loss = 0.1112, Accuracy = 96.88%\n",
            "  Batch 131: Loss = 0.0012, Accuracy = 100.00%\n",
            "==> Epoch 99 Summary: Total Loss = 8.1687, Accuracy = 97.86%\n",
            "\n",
            "Epoch 100\n",
            "  Batch   1: Loss = 0.0175, Accuracy = 100.00%\n",
            "  Batch   2: Loss = 0.1490, Accuracy = 93.75%\n",
            "  Batch   3: Loss = 0.0303, Accuracy = 98.44%\n",
            "  Batch   4: Loss = 0.0728, Accuracy = 96.88%\n",
            "  Batch   5: Loss = 0.0934, Accuracy = 95.31%\n",
            "  Batch   6: Loss = 0.0349, Accuracy = 98.44%\n",
            "  Batch   7: Loss = 0.1623, Accuracy = 96.88%\n",
            "  Batch   8: Loss = 0.1252, Accuracy = 95.31%\n",
            "  Batch   9: Loss = 0.1340, Accuracy = 96.88%\n",
            "  Batch  10: Loss = 0.0833, Accuracy = 96.88%\n",
            "  Batch  11: Loss = 0.0824, Accuracy = 96.88%\n",
            "  Batch  12: Loss = 0.1126, Accuracy = 95.31%\n",
            "  Batch  13: Loss = 0.1095, Accuracy = 96.88%\n",
            "  Batch  14: Loss = 0.0294, Accuracy = 100.00%\n",
            "  Batch  15: Loss = 0.0383, Accuracy = 98.44%\n",
            "  Batch  16: Loss = 0.1374, Accuracy = 96.88%\n",
            "  Batch  17: Loss = 0.0687, Accuracy = 98.44%\n",
            "  Batch  18: Loss = 0.0567, Accuracy = 96.88%\n",
            "  Batch  19: Loss = 0.0618, Accuracy = 96.88%\n",
            "  Batch  20: Loss = 0.0092, Accuracy = 100.00%\n",
            "  Batch  21: Loss = 0.0657, Accuracy = 98.44%\n",
            "  Batch  22: Loss = 0.1019, Accuracy = 98.44%\n",
            "  Batch  23: Loss = 0.1254, Accuracy = 96.88%\n",
            "  Batch  24: Loss = 0.0987, Accuracy = 98.44%\n",
            "  Batch  25: Loss = 0.0320, Accuracy = 98.44%\n",
            "  Batch  26: Loss = 0.1127, Accuracy = 95.31%\n",
            "  Batch  27: Loss = 0.1652, Accuracy = 96.88%\n",
            "  Batch  28: Loss = 0.0069, Accuracy = 100.00%\n",
            "  Batch  29: Loss = 0.0453, Accuracy = 98.44%\n",
            "  Batch  30: Loss = 0.0150, Accuracy = 100.00%\n",
            "  Batch  31: Loss = 0.0520, Accuracy = 96.88%\n",
            "  Batch  32: Loss = 0.0407, Accuracy = 98.44%\n",
            "  Batch  33: Loss = 0.0529, Accuracy = 98.44%\n",
            "  Batch  34: Loss = 0.0683, Accuracy = 98.44%\n",
            "  Batch  35: Loss = 0.1636, Accuracy = 93.75%\n",
            "  Batch  36: Loss = 0.0226, Accuracy = 100.00%\n",
            "  Batch  37: Loss = 0.0230, Accuracy = 98.44%\n",
            "  Batch  38: Loss = 0.0909, Accuracy = 96.88%\n",
            "  Batch  39: Loss = 0.1396, Accuracy = 93.75%\n",
            "  Batch  40: Loss = 0.0797, Accuracy = 96.88%\n",
            "  Batch  41: Loss = 0.0844, Accuracy = 96.88%\n",
            "  Batch  42: Loss = 0.0662, Accuracy = 98.44%\n",
            "  Batch  43: Loss = 0.1322, Accuracy = 96.88%\n",
            "  Batch  44: Loss = 0.0160, Accuracy = 100.00%\n",
            "  Batch  45: Loss = 0.1217, Accuracy = 95.31%\n",
            "  Batch  46: Loss = 0.1274, Accuracy = 93.75%\n",
            "  Batch  47: Loss = 0.0752, Accuracy = 98.44%\n",
            "  Batch  48: Loss = 0.0969, Accuracy = 96.88%\n",
            "  Batch  49: Loss = 0.1553, Accuracy = 93.75%\n",
            "  Batch  50: Loss = 0.0086, Accuracy = 100.00%\n",
            "  Batch  51: Loss = 0.1278, Accuracy = 95.31%\n",
            "  Batch  52: Loss = 0.1535, Accuracy = 95.31%\n",
            "  Batch  53: Loss = 0.0699, Accuracy = 96.88%\n",
            "  Batch  54: Loss = 0.0743, Accuracy = 96.88%\n",
            "  Batch  55: Loss = 0.0233, Accuracy = 100.00%\n",
            "  Batch  56: Loss = 0.0267, Accuracy = 100.00%\n",
            "  Batch  57: Loss = 0.2088, Accuracy = 93.75%\n",
            "  Batch  58: Loss = 0.0459, Accuracy = 98.44%\n",
            "  Batch  59: Loss = 0.0740, Accuracy = 96.88%\n",
            "  Batch  60: Loss = 0.1555, Accuracy = 95.31%\n",
            "  Batch  61: Loss = 0.0841, Accuracy = 96.88%\n",
            "  Batch  62: Loss = 0.0462, Accuracy = 98.44%\n",
            "  Batch  63: Loss = 0.0350, Accuracy = 98.44%\n",
            "  Batch  64: Loss = 0.0044, Accuracy = 100.00%\n",
            "  Batch  65: Loss = 0.0562, Accuracy = 96.88%\n",
            "  Batch  66: Loss = 0.0611, Accuracy = 98.44%\n",
            "  Batch  67: Loss = 0.0733, Accuracy = 95.31%\n",
            "  Batch  68: Loss = 0.0917, Accuracy = 96.88%\n",
            "  Batch  69: Loss = 0.0171, Accuracy = 100.00%\n",
            "  Batch  70: Loss = 0.0382, Accuracy = 100.00%\n",
            "  Batch  71: Loss = 0.0603, Accuracy = 96.88%\n",
            "  Batch  72: Loss = 0.1096, Accuracy = 95.31%\n",
            "  Batch  73: Loss = 0.0340, Accuracy = 100.00%\n",
            "  Batch  74: Loss = 0.0778, Accuracy = 96.88%\n",
            "  Batch  75: Loss = 0.0511, Accuracy = 96.88%\n",
            "  Batch  76: Loss = 0.0200, Accuracy = 100.00%\n",
            "  Batch  77: Loss = 0.0436, Accuracy = 98.44%\n",
            "  Batch  78: Loss = 0.0889, Accuracy = 96.88%\n",
            "  Batch  79: Loss = 0.1365, Accuracy = 96.88%\n",
            "  Batch  80: Loss = 0.0626, Accuracy = 98.44%\n",
            "  Batch  81: Loss = 0.0934, Accuracy = 96.88%\n",
            "  Batch  82: Loss = 0.0263, Accuracy = 98.44%\n",
            "  Batch  83: Loss = 0.1245, Accuracy = 93.75%\n",
            "  Batch  84: Loss = 0.0582, Accuracy = 96.88%\n",
            "  Batch  85: Loss = 0.0459, Accuracy = 98.44%\n",
            "  Batch  86: Loss = 0.0750, Accuracy = 96.88%\n",
            "  Batch  87: Loss = 0.0704, Accuracy = 98.44%\n",
            "  Batch  88: Loss = 0.1168, Accuracy = 95.31%\n",
            "  Batch  89: Loss = 0.0654, Accuracy = 96.88%\n",
            "  Batch  90: Loss = 0.0940, Accuracy = 95.31%\n",
            "  Batch  91: Loss = 0.0298, Accuracy = 100.00%\n",
            "  Batch  92: Loss = 0.1022, Accuracy = 95.31%\n",
            "  Batch  93: Loss = 0.0386, Accuracy = 96.88%\n",
            "  Batch  94: Loss = 0.0182, Accuracy = 98.44%\n",
            "  Batch  95: Loss = 0.0670, Accuracy = 96.88%\n",
            "  Batch  96: Loss = 0.0549, Accuracy = 98.44%\n",
            "  Batch  97: Loss = 0.0442, Accuracy = 98.44%\n",
            "  Batch  98: Loss = 0.1205, Accuracy = 98.44%\n",
            "  Batch  99: Loss = 0.0201, Accuracy = 100.00%\n",
            "  Batch 100: Loss = 0.0408, Accuracy = 98.44%\n",
            "  Batch 101: Loss = 0.1634, Accuracy = 93.75%\n",
            "  Batch 102: Loss = 0.0586, Accuracy = 98.44%\n",
            "  Batch 103: Loss = 0.2088, Accuracy = 93.75%\n",
            "  Batch 104: Loss = 0.0434, Accuracy = 98.44%\n",
            "  Batch 105: Loss = 0.0130, Accuracy = 100.00%\n",
            "  Batch 106: Loss = 0.0631, Accuracy = 96.88%\n",
            "  Batch 107: Loss = 0.0661, Accuracy = 98.44%\n",
            "  Batch 108: Loss = 0.0440, Accuracy = 100.00%\n",
            "  Batch 109: Loss = 0.1474, Accuracy = 93.75%\n",
            "  Batch 110: Loss = 0.0831, Accuracy = 96.88%\n",
            "  Batch 111: Loss = 0.1069, Accuracy = 95.31%\n",
            "  Batch 112: Loss = 0.0386, Accuracy = 98.44%\n",
            "  Batch 113: Loss = 0.0443, Accuracy = 98.44%\n",
            "  Batch 114: Loss = 0.0278, Accuracy = 98.44%\n",
            "  Batch 115: Loss = 0.0109, Accuracy = 100.00%\n",
            "  Batch 116: Loss = 0.1196, Accuracy = 95.31%\n",
            "  Batch 117: Loss = 0.0338, Accuracy = 100.00%\n",
            "  Batch 118: Loss = 0.2355, Accuracy = 92.19%\n",
            "  Batch 119: Loss = 0.0581, Accuracy = 98.44%\n",
            "  Batch 120: Loss = 0.0149, Accuracy = 100.00%\n",
            "  Batch 121: Loss = 0.1390, Accuracy = 95.31%\n",
            "  Batch 122: Loss = 0.0878, Accuracy = 98.44%\n",
            "  Batch 123: Loss = 0.0967, Accuracy = 95.31%\n",
            "  Batch 124: Loss = 0.1181, Accuracy = 93.75%\n",
            "  Batch 125: Loss = 0.1621, Accuracy = 93.75%\n",
            "  Batch 126: Loss = 0.0838, Accuracy = 96.88%\n",
            "  Batch 127: Loss = 0.0320, Accuracy = 100.00%\n",
            "  Batch 128: Loss = 0.0915, Accuracy = 95.31%\n",
            "  Batch 129: Loss = 0.0384, Accuracy = 98.44%\n",
            "  Batch 130: Loss = 0.0536, Accuracy = 98.44%\n",
            "  Batch 131: Loss = 0.1714, Accuracy = 80.00%\n",
            "==> Epoch 100 Summary: Total Loss = 10.1078, Accuracy = 97.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vision Transformer Class - Switch to evaluation mode\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "test_acc = 100.0 * correct / total\n",
        "print(f\"\\n==> Val Accuracy: {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKkt74Ochog5",
        "outputId": "60a28859-e981-40d6-a8bb-8099681f2a92"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==> Val Accuracy: 93.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(loss_list) + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, loss_list, label='Train Loss', color='red')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs Epochs')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, accuracy_list, label='Train Accuracy', color='blue')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "CCMNcuLjWyo4",
        "outputId": "ca644dfc-1da4-4295-dc42-ae6ca1ea2bf1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()   # paste your HF token when prompted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "b7418618c1e341d38f7e211a3d24bf08",
            "f013cd7ff46c4413a29a7209a836286e",
            "4e45bca530484eb0946d144543469900",
            "7564cd2ad96846afadf1598defe92ff2",
            "7c584954e3bf4f8191f8673d7a2a4e69",
            "f07043325063430fbe7a6a8e2a9c74ad",
            "d0e6f1c2abb04098a254fc322026b40c",
            "81d6aa29ee8c4ca4a2144e2c2e0b3a11",
            "dfdf68b6ae7e4a73b98e4a788de05cc7",
            "49e1377db5534de9afa7194e127995d1",
            "637dfc58073748f7b75ef8295b03d4bc",
            "13357fa6f95344cc83fcc422b8dbb4c4",
            "f04b6e1a9e2c485e912d555fffa82848",
            "05e9cc9c7ce24af0bbe964867d961a21",
            "e238b1120dce4dc080298f124946a432",
            "135ab96170f94465923e4124d280f412",
            "33a6bf35720a48b2a74ad3574a744f6b",
            "30f913290bb64e89844d6f2ae3b5498b",
            "fdf13cd7c7714062a54a563b4eb8246b",
            "43ea79a399494178b026ebde78d87ba0"
          ]
        },
        "id": "RcJP5GmhcUFU",
        "outputId": "6642837a-1583-4ce2-abf7-55a58a35f087"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7418618c1e341d38f7e211a3d24bf08"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, torch\n",
        "\n",
        "save_dir = \"vit_paddy_pytorch\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# 1) save weights\n",
        "torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
        "\n",
        "# 2) save config so you can rebuild the model later\n",
        "config = {\n",
        "    \"model_type\": \"custom_vit_paddy\",\n",
        "    \"image_size\": image_size,\n",
        "    \"num_channels\": num_channels,\n",
        "    \"patch_size\": patch_size,\n",
        "    \"num_patches\": num_patches,\n",
        "    \"token_dim\": token_dim,\n",
        "    \"num_heads\": num_heads,\n",
        "    \"transformer_blocks\": transformer_blocks,\n",
        "    \"mlp_hidden_dim\": mlp_hidden_dim,\n",
        "    \"num_classes\": num_classes,\n",
        "}\n",
        "\n",
        "with open(os.path.join(save_dir, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "# 3) (optional but recommended) simple model card\n",
        "readme_text = f\"\"\"\n",
        "# ViT Paddy Disease Classifier\n",
        "\n",
        "- Architecture: Custom Vision Transformer in PyTorch\n",
        "- Classes: {num_classes} paddy diseases + normal\n",
        "- Image size: {image_size}x{image_size}\n",
        "- Train accuracy (final): 97.35%\n",
        "- Val accuracy (final): 93.32%\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(save_dir, \"README.md\"), \"w\") as f:\n",
        "    f.write(readme_text)\n"
      ],
      "metadata": {
        "id": "m4YYEZHucVgT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo, upload_folder\n",
        "\n",
        "repo_id = \"prashanth2000/vit-paddy-disease-classifier\"\n",
        "\n",
        "# create the repo (does nothing if it already exists)\n",
        "create_repo(repo_id, exist_ok=True)\n",
        "\n",
        "# upload everything in the folder\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=save_dir,\n",
        "    commit_message=\"Add ViT paddy disease model\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "65b0f5f3301c4e18a86e7301b2a39e62",
            "33274c5f1d0743d097fe28b1022cfd38",
            "edc272001d374fbe83305abdde4c438c",
            "2afbe64238c44c9ab3caa2d58fe6f694",
            "28f31d0229ca467cac2d3f5a8df7f328",
            "3faa6f8efac94117854fb22d9db3c2ef",
            "c837ece78ef24c66970998dcb6e453f3",
            "a4e246ad9c9d44959f799178ab753e5b",
            "55dd966df1f444bd99971f56efec2c0a",
            "8fd406882e92467fad0004a259ef23d3",
            "508c5670b790449fb45a1151ceb05df3",
            "955b0870b3a94fa8a787918e65f55803",
            "5ca80d4fc98d445490bcd32e07f8e531",
            "c40e07b3e9ca4f249fa8aed395d83e07",
            "aea4e230198e48a58d1a100057daea48",
            "0bd33fc529de4b41aa56e3349823d8d0",
            "63d22d6808d2424a9e668b24fb698856",
            "9ce39f1cd69140a8b87bfca6ac04b8c5",
            "22a7d846d3d4453f810ff1bd117052a6",
            "c02aca0a6f2940fd98b0a469acb745b4",
            "7931b11d3fc040cb8866fec997ad0603",
            "265b99b33a4948be999f9315383f39f6",
            "dc0741fe11c746de80eb232c52bcd6fa",
            "f08d7e9bb543447190eb9f85ab186f68",
            "efbc20312c8146608b834e0749526191",
            "c96ddf97267d49afa25f917484a78fb6",
            "b632c17571d14157ab6148ef30446c4e",
            "5dd0059dc50748ac9b56c172f106829f",
            "84838508289441d89c97b43f69cea560",
            "f81e9f6b9e9f4b74b6b0fdc52e5038b9",
            "7024f2b77b134d728a730bb63f69de00",
            "8d56a18dcce74fc181ba39b852428736",
            "f23dbed3b03a403db397b631487b1586"
          ]
        },
        "id": "P0I6Ly5-deQx",
        "outputId": "27ccf67b-4f69-4f24-e7db-95b405ce9060"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py:9662: UserWarning: Warnings while validating metadata in README.md:\n",
            "- empty or missing yaml metadata in repo card\n",
            "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65b0f5f3301c4e18a86e7301b2a39e62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "955b0870b3a94fa8a787918e65f55803"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...pytorch/pytorch_model.bin:   3%|3         |  561kB / 17.9MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc0741fe11c746de80eb232c52bcd6fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/prashanth2000/vit-paddy-disease-classifier/commit/9725763a34c4a29dcaa5b4954da19a51bf15b53c', commit_message='Add ViT paddy disease model', commit_description='', oid='9725763a34c4a29dcaa5b4954da19a51bf15b53c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/prashanth2000/vit-paddy-disease-classifier', endpoint='https://huggingface.co', repo_type='model', repo_id='prashanth2000/vit-paddy-disease-classifier'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"vit_paddy_disease.pth\")"
      ],
      "metadata": {
        "id": "BvqMuzVWgOSp"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}